{
    "repository_name": "scikit-learn",
    "repository_purpose": "A Python library designed to provide accessible and efficient tools for implementing machine learning techniques, making it easier for developers and data scientists to apply machine learning in their projects.",
    "category": "Machine Learning Framework",
    "scope": "This repository focuses on providing a comprehensive machine learning framework for Python developers and data scientists. It is designed to streamline the application of machine learning in various tasks, enabling users to work efficiently within the Python ecosystem.",
    "subtrees": [
        {
            "refactored_subtree": {
                "Supervised Learning": {
                    "Regression Algorithms": {
                        "Linear Regression": [
                            "linear regression",
                            "ordinary least squares",
                            "multiple linear regression",
                            "simple linear regression"
                        ],
                        "Regularized Regression": [
                            "feature selection",
                            "l1 regularization",
                            "l2 regularization",
                            "kernel ridge regression"
                        ]
                    },
                    "Classification Algorithms": {
                        "SVM Classification": [
                            "support vector machines",
                            "kernel svm",
                            "hard-margin svm",
                            "smo algorithm",
                            "stochastic gradient descent"
                        ],
                        "Decision Trees": [
                            "decision trees"
                        ],
                        "Naive Bayes": [
                            "naive bayes"
                        ],
                        "Logistic Regression": [
                            "logistic regression",
                            "multinomial logistic regression",
                            "softmax function",
                            "cross-entropy loss",
                            "l1 regularization",
                            "threshold optimization",
                            "probit regression",
                            "log loss"
                        ],
                        "Imbalanced Classification": [
                            "imbalanced classification"
                        ],
                        "Multi-label Classification": [
                            "classifier chains",
                            "multi-label decision trees",
                            "multi-label knn"
                        ]
                    },
                    "Semi-Supervised Learning": {
                        "Semi-Supervised Techniques": [
                            "co-training",
                            "self-training"
                        ]
                    },
                    "Additional Supervised Methods": {
                        "Miscellaneous": [
                            "5"
                        ]
                    }
                },
                "Unsupervised Learning": {
                    "Dimensionality Reduction": {
                        "PCA Methods": [
                            "singular value decomposition",
                            "eigen decomposition",
                            "dimensionality reduction",
                            "kernel pca",
                            "sparse pca",
                            "incremental pca"
                        ],
                        "Other Reduction Methods": [
                            "lda",
                            "t-sne",
                            "umap",
                            "min_dist parameter",
                            "number of neighbors",
                            "latent topics",
                            "topic distribution"
                        ]
                    },
                    "Clustering": {
                        "K-means Clustering": [
                            "k-means clustering",
                            "spectral clustering",
                            "fuzzy k-means",
                            "mini-batch k-means",
                            "initialization methods",
                            "k-means++ initialization",
                            "convergence criteria",
                            "k-medoids",
                            "optimize clusters",
                            "elbow method",
                            "centroid initialization"
                        ],
                        "Hierarchical & Density Clustering": [
                            "divisive",
                            "agglomerative",
                            "density-based clustering"
                        ]
                    },
                    "Anomaly and Self-Organizing": {
                        "Anomaly Detection": [
                            "isolation forest",
                            "one-class svm",
                            "local outlier factor"
                        ],
                        "Self-Organizing Maps": [
                            "topology preservation",
                            "learning rate"
                        ]
                    }
                },
                "Optimization Techniques": {
                    "Gradient Descent Methods": {
                        "Adaptive & Batch Methods": [
                            "adam optimizer",
                            "nesterov accelerated gd",
                            "gd with line search",
                            "adagrad",
                            "rmsprop"
                        ],
                        "Stochastic Methods": [
                            "mini-batch sgd",
                            "sgd with momentum"
                        ]
                    },
                    "SVM Optimization": {
                        "Optimization Algorithms": [
                            "smo algorithm",
                            "stochastic gradient descent"
                        ]
                    }
                },
                "Tree-based and Ensemble Methods": {
                    "Ensemble and Tree Algorithms": {
                        "Decision Tree Methods": [
                            "early stopping",
                            "depth limitation",
                            "node-limited pre-pruning",
                            "reduced error pruning",
                            "cost complexity pruning"
                        ],
                        "Forest & Voting Methods": [
                            "multi-class classification",
                            "binary classification",
                            "quantile regression",
                            "random forest",
                            "gradient boosting",
                            "bagging",
                            "voting"
                        ]
                    }
                }
            },
            "name": "Algorithms",
            "file_path": "src/algorithms"
        },
        {
            "refactored_subtree": {
                "Preprocessing": {
                    "Feature Engineering": {
                        "Data Transformation": [
                            "scaling and normalization",
                            "one-hot encoding"
                        ],
                        "Feature Extraction": [
                            "pca"
                        ],
                        "Data Augmentation": [
                            "image augmentation",
                            "text augmentation"
                        ]
                    },
                    "Normalization": {
                        "Z-score Normalization": [
                            "normalize to mean 0 and std 1"
                        ]
                    }
                },
                "Evaluation": {
                    "Accuracy Metrics": {
                        "F1 Score": [
                            "f1 score"
                        ]
                    },
                    "Performance Metrics": {
                        "Confusion Matrix": [
                            "confusion matrix"
                        ]
                    }
                }
            },
            "name": "Workflow",
            "file_path": "src/workflow"
        },
        {
            "refactored_subtree": {
                "Data Preparation": {
                    "Data Cleaning": {
                        "Initial Cleaning": [
                            "standardize numeric formats",
                            "remove outlier rows",
                            "standardize date formats",
                            "drop null columns",
                            "dropna",
                            "convert units",
                            "use z-score method",
                            "convert to appropriate types",
                            "fill with constant",
                            "remove by standard deviation",
                            "correct data inconsistencies",
                            "remove top 1%",
                            "tag outliers",
                            "drop rows",
                            "convert text to number",
                            "drop columns with missing",
                            "reset index to default",
                            "use iqr method",
                            "detect using clustering",
                            "reset and drop old index",
                            "clip values",
                            "standardize string data",
                            "replace with default value",
                            "use forward fill"
                        ],
                        "Supplementary Adjustments": [
                            "convert to integer",
                            "keep first",
                            "convert to ordinal",
                            "standardize using robust scaling",
                            "fix inconsistent data formats",
                            "drop duplicate columns",
                            "correct typos",
                            "fill with zero",
                            "enforce consistency rules",
                            "correct data entry errors",
                            "remove special characters",
                            "fill with custom value",
                            "standardize categorical data",
                            "decimal scaling",
                            "resolve conflicting data",
                            "active learning strategies",
                            "fuzzy deduplication",
                            "remove exact duplicates",
                            "drop columns with all zeros",
                            "filter based on percentile",
                            "filter high outliers",
                            "identify outliers",
                            "filter by threshold",
                            "identify zero columns",
                            "remove based on iqr"
                        ]
                    },
                    "Imputation Techniques": {
                        "Imputation Methods": [
                            "drop based on percentage missing",
                            "identify missing",
                            "knn imputation",
                            "handle large datasets for mode imputation",
                            "delete columns",
                            "impute missing binary data with mode",
                            "pairwise deletion",
                            "mode of subset of dataset",
                            "impute with global median",
                            "impute using local median",
                            "impute with conditional mode",
                            "Weighted mean",
                            "impute with mode",
                            "impute by group median",
                            "simple imputation",
                            "polynomial interpolation",
                            "mode imputation for numbers",
                            "delete incomplete records",
                            "impute categorical columns",
                            "impute with global mean",
                            "threshold-based removal",
                            "conditional median imputation",
                            "flag rows with missing values",
                            "flag outliers",
                            "impute by group mean",
                            "conditional mean imputation",
                            "linear interpolation",
                            "impute with column median",
                            "model-based imputation"
                        ]
                    },
                    "Feature Engineering": {
                        "Combine Features": [
                            "merge feature sets",
                            "concatenate features",
                            "aggregate features",
                            "combine numerical features",
                            "combine categorical features"
                        ],
                        "Create New Features": [
                            "derive new metrics",
                            "generate ratios",
                            "time-based features",
                            "create features from external data",
                            "create features from date",
                            "create features from existing data",
                            "location-based features"
                        ],
                        "Feature Extraction": [
                            "image feature extraction",
                            "extract features from text",
                            "extract numerical features",
                            "extract categorical features"
                        ],
                        "Feature Scaling": [
                            "scale features to range",
                            "min-max scaling",
                            "standard scaling"
                        ],
                        "Interaction Terms": [
                            "create polynomial terms",
                            "categorical interaction terms",
                            "create multiplicative terms",
                            "create ratio terms"
                        ],
                        "Polynomial Features": [
                            "create polynomial features of degree 2",
                            "create third-order terms",
                            "generate higher-order terms"
                        ],
                        "Feature Selection & Reduction": [
                            "filter methods",
                            "feature importance ranking",
                            "select with chi-squared",
                            "select with mutual information",
                            "wrapper methods",
                            "select features with high correlation",
                            "select features with low variance",
                            "recursive feature elimination"
                        ]
                    },
                    "Data Aggregation & Profiling": {
                        "Bucketization": [
                            "dynamic bucketization",
                            "bucketize by custom bins",
                            "bucketize by quantiles",
                            "bucketize by range"
                        ],
                        "Aggregation": [
                            "mean",
                            "median",
                            "group and count"
                        ],
                        "Zero Column Handling": [
                            "drop columns with zero variance",
                            "drop columns with mostly zeros"
                        ],
                        "Profiling": [
                            "correlation analysis"
                        ]
                    }
                },
                "Data Encoding": {
                    "Label Encoding": {
                        "Details": [
                            "frequency-based label encoding",
                            "ordinal label encoding"
                        ]
                    },
                    "One-hot Encoding": {
                        "Combined One-hot": [
                            "sparse matrix optimization",
                            "dense one-hot encoding"
                        ]
                    },
                    "Binary Encoding": {
                        "Details": [
                            "compressed binary encoding"
                        ]
                    }
                },
                "Data Splitting": {
                    "Temporal Split": {
                        "Time-based Split": [
                            "split by time zones",
                            "split by year",
                            "split by hour",
                            "split by month",
                            "split by day"
                        ],
                        "Event-based Split": [
                            "split by event types",
                            "split by event count",
                            "split by event frequency"
                        ],
                        "Cross-validation Temporal Split": [
                            "cross-validation with event windows",
                            "time-series cross-validation",
                            "cross-validation with time windows"
                        ],
                        "Train-Test Temporal Split": [
                            "split by time intervals",
                            "split by test period",
                            "split by event intervals"
                        ]
                    }
                },
                "Data Validation": {
                    "Cross-validation": {
                        "Details": [
                            "stratified cross-validation"
                        ]
                    }
                }
            },
            "name": "Data Engineering",
            "file_path": "src/data_engineering"
        },
        {
            "refactored_subtree": {
                "Model Evaluation": {
                    "Comparative Evaluation": {
                        "ROC Analysis": [
                            "precision-recall curve",
                            "multi-class roc",
                            "roc curve with threshold selection",
                            "roc curve plotting",
                            "roc curve with auc",
                            "micro-average roc"
                        ],
                        "Precision-Recall Analysis": [
                            "plot precision-recall curves"
                        ],
                        "Statistical Testing": [
                            "paired t-test with confidence interval",
                            "two-tailed t-test",
                            "paired t-test with multiple metrics",
                            "visualize paired t-test results",
                            "interpret results",
                            "one-tailed test",
                            "test assumptions"
                        ]
                    }
                },
                "Optimization Methods": {
                    "Evolutionary Optimization": {
                        "Genetic Algorithms": [
                            "genetic algorithm with elitism",
                            "crossover operation",
                            "genetic algorithm with custom fitness function"
                        ]
                    },
                    "Hyperparameter Optimization": {
                        "Tuning Strategies": [
                            "hyperband"
                        ]
                    },
                    "Ensemble Strategies": {
                        "Meta-Learning": [
                            "meta-learning"
                        ]
                    }
                }
            },
            "name": "Advanced Modeling Techniques",
            "file_path": "src/advanced_modeling"
        }
    ],
    "data_flow_graph": [
        {
            "from": "Data Engineering",
            "to": "Algorithms",
            "data_id": "preprocessed_training_data",
            "data_type": "DataFrame",
            "transformation": "Raw data is cleaned, encoded, and feature-engineered to create a training dataset."
        },
        {
            "from": "Data Engineering",
            "to": "Workflow",
            "data_id": "enriched_feature_dataset",
            "data_type": "Enhanced DataFrame",
            "transformation": "Further feature enrichment and aggregation prepare data for pipeline processing in workflow."
        },
        {
            "from": "Algorithms",
            "to": "Workflow",
            "data_id": "trained_model_output",
            "data_type": "Model Object with Prediction Array",
            "transformation": "Trained models generate predictions and produce diagnostic data packaged for evaluation."
        },
        {
            "from": "Algorithms",
            "to": "Advanced Modeling Techniques",
            "data_id": "model_diagnostics",
            "data_type": "Diagnostic Report Object",
            "transformation": "Aggregated performance metrics and error analysis from model outputs are formatted for advanced evaluation."
        },
        {
            "from": "Workflow",
            "to": "Advanced Modeling Techniques",
            "data_id": "evaluation_metrics",
            "data_type": "Dictionary (metric names to float values)",
            "transformation": "Aggregated evaluation metrics (e.g., confusion matrix, F1 score) are computed and summarized for further analysis."
        }
    ],
    "design_itfs": {
        "Data Engineering": {
            "files_order": [
                "src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                "src/data_engineering/data_preparation/imputation/imputation_methods.py",
                "src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                "src/data_engineering/data_preparation/feature_engineering/create_features.py",
                "src/data_engineering/data_preparation/feature_engineering/feature_extraction.py",
                "src/data_engineering/data_preparation/feature_engineering/feature_scaling.py",
                "src/data_engineering/data_preparation/feature_engineering/interaction_terms.py",
                "src/data_engineering/data_preparation/feature_engineering/polynomial_features.py",
                "src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                "src/data_engineering/data_preparation/aggregation/bucketization.py",
                "src/data_engineering/data_preparation/aggregation/aggregation.py",
                "src/data_engineering/data_preparation/aggregation/zero_column.py",
                "src/data_engineering/data_preparation/aggregation/profiling.py",
                "src/data_engineering/data_encoding/label_encoding.py",
                "src/data_engineering/data_encoding/onehot_encoding.py",
                "src/data_engineering/data_encoding/binary_encoding.py",
                "src/data_engineering/data_splitting/time_based.py",
                "src/data_engineering/data_splitting/event_based.py",
                "src/data_engineering/data_splitting/cv_temporal.py",
                "src/data_engineering/data_splitting/train_test_temporal.py",
                "src/data_engineering/data_validation/cross_validation.py"
            ],
            "all_files_feature_map": {
                "src/data_engineering/data_preparation/cleaning/initial_cleaning.py": {
                    "function dropna_values": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/dropna"
                    ],
                    "function convert_to_appropriate_types": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/convert to appropriate types"
                    ],
                    "function clip_dataframe_values": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/clip values"
                    ],
                    "function detect_outliers_using_clustering": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/detect using clustering"
                    ],
                    "function drop_columns_with_missing": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/drop columns with missing"
                    ],
                    "function tag_outliers": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/tag outliers"
                    ],
                    "function remove_by_standard_deviation": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/remove by standard deviation"
                    ],
                    "function remove_outlier_rows": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/remove outlier rows"
                    ],
                    "function remove_top_percent": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/remove top 1%"
                    ],
                    "function standardize_string_data": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/standardize string data"
                    ],
                    "function standardize_date_formats": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/standardize date formats"
                    ],
                    "function convert_units": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/convert units"
                    ],
                    "function use_forward_fill": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/use forward fill"
                    ],
                    "function use_z_score_method": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/use z-score method"
                    ],
                    "function use_iqr_method": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/use iqr method"
                    ],
                    "function standardize_numeric_formats": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/standardize numeric formats"
                    ],
                    "function fill_with_constant": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/fill with constant"
                    ],
                    "function reset_index_to_default": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/reset index to default"
                    ],
                    "function convert_text_to_number": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/convert text to number"
                    ],
                    "function correct_data_inconsistencies": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/correct data inconsistencies"
                    ],
                    "function drop_null_columns": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/drop null columns"
                    ],
                    "function reset_and_drop_old_index": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/reset and drop old index"
                    ],
                    "function replace_with_default_value": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/replace with default value"
                    ],
                    "function drop_rows": [
                        "Data Preparation/Data Cleaning/Initial Cleaning/drop rows"
                    ]
                },
                "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py": {
                    "class OutlierFilter": [
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/filter based on percentile",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/filter high outliers",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/filter by threshold",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/remove based on iqr",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/identify outliers"
                    ],
                    "class DuplicateCleaner": [
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/keep first",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/drop duplicate columns",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/remove exact duplicates",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/fuzzy deduplication"
                    ],
                    "class DataConversionScaler": [
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/convert to integer",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/convert to ordinal",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/decimal scaling",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/standardize using robust scaling"
                    ],
                    "class CategoricalCleaner": [
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/standardize categorical data",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/remove special characters",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/fill with custom value",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/fill with zero"
                    ],
                    "class ConsistencyCorrector": [
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/fix inconsistent data formats",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/correct typos",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/enforce consistency rules",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/correct data entry errors",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/resolve conflicting data"
                    ],
                    "class ZeroColumnHandler": [
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/drop columns with all zeros",
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/identify zero columns"
                    ],
                    "function apply_active_learning_strategy": [
                        "Data Preparation/Data Cleaning/Supplementary Adjustments/active learning strategies"
                    ]
                },
                "src/data_engineering/data_preparation/imputation/imputation_methods.py": {
                    "function group_based_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/impute by group mean",
                        "Data Preparation/Imputation Techniques/Imputation Methods/impute by group median"
                    ],
                    "function simple_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/simple imputation"
                    ],
                    "function identify_missing": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/identify missing"
                    ],
                    "function weighted_mean_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/Weighted mean"
                    ],
                    "function delete_columns": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/delete columns"
                    ],
                    "function local_median_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/impute using local median"
                    ],
                    "function global_median_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/impute with global median"
                    ],
                    "function conditional_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/conditional mean imputation",
                        "Data Preparation/Imputation Techniques/Imputation Methods/conditional median imputation"
                    ],
                    "function pairwise_deletion": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/pairwise deletion"
                    ],
                    "function binary_mode_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/impute missing binary data with mode"
                    ],
                    "function polynomial_interpolation_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/polynomial interpolation"
                    ],
                    "function conditional_mode_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/impute with conditional mode"
                    ],
                    "function optimized_mode_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/handle large datasets for mode imputation"
                    ],
                    "function linear_interpolation_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/linear interpolation"
                    ],
                    "function mode_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/mode imputation for numbers",
                        "Data Preparation/Imputation Techniques/Imputation Methods/impute with mode",
                        "Data Preparation/Imputation Techniques/Imputation Methods/mode of subset of dataset"
                    ],
                    "function global_mean_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/impute with global mean"
                    ],
                    "function flag_outliers": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/flag outliers"
                    ],
                    "function flag_missing_rows": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/flag rows with missing values"
                    ],
                    "function threshold_based_removal": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/threshold-based removal"
                    ],
                    "function categorical_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/impute categorical columns"
                    ],
                    "function model_based_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/model-based imputation"
                    ],
                    "function column_median_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/impute with column median"
                    ],
                    "function delete_incomplete_records": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/delete incomplete records"
                    ],
                    "function drop_columns_based_on_percentage": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/drop based on percentage missing"
                    ],
                    "function knn_imputation": [
                        "Data Preparation/Imputation Techniques/Imputation Methods/knn imputation"
                    ]
                },
                "src/data_engineering/data_preparation/feature_engineering/combine_features.py": {
                    "function combine_numerical_features": [
                        "Data Preparation/Feature Engineering/Combine Features/combine numerical features"
                    ],
                    "function concatenate_features": [
                        "Data Preparation/Feature Engineering/Combine Features/concatenate features"
                    ],
                    "function combine_categorical_features": [
                        "Data Preparation/Feature Engineering/Combine Features/combine categorical features"
                    ],
                    "function aggregate_features": [
                        "Data Preparation/Feature Engineering/Combine Features/aggregate features"
                    ],
                    "function merge_feature_sets": [
                        "Data Preparation/Feature Engineering/Combine Features/merge feature sets"
                    ]
                },
                "src/data_engineering/data_preparation/feature_engineering/create_features.py": {
                    "function derive_new_metrics": [
                        "Data Preparation/Feature Engineering/Create New Features/derive new metrics"
                    ],
                    "function create_time_based_features": [
                        "Data Preparation/Feature Engineering/Create New Features/time-based features"
                    ],
                    "function generate_ratios": [
                        "Data Preparation/Feature Engineering/Create New Features/generate ratios"
                    ],
                    "function create_location_based_features": [
                        "Data Preparation/Feature Engineering/Create New Features/location-based features"
                    ],
                    "function create_date_features": [
                        "Data Preparation/Feature Engineering/Create New Features/create features from date"
                    ],
                    "function create_external_data_features": [
                        "Data Preparation/Feature Engineering/Create New Features/create features from external data"
                    ],
                    "function create_existing_data_features": [
                        "Data Preparation/Feature Engineering/Create New Features/create features from existing data"
                    ]
                },
                "src/data_engineering/data_preparation/feature_engineering/feature_extraction.py": {
                    "function extract_categorical_features": [
                        "Data Preparation/Feature Engineering/Feature Extraction/extract categorical features"
                    ],
                    "function extract_numerical_features": [
                        "Data Preparation/Feature Engineering/Feature Extraction/extract numerical features"
                    ],
                    "function extract_text_features": [
                        "Data Preparation/Feature Engineering/Feature Extraction/extract features from text"
                    ],
                    "function extract_image_features": [
                        "Data Preparation/Feature Engineering/Feature Extraction/image feature extraction"
                    ]
                },
                "src/data_engineering/data_preparation/feature_engineering/feature_scaling.py": {
                    "function scale_features_to_range": [
                        "Data Preparation/Feature Engineering/Feature Scaling/min-max scaling",
                        "Data Preparation/Feature Engineering/Feature Scaling/scale features to range"
                    ],
                    "function standard_scale_features": [
                        "Data Preparation/Feature Engineering/Feature Scaling/standard scaling"
                    ]
                },
                "src/data_engineering/data_preparation/feature_engineering/interaction_terms.py": {
                    "class InteractionTermsGenerator": [
                        "Data Preparation/Feature Engineering/Interaction Terms/categorical interaction terms",
                        "Data Preparation/Feature Engineering/Interaction Terms/create polynomial terms",
                        "Data Preparation/Feature Engineering/Interaction Terms/create multiplicative terms",
                        "Data Preparation/Feature Engineering/Interaction Terms/create ratio terms"
                    ]
                },
                "src/data_engineering/data_preparation/feature_engineering/polynomial_features.py": {
                    "class PolynomialFeaturesGenerator": [
                        "Data Preparation/Feature Engineering/Polynomial Features/generate higher-order terms",
                        "Data Preparation/Feature Engineering/Polynomial Features/create third-order terms",
                        "Data Preparation/Feature Engineering/Polynomial Features/create polynomial features of degree 2"
                    ]
                },
                "src/data_engineering/data_preparation/feature_engineering/feature_selection.py": {
                    "function recursive_feature_elimination": [
                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/recursive feature elimination"
                    ],
                    "function wrapper_feature_selection": [
                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/wrapper methods"
                    ],
                    "class FilterFeatureSelector": [
                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/select with chi-squared",
                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/select with mutual information",
                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/select features with high correlation",
                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/filter methods",
                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/select features with low variance",
                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/feature importance ranking"
                    ]
                },
                "src/data_engineering/data_preparation/aggregation/bucketization.py": {
                    "function bucketize_by_quantiles": [
                        "Data Preparation/Data Aggregation & Profiling/Bucketization/bucketize by quantiles"
                    ],
                    "function dynamic_bucketization": [
                        "Data Preparation/Data Aggregation & Profiling/Bucketization/dynamic bucketization"
                    ],
                    "function bucketize_by_range": [
                        "Data Preparation/Data Aggregation & Profiling/Bucketization/bucketize by range"
                    ],
                    "function bucketize_by_custom_bins": [
                        "Data Preparation/Data Aggregation & Profiling/Bucketization/bucketize by custom bins"
                    ]
                },
                "src/data_engineering/data_preparation/aggregation/aggregation.py": {
                    "function aggregate_mean": [
                        "Data Preparation/Data Aggregation & Profiling/Aggregation/mean"
                    ],
                    "function aggregate_median": [
                        "Data Preparation/Data Aggregation & Profiling/Aggregation/median"
                    ],
                    "function group_and_count": [
                        "Data Preparation/Data Aggregation & Profiling/Aggregation/group and count"
                    ]
                },
                "src/data_engineering/data_preparation/aggregation/zero_column.py": {
                    "function drop_columns_with_mostly_zeros": [
                        "Data Preparation/Data Aggregation & Profiling/Zero Column Handling/drop columns with mostly zeros"
                    ],
                    "function drop_columns_with_zero_variance": [
                        "Data Preparation/Data Aggregation & Profiling/Zero Column Handling/drop columns with zero variance"
                    ]
                },
                "src/data_engineering/data_preparation/aggregation/profiling.py": {
                    "function analyze_correlation": [
                        "Data Preparation/Data Aggregation & Profiling/Profiling/correlation analysis"
                    ]
                },
                "src/data_engineering/data_encoding/label_encoding.py": {
                    "function frequency_based_label_encoding": [
                        "Data Encoding/Label Encoding/Details/frequency-based label encoding"
                    ],
                    "function ordinal_label_encoding": [
                        "Data Encoding/Label Encoding/Details/ordinal label encoding"
                    ]
                },
                "src/data_engineering/data_encoding/onehot_encoding.py": {
                    "class CombinedOneHotEncoder": [
                        "Data Encoding/One-hot Encoding/Combined One-hot/dense one-hot encoding",
                        "Data Encoding/One-hot Encoding/Combined One-hot/sparse matrix optimization"
                    ]
                },
                "src/data_engineering/data_encoding/binary_encoding.py": {
                    "function compress_binary_encoding": [
                        "Data Encoding/Binary Encoding/Details/compressed binary encoding"
                    ]
                },
                "src/data_engineering/data_splitting/time_based.py": {
                    "class TimeBasedSplitter": [
                        "Data Splitting/Temporal Split/Time-based Split/split by time zones",
                        "Data Splitting/Temporal Split/Time-based Split/split by month",
                        "Data Splitting/Temporal Split/Time-based Split/split by hour",
                        "Data Splitting/Temporal Split/Time-based Split/split by day",
                        "Data Splitting/Temporal Split/Time-based Split/split by year"
                    ]
                },
                "src/data_engineering/data_splitting/event_based.py": {
                    "class EventBasedSplitter": [
                        "Data Splitting/Temporal Split/Event-based Split/split by event count",
                        "Data Splitting/Temporal Split/Event-based Split/split by event frequency",
                        "Data Splitting/Temporal Split/Event-based Split/split by event types"
                    ]
                },
                "src/data_engineering/data_splitting/cv_temporal.py": {
                    "class CVTemporalSplitter": [
                        "Data Splitting/Temporal Split/Cross-validation Temporal Split/time-series cross-validation",
                        "Data Splitting/Temporal Split/Cross-validation Temporal Split/cross-validation with event windows",
                        "Data Splitting/Temporal Split/Cross-validation Temporal Split/cross-validation with time windows"
                    ]
                },
                "src/data_engineering/data_splitting/train_test_temporal.py": {
                    "class TrainTestTemporalSplitter": [
                        "Data Splitting/Temporal Split/Train-Test Temporal Split/split by test period",
                        "Data Splitting/Temporal Split/Train-Test Temporal Split/split by time intervals",
                        "Data Splitting/Temporal Split/Train-Test Temporal Split/split by event intervals"
                    ]
                },
                "src/data_engineering/data_validation/cross_validation.py": {
                    "function stratified_cross_validation": [
                        "Data Validation/Cross-validation/Details/stratified cross-validation"
                    ]
                }
            }
        },
        "Algorithms": {
            "files_order": [
                "src/algorithms/supervised/regression/linear_regression.py",
                "src/algorithms/supervised/regression/regularized_regression.py",
                "src/algorithms/supervised/classification/svm_classification.py",
                "src/algorithms/supervised/classification/decision_trees.py",
                "src/algorithms/supervised/classification/naive_bayes.py",
                "src/algorithms/supervised/classification/logistic_regression.py",
                "src/algorithms/supervised/classification/imbalanced_classification.py",
                "src/algorithms/supervised/classification/multi_label.py",
                "src/algorithms/supervised/semi_supervised.py",
                "src/algorithms/supervised/unclassified_method.py",
                "src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                "src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                "src/algorithms/unsupervised/clustering/k_means.py",
                "src/algorithms/unsupervised/clustering/hierarchical_density.py",
                "src/algorithms/unsupervised/anomaly_detection.py",
                "src/algorithms/unsupervised/self_organizing_maps.py",
                "src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                "src/algorithms/optimization/gradient_descent/stochastic.py",
                "src/algorithms/optimization/svm_optimization.py",
                "src/algorithms/tree_ensemble/decision_tree_methods.py",
                "src/algorithms/tree_ensemble/forest_voting_methods.py"
            ],
            "all_files_feature_map": {
                "src/algorithms/supervised/regression/linear_regression.py": {
                    "class LinearRegression": [
                        "Supervised Learning/Regression Algorithms/Linear Regression/multiple linear regression",
                        "Supervised Learning/Regression Algorithms/Linear Regression/ordinary least squares",
                        "Supervised Learning/Regression Algorithms/Linear Regression/linear regression",
                        "Supervised Learning/Regression Algorithms/Linear Regression/simple linear regression"
                    ]
                },
                "src/algorithms/supervised/regression/regularized_regression.py": {
                    "function logistic_regression_l1_penalty": [
                        "Supervised Learning/Classification Algorithms/Logistic Regression/l1 regularization"
                    ],
                    "function regularized_regression_feature_selection": [
                        "Supervised Learning/Regression Algorithms/Regularized Regression/feature selection"
                    ],
                    "class KernelRidgeRegression": [
                        "Supervised Learning/Regression Algorithms/Regularized Regression/kernel ridge regression"
                    ],
                    "function l2_regularization_penalty": [
                        "Supervised Learning/Regression Algorithms/Regularized Regression/l2 regularization"
                    ],
                    "function regularized_regression_l1_penalty": [
                        "Supervised Learning/Regression Algorithms/Regularized Regression/l1 regularization"
                    ]
                },
                "src/algorithms/supervised/classification/svm_classification.py": {
                    "class KernelSVMClassifier": [
                        "Supervised Learning/Classification Algorithms/SVM Classification/kernel svm",
                        "Supervised Learning/Classification Algorithms/SVM Classification/support vector machines"
                    ],
                    "class HardMarginSVMClassifier": [
                        "Supervised Learning/Classification Algorithms/SVM Classification/hard-margin svm"
                    ]
                },
                "src/algorithms/supervised/classification/decision_trees.py": {
                    "class DecisionTreeClassifier": [
                        "Supervised Learning/Classification Algorithms/Decision Trees/decision trees"
                    ]
                },
                "src/algorithms/supervised/classification/naive_bayes.py": {
                    "class NaiveBayesClassifier": [
                        "Supervised Learning/Classification Algorithms/Naive Bayes/naive bayes"
                    ]
                },
                "src/algorithms/supervised/classification/logistic_regression.py": {
                    "class LogisticRegressionClassifier": [
                        "Supervised Learning/Classification Algorithms/Logistic Regression/logistic regression"
                    ],
                    "class MultinomialLogisticRegressionClassifier": [
                        "Supervised Learning/Classification Algorithms/Logistic Regression/multinomial logistic regression"
                    ],
                    "class ProbitRegressionClassifier": [
                        "Supervised Learning/Classification Algorithms/Logistic Regression/probit regression"
                    ],
                    "function softmax": [
                        "Supervised Learning/Classification Algorithms/Logistic Regression/softmax function"
                    ],
                    "function compute_cross_entropy_loss": [
                        "Supervised Learning/Classification Algorithms/Logistic Regression/cross-entropy loss"
                    ],
                    "function compute_log_loss": [
                        "Supervised Learning/Classification Algorithms/Logistic Regression/log loss"
                    ],
                    "function optimize_threshold": [
                        "Supervised Learning/Classification Algorithms/Logistic Regression/threshold optimization"
                    ]
                },
                "src/algorithms/supervised/classification/imbalanced_classification.py": {
                    "class ImbalancedClassifier": [
                        "Supervised Learning/Classification Algorithms/Imbalanced Classification/imbalanced classification"
                    ]
                },
                "src/algorithms/supervised/classification/multi_label.py": {
                    "class MultiLabelKNNClassifier": [
                        "Supervised Learning/Classification Algorithms/Multi-label Classification/multi-label knn"
                    ],
                    "class ClassifierChainsClassifier": [
                        "Supervised Learning/Classification Algorithms/Multi-label Classification/classifier chains"
                    ],
                    "class MultiLabelDecisionTreeClassifier": [
                        "Supervised Learning/Classification Algorithms/Multi-label Classification/multi-label decision trees"
                    ]
                },
                "src/algorithms/supervised/semi_supervised.py": {
                    "class CoTrainingClassifier": [
                        "Supervised Learning/Semi-Supervised Learning/Semi-Supervised Techniques/co-training"
                    ],
                    "class SelfTrainingClassifier": [
                        "Supervised Learning/Semi-Supervised Learning/Semi-Supervised Techniques/self-training"
                    ]
                },
                "src/algorithms/supervised/unclassified_method.py": {
                    "class UnclassifiedMethod": [
                        "Supervised Learning/Additional Supervised Methods/Miscellaneous/5"
                    ]
                },
                "src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py": {
                    "class KernelPCA": [
                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/kernel pca"
                    ],
                    "class StandardPCA": [
                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/eigen decomposition",
                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/singular value decomposition",
                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/dimensionality reduction"
                    ],
                    "class SparsePCA": [
                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/sparse pca"
                    ],
                    "class IncrementalPCA": [
                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/incremental pca"
                    ]
                },
                "src/algorithms/unsupervised/dimensionality_reduction/other_methods.py": {
                    "class UMAPReducer": [
                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/umap",
                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/min_dist parameter",
                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/number of neighbors"
                    ],
                    "class TSNEReducer": [
                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/t-sne"
                    ],
                    "class LDATopicModeler": [
                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/lda",
                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/latent topics",
                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/topic distribution"
                    ]
                },
                "src/algorithms/unsupervised/clustering/k_means.py": {
                    "class KMeansInitializer": [
                        "Unsupervised Learning/Clustering/K-means Clustering/initialization methods",
                        "Unsupervised Learning/Clustering/K-means Clustering/centroid initialization",
                        "Unsupervised Learning/Clustering/K-means Clustering/k-means++ initialization"
                    ],
                    "class KMeansOptimizer": [
                        "Unsupervised Learning/Clustering/K-means Clustering/convergence criteria",
                        "Unsupervised Learning/Clustering/K-means Clustering/elbow method",
                        "Unsupervised Learning/Clustering/K-means Clustering/optimize clusters"
                    ],
                    "class KMeansClustering": [
                        "Unsupervised Learning/Clustering/K-means Clustering/k-means clustering"
                    ],
                    "class MiniBatchKMeansClustering": [
                        "Unsupervised Learning/Clustering/K-means Clustering/mini-batch k-means"
                    ],
                    "class SpectralClustering": [
                        "Unsupervised Learning/Clustering/K-means Clustering/spectral clustering"
                    ],
                    "class KMedoidsClustering": [
                        "Unsupervised Learning/Clustering/K-means Clustering/k-medoids"
                    ],
                    "class FuzzyKMeansClustering": [
                        "Unsupervised Learning/Clustering/K-means Clustering/fuzzy k-means"
                    ]
                },
                "src/algorithms/unsupervised/clustering/hierarchical_density.py": {
                    "class DensityBasedClustering": [
                        "Unsupervised Learning/Clustering/Hierarchical & Density Clustering/density-based clustering"
                    ],
                    "class AgglomerativeClustering": [
                        "Unsupervised Learning/Clustering/Hierarchical & Density Clustering/agglomerative"
                    ],
                    "class DivisiveClustering": [
                        "Unsupervised Learning/Clustering/Hierarchical & Density Clustering/divisive"
                    ]
                },
                "src/algorithms/unsupervised/anomaly_detection.py": {
                    "class OneClassSVMAnomalyDetector": [
                        "Unsupervised Learning/Anomaly and Self-Organizing/Anomaly Detection/one-class svm"
                    ],
                    "class IsolationForestAnomalyDetector": [
                        "Unsupervised Learning/Anomaly and Self-Organizing/Anomaly Detection/isolation forest"
                    ],
                    "class LocalOutlierFactorAnomalyDetector": [
                        "Unsupervised Learning/Anomaly and Self-Organizing/Anomaly Detection/local outlier factor"
                    ]
                },
                "src/algorithms/unsupervised/self_organizing_maps.py": {
                    "class SelfOrganizingMap": [
                        "Unsupervised Learning/Anomaly and Self-Organizing/Self-Organizing Maps/learning rate",
                        "Unsupervised Learning/Anomaly and Self-Organizing/Self-Organizing Maps/topology preservation"
                    ]
                },
                "src/algorithms/optimization/gradient_descent/adaptive_batch.py": {
                    "class AdagradOptimizer": [
                        "Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/adagrad"
                    ],
                    "class NesterovAcceleratedGradient": [
                        "Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/nesterov accelerated gd"
                    ],
                    "class RMSPropOptimizer": [
                        "Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/rmsprop"
                    ],
                    "class GDWithLineSearch": [
                        "Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/gd with line search"
                    ],
                    "class AdamOptimizer": [
                        "Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/adam optimizer"
                    ]
                },
                "src/algorithms/optimization/gradient_descent/stochastic.py": {
                    "class SGDWithMomentum": [
                        "Optimization Techniques/Gradient Descent Methods/Stochastic Methods/sgd with momentum"
                    ],
                    "class MiniBatchSGD": [
                        "Optimization Techniques/Gradient Descent Methods/Stochastic Methods/mini-batch sgd"
                    ]
                },
                "src/algorithms/optimization/svm_optimization.py": {
                    "class SVMStochasticGradientDescentOptimizer": [
                        "Optimization Techniques/SVM Optimization/Optimization Algorithms/stochastic gradient descent",
                        "Supervised Learning/Classification Algorithms/SVM Classification/stochastic gradient descent"
                    ],
                    "class SMOSVMOptimizer": [
                        "Optimization Techniques/SVM Optimization/Optimization Algorithms/smo algorithm",
                        "Supervised Learning/Classification Algorithms/SVM Classification/smo algorithm"
                    ]
                },
                "src/algorithms/tree_ensemble/decision_tree_methods.py": {
                    "function apply_reduced_error_pruning": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/reduced error pruning"
                    ],
                    "function apply_node_limited_pre_pruning": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/node-limited pre-pruning"
                    ],
                    "function apply_early_stopping": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/early stopping"
                    ],
                    "function apply_cost_complexity_pruning": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/cost complexity pruning"
                    ],
                    "function apply_depth_limitation": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/depth limitation"
                    ]
                },
                "src/algorithms/tree_ensemble/forest_voting_methods.py": {
                    "class VotingEnsemble": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/voting"
                    ],
                    "class GradientBoostingEnsemble": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/gradient boosting"
                    ],
                    "class MultiClassVotingClassifier": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/multi-class classification"
                    ],
                    "class RandomForestEnsemble": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/random forest"
                    ],
                    "class BaggingEnsemble": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/bagging"
                    ],
                    "class BinaryVotingClassifier": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/binary classification"
                    ],
                    "class QuantileRegressionForest": [
                        "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/quantile regression"
                    ]
                }
            }
        },
        "Workflow": {
            "files_order": [
                "src/workflow/preprocessing/feature_engineering/data_transformation.py",
                "src/workflow/preprocessing/feature_engineering/feature_extraction.py",
                "src/workflow/preprocessing/feature_engineering/data_augmentation.py",
                "src/workflow/preprocessing/normalization/z_score.py",
                "src/workflow/evaluation/accuracy.py",
                "src/workflow/evaluation/performance.py"
            ],
            "all_files_feature_map": {
                "src/workflow/preprocessing/feature_engineering/data_transformation.py": {
                    "function apply_onehot_encoding": [
                        "Preprocessing/Feature Engineering/Data Transformation/one-hot encoding"
                    ],
                    "function scale_and_normalize_features": [
                        "Preprocessing/Feature Engineering/Data Transformation/scaling and normalization"
                    ]
                },
                "src/workflow/preprocessing/feature_engineering/feature_extraction.py": {
                    "function extract_pca_features": [
                        "Preprocessing/Feature Engineering/Feature Extraction/pca"
                    ]
                },
                "src/workflow/preprocessing/feature_engineering/data_augmentation.py": {
                    "function augment_text_data": [
                        "Preprocessing/Feature Engineering/Data Augmentation/text augmentation"
                    ],
                    "function augment_image_data": [
                        "Preprocessing/Feature Engineering/Data Augmentation/image augmentation"
                    ]
                },
                "src/workflow/preprocessing/normalization/z_score.py": {
                    "function normalize_z_score": [
                        "Preprocessing/Normalization/Z-score Normalization/normalize to mean 0 and std 1"
                    ]
                },
                "src/workflow/evaluation/accuracy.py": {
                    "function compute_f1_score": [
                        "Evaluation/Accuracy Metrics/F1 Score/f1 score"
                    ]
                },
                "src/workflow/evaluation/performance.py": {
                    "function compute_confusion_matrix": [
                        "Evaluation/Performance Metrics/Confusion Matrix/confusion matrix"
                    ]
                }
            }
        },
        "Advanced Modeling Techniques": {
            "files_order": [
                "src/advanced_modeling/model_evaluation/roc_analysis.py",
                "src/advanced_modeling/model_evaluation/precision_recall.py",
                "src/advanced_modeling/model_evaluation/statistical_testing.py",
                "src/advanced_modeling/optimization/genetic_algorithms.py",
                "src/advanced_modeling/optimization/hyperparameter.py",
                "src/advanced_modeling/optimization/meta_learning.py"
            ],
            "all_files_feature_map": {
                "src/advanced_modeling/model_evaluation/roc_analysis.py": {
                    "function compute_multiclass_roc": [
                        "Model Evaluation/Comparative Evaluation/ROC Analysis/multi-class roc"
                    ],
                    "function compute_micro_average_roc": [
                        "Model Evaluation/Comparative Evaluation/ROC Analysis/micro-average roc"
                    ],
                    "function compute_precision_recall_curve": [
                        "Model Evaluation/Comparative Evaluation/ROC Analysis/precision-recall curve"
                    ],
                    "class ROCCurveVisualizer": [
                        "Model Evaluation/Comparative Evaluation/ROC Analysis/roc curve with threshold selection",
                        "Model Evaluation/Comparative Evaluation/ROC Analysis/roc curve plotting",
                        "Model Evaluation/Comparative Evaluation/ROC Analysis/roc curve with auc"
                    ]
                },
                "src/advanced_modeling/model_evaluation/precision_recall.py": {
                    "function plot_precision_recall_curve": [
                        "Model Evaluation/Comparative Evaluation/Precision-Recall Analysis/plot precision-recall curves"
                    ]
                },
                "src/advanced_modeling/model_evaluation/statistical_testing.py": {
                    "function two_tailed_t_test": [
                        "Model Evaluation/Comparative Evaluation/Statistical Testing/two-tailed t-test"
                    ],
                    "function one_tailed_t_test": [
                        "Model Evaluation/Comparative Evaluation/Statistical Testing/one-tailed test"
                    ],
                    "function paired_t_test_with_confidence_interval": [
                        "Model Evaluation/Comparative Evaluation/Statistical Testing/paired t-test with confidence interval"
                    ],
                    "function paired_t_test_multiple_metrics": [
                        "Model Evaluation/Comparative Evaluation/Statistical Testing/paired t-test with multiple metrics"
                    ],
                    "function interpret_test_results": [
                        "Model Evaluation/Comparative Evaluation/Statistical Testing/interpret results"
                    ],
                    "function visualize_paired_t_test_results": [
                        "Model Evaluation/Comparative Evaluation/Statistical Testing/visualize paired t-test results"
                    ],
                    "function test_statistical_assumptions": [
                        "Model Evaluation/Comparative Evaluation/Statistical Testing/test assumptions"
                    ]
                },
                "src/advanced_modeling/optimization/genetic_algorithms.py": {
                    "class GeneticAlgorithm": [
                        "Optimization Methods/Evolutionary Optimization/Genetic Algorithms/genetic algorithm with elitism",
                        "Optimization Methods/Evolutionary Optimization/Genetic Algorithms/crossover operation",
                        "Optimization Methods/Evolutionary Optimization/Genetic Algorithms/genetic algorithm with custom fitness function"
                    ]
                },
                "src/advanced_modeling/optimization/hyperparameter.py": {
                    "class HyperbandOptimizer": [
                        "Optimization Methods/Hyperparameter Optimization/Tuning Strategies/hyperband"
                    ]
                },
                "src/advanced_modeling/optimization/meta_learning.py": {
                    "class MetaLearningModel": [
                        "Optimization Methods/Ensemble Strategies/Meta-Learning/meta-learning"
                    ]
                }
            }
        }
    },
    "skeleton": {
        "root": {
            "type": "directory",
            "name": "scikit-learn",
            "path": ".",
            "children": [
                {
                    "type": "directory",
                    "name": "src",
                    "path": "src",
                    "children": [
                        {
                            "type": "directory",
                            "name": "algorithms",
                            "path": "src/algorithms",
                            "children": [
                                {
                                    "type": "directory",
                                    "name": "supervised",
                                    "path": "src/algorithms/supervised",
                                    "children": [
                                        {
                                            "type": "directory",
                                            "name": "regression",
                                            "path": "src/algorithms/supervised/regression",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "linear_regression.py",
                                                    "path": "src/algorithms/supervised/regression/linear_regression.py",
                                                    "code": "import pandas as pd\nfrom typing import Any\nfrom src.algorithms.base_algorithm import BaseAlgorithm\n\nclass LinearRegression(BaseAlgorithm):\n    \"\"\"\n    LinearRegression implements various linear regression techniques including:\n      - Simple linear regression: to model a relationship between one independent variable and the dependent variable.\n      - Multiple linear regression: to model the relationship between multiple independent variables and the dependent variable.\n      - Ordinary Least Squares (OLS): the standard method for estimating the parameters in linear regression.\n      - Generic linear regression behavior encapsulated as basic linear regression.\n    \n    This class leverages the ordinary least squares method to estimate coefficients. It can be configured to work for both simple and multiple regression scenarios.\n    \n    Attributes:\n        fit_intercept (bool): Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations.\n        coefficients (Any): Model coefficients computed during fitting.\n    \n    Methods:\n        fit(X: pd.DataFrame, y: pd.Series) -> 'LinearRegression':\n            Fit the linear regression model using the ordinary least squares method on the provided training data.\n            \n        predict(X: pd.DataFrame) -> pd.Series:\n            Predict the target values for the given input data based on the computed regression coefficients.\n    \n    Edge Cases and Assumptions:\n        - Assumes input features in X are numerical and properly preprocessed.\n        - The target y is expected to be continuous.\n        - May not handle singular matrices; preprocessing (e.g., feature selection) should be conducted to ensure numerical stability.\n    \"\"\"\n\n    def __init__(self, fit_intercept: bool=True) -> None:\n        \"\"\"\n        Initialize the LinearRegression model.\n\n        Args:\n            fit_intercept (bool): Flag indicating whether to calculate the intercept. Default is True.\n        \"\"\"\n        self.fit_intercept = fit_intercept\n        self.coefficients = None\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'LinearRegression':\n        \"\"\"\n        Fit the linear regression model using the ordinary least squares method.\n\n        Args:\n            X (pd.DataFrame): Training data with one or multiple features.\n            y (pd.Series): Continuous target values corresponding to X.\n\n        Returns:\n            LinearRegression: The fitted model instance with computed coefficients.\n        \n        Raises:\n            None: The method does not raise exceptions; input validation should be performed prior to calling.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict target values using the linear regression model.\n\n        Args:\n            X (pd.DataFrame): Data containing the same features as used during fitting.\n\n        Returns:\n            pd.Series: Predicted continuous target values.\n        \n        Raises:\n            None: It is assumed that the input data X is preprocessed similarly to the training data.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Supervised Learning/Regression Algorithms/Linear Regression/linear regression",
                                                        "Supervised Learning/Regression Algorithms/Linear Regression/multiple linear regression",
                                                        "Supervised Learning/Regression Algorithms/Linear Regression/ordinary least squares",
                                                        "Supervised Learning/Regression Algorithms/Linear Regression/simple linear regression"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "regularized_regression.py",
                                                    "path": "src/algorithms/supervised/regression/regularized_regression.py",
                                                    "code": "from algorithms.base_algorithm import BaseAlgorithm\nimport pandas as pd\nimport numpy as np\nfrom typing import List\n\nclass KernelRidgeRegression(BaseAlgorithm):\n    \"\"\"\n    Implements Kernel Ridge Regression for non-linear regression tasks.\n\n    This class integrates kernel methods with ridge regression to allow modeling\n    non-linear relationships. It inherently uses L2 regularization and supports the\n    use of various kernel functions for flexible modeling.\n\n    Args:\n        alpha (float): Regularization strength for L2 penalty.\n        kernel (str): Specifies the kernel type to be used (e.g., 'linear', 'rbf').\n        gamma (float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels.\n\n    Methods:\n        fit(X, y): Fit the Kernel Ridge Regression model on training data.\n        predict(X): Generate predictions using the fitted model.\n    \"\"\"\n\n    def __init__(self, alpha: float, kernel: str='rbf', gamma: float=1.0) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'KernelRidgeRegression':\n        \"\"\"\n        Fit the Kernel Ridge Regression model.\n\n        Args:\n            X (pd.DataFrame): Training data features.\n            y (pd.Series): Target values.\n\n        Returns:\n            KernelRidgeRegression: The fitted model instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict target values using the fitted Kernel Ridge Regression model.\n\n        Args:\n            X (pd.DataFrame): Data for which predictions are to be made.\n\n        Returns:\n            pd.Series: Predicted output values.\n        \"\"\"\n        pass\n\ndef logistic_regression_l1_penalty(coefficients: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the L1 regularization penalty for logistic regression.\n\n    This function computes the L1 penalty, defined as the sum of absolute\n    values of the coefficients scaled by the regularization parameter alpha.\n    It is specifically tailored for logistic regression models in a classification\n    setting.\n\n    Args:\n        coefficients (np.ndarray): Array of model coefficients.\n        alpha (float): Regularization strength parameter.\n\n    Returns:\n        float: The computed L1 penalty for logistic regression.\n\n    Edge Cases:\n        - Expects coefficients to be a non-empty numpy array.\n        - alpha should be a non-negative float.\n    \"\"\"\n    pass\n\ndef regularized_regression_feature_selection(X: pd.DataFrame, y: pd.Series, method: str='lasso') -> List[str]:\n    \"\"\"\n    Perform feature selection for regularized regression models.\n\n    This function applies a feature selection strategy, typically leveraging L1\n    regularization (e.g., LASSO), to identify a subset of important features\n    from the input dataset. The selected features aim to improve model performance\n    by reducing overfitting and enhancing interpretability.\n\n    Args:\n        X (pd.DataFrame): Input feature dataset.\n        y (pd.Series): Target variable.\n        method (str, optional): Feature selection method to apply. Defaults to \"lasso\".\n\n    Returns:\n        List[str]: A list of selected feature names.\n\n    Edge Cases:\n        - Assumes non-empty DataFrame and Series.\n        - The method parameter must be one of the supported feature selection strategies.\n    \"\"\"\n    pass\n\ndef l2_regularization_penalty(weights: np.ndarray, lambda_value: float) -> float:\n    \"\"\"\n    Compute the L2 regularization (ridge) penalty for a set of model weights.\n\n    This function calculates the L2 penalty as the sum of squared weights multiplied\n    by the regularization factor lambda_value. It is commonly used in regularized regression\n    to reduce overfitting by penalizing large coefficients.\n\n    Args:\n        weights (np.ndarray): Array of model weights.\n        lambda_value (float): Regularization factor (non-negative).\n\n    Returns:\n        float: The computed L2 regularization penalty.\n\n    Edge Cases:\n        - Assumes weights is a non-empty numpy array.\n        - lambda_value must be a non-negative float.\n    \"\"\"\n    pass\n\ndef regularized_regression_l1_penalty(weights: np.ndarray, lambda_value: float) -> float:\n    \"\"\"\n    Compute the L1 regularization penalty for regularized regression models.\n\n    This function computes the L1 penalty as the sum of the absolute values of the weights,\n    scaled by the regularization parameter lambda_value. This penalty is used to enforce sparsity\n    in regression models and is distinct from the logistic regression context.\n\n    Args:\n        weights (np.ndarray): Array of model weights.\n        lambda_value (float): Regularization parameter controlling the strength of the penalty.\n\n    Returns:\n        float: The calculated L1 regularization penalty.\n\n    Edge Cases:\n        - Expects weights to be a non-empty numpy array.\n        - lambda_value should be a non-negative float.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Supervised Learning/Classification Algorithms/Logistic Regression/l1 regularization",
                                                        "Supervised Learning/Regression Algorithms/Regularized Regression/feature selection",
                                                        "Supervised Learning/Regression Algorithms/Regularized Regression/kernel ridge regression",
                                                        "Supervised Learning/Regression Algorithms/Regularized Regression/l1 regularization",
                                                        "Supervised Learning/Regression Algorithms/Regularized Regression/l2 regularization"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "__init__.py",
                                                    "path": "src/algorithms/supervised/regression/__init__.py",
                                                    "code": "",
                                                    "feature_paths": [],
                                                    "units": []
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "classification",
                                            "path": "src/algorithms/supervised/classification",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "svm_classification.py",
                                                    "path": "src/algorithms/supervised/classification/svm_classification.py",
                                                    "code": "import pandas as pd\nfrom typing import Any\nfrom algorithms.base_algorithm import BaseAlgorithm\n\nclass KernelSVMClassifier(BaseAlgorithm):\n    \"\"\"\n    KernelSVMClassifier implements a support vector machine (SVM) classifier using kernel methods.\n    This classifier supports various kernel functions (e.g., linear, polynomial, rbf) to map input data\n    into higher-dimensional spaces for improved separation in cases where data is not linearly separable.\n\n    Attributes:\n        kernel (str): The kernel type to be used in the algorithm (e.g., 'linear', 'rbf', 'poly').\n        C (float): Regularization parameter. The strength of the regularization is inversely proportional to C.\n        gamma (str or float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. Can be 'scale', 'auto', or a numeric value.\n\n    Methods:\n        fit(X, y):\n            Fit the SVM classifier on the training data.\n        predict(X):\n            Predict the class labels for the provided data.\n    \"\"\"\n\n    def __init__(self, kernel: str='rbf', C: float=1.0, gamma: Any='scale') -> None:\n        \"\"\"\n        Initialize the KernelSVMClassifier with specified hyperparameters.\n\n        Args:\n            kernel (str): Specifies the kernel type to be used in the algorithm.\n            C (float): Regularization parameter. Lower values create a softer margin.\n            gamma (str or float): Kernel coefficient, controlling the influence of individual training examples.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'KernelSVMClassifier':\n        \"\"\"\n        Fit the SVM classifier on the provided training data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target class labels.\n\n        Returns:\n            KernelSVMClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for the input data using the fitted model.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n\n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n        pass\n\nclass HardMarginSVMClassifier(BaseAlgorithm):\n    \"\"\"\n    HardMarginSVMClassifier implements a variant of the support vector machine classifier\n    that assumes strict linear separability. This model enforces a hard margin, meaning no\n    misclassifications are allowed in the training data, resulting in no slack variables.\n\n    Attributes:\n        kernel (str): The kernel function used; typically 'linear' for hard-margin SVM.\n        C (float): Regularization parameter; for hard-margin, C is set to a very high value to enforce strict separation.\n    \n    Methods:\n        fit(X, y):\n            Train the hard-margin SVM classifier on provided training data.\n        predict(X):\n            Predict class labels for new data based on the hard-margin decision boundary.\n    \"\"\"\n\n    def __init__(self, kernel: str='linear', C: float=100000.0) -> None:\n        \"\"\"\n        Initialize the HardMarginSVMClassifier with strict margin parameters.\n        \n        Args:\n            kernel (str): Specifies the kernel type. The hard-margin version typically relies on a linear kernel.\n            C (float): A large regularization constant to enforce the hard margin constraint.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'HardMarginSVMClassifier':\n        \"\"\"\n        Fit the hard-margin SVM classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target class labels.\n\n        Returns:\n            HardMarginSVMClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for the input data using the hard-margin decision boundary.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n\n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Supervised Learning/Classification Algorithms/SVM Classification/hard-margin svm",
                                                        "Supervised Learning/Classification Algorithms/SVM Classification/kernel svm",
                                                        "Supervised Learning/Classification Algorithms/SVM Classification/support vector machines"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "decision_trees.py",
                                                    "path": "src/algorithms/supervised/classification/decision_trees.py",
                                                    "code": "from algorithms.base_algorithm import BaseAlgorithm\nimport pandas as pd\nfrom typing import Any\n\nclass DecisionTreeClassifier(BaseAlgorithm):\n    \"\"\"\n    DecisionTreeClassifier implements a decision tree based classifier that partitions the feature space\n    and assigns class labels based on learned decision rules.\n\n    This classifier recursively splits the input feature space based on criteria that maximize the\n    separation between classes. It is designed to handle both categorical and numerical features.\n    \n    Args:\n        max_depth (int, optional): The maximum depth of the tree. If None, the tree is expanded until leaves are pure.\n        min_samples_split (int): The minimum number of samples required to split an internal node.\n\n    Methods:\n        fit(pd.DataFrame, pd.Series) -> DecisionTreeClassifier:\n            Trains the decision tree classifier using the provided training data and corresponding labels.\n        predict(pd.DataFrame) -> pd.Series:\n            Predicts class labels for the given input data based on the learned decision tree.\n    \"\"\"\n\n    def __init__(self, max_depth: int=None, min_samples_split: int=2) -> None:\n        \"\"\"\n        Initialize the DecisionTreeClassifier with specific hyperparameters.\n\n        Args:\n            max_depth (int, optional): Maximum depth that the tree can grow. Use None for unlimited depth.\n            min_samples_split (int): Minimum number of samples required to split an internal node.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'DecisionTreeClassifier':\n        \"\"\"\n        Build the decision tree classifier from the training set.\n\n        Args:\n            X (pd.DataFrame): The input training features.\n            y (pd.Series): The target class labels.\n        \n        Returns:\n            DecisionTreeClassifier: The trained decision tree classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for the provided input data using the trained decision tree.\n\n        Args:\n            X (pd.DataFrame): Input features for which to predict class labels.\n        \n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Supervised Learning/Classification Algorithms/Decision Trees/decision trees"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "naive_bayes.py",
                                                    "path": "src/algorithms/supervised/classification/naive_bayes.py",
                                                    "code": "import pandas as pd\nfrom typing import Any\nimport numpy as np\nfrom algorithms.base_algorithm import BaseAlgorithm\n\nclass NaiveBayesClassifier(BaseAlgorithm):\n    \"\"\"\n    NaiveBayesClassifier implements the Naive Bayes algorithm for classification tasks.\n    \n    This classifier uses Bayes' theorem with the assumption of strong (naive) independence between features.\n    It calculates prior probabilities and likelihoods based on the training data, and employs these to predict\n    the class labels for new data instances.\n\n    Attributes:\n        smoothing (float): The Laplace smoothing parameter to ensure robustness against zero-frequency issues \n                           in likelihood estimates.\n\n    Methods:\n        fit(X, y): Estimates the prior probabilities and feature likelihoods from the training data.\n        predict(X): Predicts class labels for new input data based on the computed probabilities.\n    \"\"\"\n\n    def __init__(self, smoothing: float=1.0) -> None:\n        \"\"\"\n        Initialize the NaiveBayesClassifier with a smoothing parameter.\n\n        Args:\n            smoothing (float, optional): Laplace smoothing constant to handle zero probabilities. Defaults to 1.0.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'NaiveBayesClassifier':\n        \"\"\"\n        Fit the Naive Bayes classifier by computing the class prior probabilities and feature likelihoods from training data.\n\n        Args:\n            X (pd.DataFrame): A DataFrame containing the training features.\n            y (pd.Series): A Series containing the training class labels.\n\n        Returns:\n            NaiveBayesClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for given input data using the Naive Bayes probabilistic model.\n\n        Args:\n            X (pd.DataFrame): A DataFrame containing the features for which to predict class labels.\n\n        Returns:\n            pd.Series: A Series containing the predicted class labels.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Supervised Learning/Classification Algorithms/Naive Bayes/naive bayes"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "logistic_regression.py",
                                                    "path": "src/algorithms/supervised/classification/logistic_regression.py",
                                                    "code": "from typing import Optional\nfrom algorithms.base_algorithm import BaseAlgorithm\nimport pandas as pd\nfrom typing import Any\nimport numpy as np\n\nclass LogisticRegressionClassifier(BaseAlgorithm):\n    \"\"\"\n    LogisticRegressionClassifier implements the basic logistic regression algorithm.\n    \n    This classifier provides methods to train a logistic regression model on provided data\n    and to generate predictions. It encapsulates the standard logistic regression functionality,\n    addressing binary classification tasks.\n\n    Methods:\n        __init__(self, penalty: str = 'l2', C: float = 1.0) -> None:\n            Initializes the classifier with regularization parameters.\n        \n        fit(self, X: pd.DataFrame, y: pd.Series) -> 'LogisticRegressionClassifier':\n            Fits the logistic regression model to the training data.\n        \n        predict(self, X: pd.DataFrame) -> pd.Series:\n            Predicts class labels for the input data.\n    \"\"\"\n\n    def __init__(self, penalty: str='l2', C: float=1.0) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'LogisticRegressionClassifier':\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        pass\n\nclass MultinomialLogisticRegressionClassifier(BaseAlgorithm):\n    \"\"\"\n    MultinomialLogisticRegressionClassifier extends logistic regression for multiclass problems.\n    \n    This classifier is designed to handle multinomial (multiclass) logistic regression tasks\n    where the response variable can take more than two classes. It incorporates methods to fit\n    the model and predict class probabilities.\n\n    Methods:\n        __init__(self, penalty: str = 'l2', C: float = 1.0) -> None:\n            Initializes the classifier with parameters suitable for multinomial regression.\n        \n        fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultinomialLogisticRegressionClassifier':\n            Trains the model using the provided training data.\n        \n        predict(self, X: pd.DataFrame) -> pd.Series:\n            Generates predictions for the input data.\n    \"\"\"\n\n    def __init__(self, penalty: str='l2', C: float=1.0) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultinomialLogisticRegressionClassifier':\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        pass\n\nclass ProbitRegressionClassifier(BaseAlgorithm):\n    \"\"\"\n    ProbitRegressionClassifier implements the probit regression model.\n    \n    This classifier uses the cumulative distribution function of the standard normal distribution\n    as the link function. It is useful in scenarios where the assumption of logistic regression may not hold.\n\n    Methods:\n        __init__(self) -> None:\n            Initializes the probit regression classifier.\n        \n        fit(self, X: pd.DataFrame, y: pd.Series) -> 'ProbitRegressionClassifier':\n            Fits the probit regression model to the training data.\n        \n        predict(self, X: pd.DataFrame) -> pd.Series:\n            Generates predictions based on the fitted model.\n    \"\"\"\n\n    def __init__(self) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'ProbitRegressionClassifier':\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        pass\n\ndef softmax(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the softmax function for each row of the input array.\n    \n    The softmax function is used to convert raw model output scores into probabilities that sum to 1.\n    This is particularly useful for multiclass classification problems.\n\n    Args:\n        z (np.ndarray): A 2D array of shape (n_samples, n_classes) containing raw scores.\n\n    Returns:\n        np.ndarray: A 2D array of the same shape as z, where each row represents a probability distribution\n                    over the classes.\n\n    Edge Cases:\n        - If the input is empty, returns an empty array.\n        - Numerical stability should be ensured by subtracting the max value in each row before exponentiation.\n    \"\"\"\n    pass\n\ndef compute_cross_entropy_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Computes the cross-entropy loss between true labels and predicted probabilities.\n    \n    Cross-entropy loss is commonly used as a cost function for classification models,\n    particularly in logistic and multinomial logistic regression.\n\n    Args:\n        y_true (np.ndarray): Array of true labels (one-hot encoded or as indices).\n        y_pred (np.ndarray): Array of predicted probabilities for each class.\n\n    Returns:\n        float: The computed cross-entropy loss value.\n\n    Edge Cases:\n        - Handles cases where predicted probabilities might be zero by clipping values.\n        - Assumes y_true and y_pred have compatible shapes.\n    \"\"\"\n    pass\n\ndef compute_log_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Computes the log loss (negative log-likelihood) for classification predictions.\n    \n    Log loss is equivalent to cross-entropy loss and is used to evaluate the performance of\n    classifiers built using probabilistic approaches like logistic regression.\n\n    Args:\n        y_true (np.ndarray): Array of true binary or categorical labels (appropriately encoded).\n        y_pred (np.ndarray): Array of predicted probabilities for the positive class or each class.\n\n    Returns:\n        float: The calculated log loss value.\n\n    Edge Cases:\n        - Applies clipping to y_pred to prevent taking log of zero.\n        - Assumes the input arrays are of compatible sizes.\n    \"\"\"\n    pass\n\ndef optimize_threshold(y_true: np.ndarray, y_scores: np.ndarray, metric: str='f1') -> float:\n    \"\"\"\n    Optimizes the classification threshold based on a given performance metric.\n    \n    This function iterates over possible threshold values to determine the threshold that maximizes \n    the specified evaluation metric (e.g., F1 score). It is particularly useful in logistic regression,\n    where adjusting the threshold can balance precision and recall.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels.\n        y_scores (np.ndarray): Array of predicted scores or probabilities.\n        metric (str, optional): The performance metric to optimize. Default is \"f1\".\n\n    Returns:\n        float: The optimal threshold value that maximizes the specified metric.\n\n    Edge Cases:\n        - If y_scores is empty, the function should handle the error gracefully.\n        - Assumes that y_true and y_scores are aligned and of equal length.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Supervised Learning/Classification Algorithms/Logistic Regression/cross-entropy loss",
                                                        "Supervised Learning/Classification Algorithms/Logistic Regression/log loss",
                                                        "Supervised Learning/Classification Algorithms/Logistic Regression/logistic regression",
                                                        "Supervised Learning/Classification Algorithms/Logistic Regression/multinomial logistic regression",
                                                        "Supervised Learning/Classification Algorithms/Logistic Regression/probit regression",
                                                        "Supervised Learning/Classification Algorithms/Logistic Regression/softmax function",
                                                        "Supervised Learning/Classification Algorithms/Logistic Regression/threshold optimization"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "imbalanced_classification.py",
                                                    "path": "src/algorithms/supervised/classification/imbalanced_classification.py",
                                                    "code": "from algorithms.base_algorithm import BaseAlgorithm\nimport pandas as pd\n\nclass ImbalancedClassifier(BaseAlgorithm):\n    \"\"\"\n    A classifier tailored to address imbalanced classification problems.\n\n    This classifier implements the core methods for fitting and predicting on datasets\n    where class distribution is skewed. It inherits from the BaseAlgorithm and integrates\n    seamlessly with the overall machine learning framework. The classifier allows for the\n    specification of a strategy to handle class imbalances (e.g., undersampling, oversampling,\n    or synthetic sample generation).\n\n    Args:\n        strategy (str): The strategy used to mitigate class imbalance. Common options include\n                        'undersample', 'oversample', or 'smote'. Defaults to 'undersample'.\n\n    Methods:\n        __init__(strategy: str = 'undersample') -> None:\n            Initializes the classifier with the given imbalance handling strategy.\n        \n        fit(X: pd.DataFrame, y: pd.Series) -> ImbalancedClassifier:\n            Trains the classifier on the provided training data and labels.\n        \n        predict(X: pd.DataFrame) -> pd.Series:\n            Generates predictions for the specified input features.\n\n    Returns:\n        An instance of the ImbalancedClassifier after fitting the model.\n\n    Assumptions and Edge Cases:\n        - Assumes preprocessed input where missing data and data scaling have been addressed.\n        - The actual internal logic for handling imbalanced data is not implemented here.\n    \"\"\"\n\n    def __init__(self, strategy: str='undersample') -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'ImbalancedClassifier':\n        \"\"\"\n        Train the imbalanced classifier on the provided dataset.\n\n        Args:\n            X (pd.DataFrame): The feature set used for training.\n            y (pd.Series): The target labels corresponding to the training data.\n\n        Returns:\n            ImbalancedClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions from the imbalanced classifier for the provided input data.\n\n        Args:\n            X (pd.DataFrame): The feature set for which predictions are to be made.\n\n        Returns:\n            pd.Series: The predicted class labels.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Supervised Learning/Classification Algorithms/Imbalanced Classification/imbalanced classification"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "multi_label.py",
                                                    "path": "src/algorithms/supervised/classification/multi_label.py",
                                                    "code": "import numpy as np\nfrom typing import Any, Optional\nfrom typing import Any, List\nfrom algorithms.base_algorithm import BaseAlgorithm\nimport pandas as pd\nfrom typing import Any\n\nclass MultiLabelKNNClassifier(BaseAlgorithm):\n    \"\"\"\n    MultiLabelKNNClassifier implements a k-nearest neighbors method tailored for multi-label classification.\n\n    This classifier applies the KNN algorithm in scenarios where each instance may be associated with multiple labels.\n    It computes distances between instances and aggregates the labels from the k nearest neighbors to derive multi-label predictions.\n\n    Args:\n        k (int): Number of nearest neighbors to consider.\n        metric (str): Distance metric to be used (e.g., 'euclidean', 'manhattan'). Defaults to 'euclidean'.\n\n    Methods:\n        fit(X, y): Fit the classifier using the training data.\n        predict(X): Predict multi-label targets for the provided input data.\n\n    Attributes:\n        trained (bool): Indicates whether the classifier has been fitted.\n    \"\"\"\n\n    def __init__(self, k: int=5, metric: str='euclidean') -> None:\n        self.k = k\n        self.metric = metric\n        self.trained = False\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'MultiLabelKNNClassifier':\n        \"\"\"\n        Fit the multi-label KNN classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe used for training.\n            y (pd.DataFrame): Dataframe containing multi-label targets.\n\n        Returns:\n            MultiLabelKNNClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Predict multi-label targets for new data instances.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for which predictions are to be made.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the predicted multi-label outputs.\n        \"\"\"\n        pass\n\nclass ClassifierChainsClassifier(BaseAlgorithm):\n    \"\"\"\n    ClassifierChainsClassifier implements the classifier chains approach for multi-label classification.\n\n    This technique builds a chain of binary classifiers where each classifier predicts one label while taking into\n    account the predictions of previous classifiers in the chain, enabling the capture of label dependencies.\n\n    Args:\n        base_estimator (Any): The base classifier to be used for each link in the chain.\n        order (List[int], optional): A list defining the order in which labels are predicted. If None, a default order is applied.\n\n    Methods:\n        fit(X, y): Train the chain of classifiers on the training data.\n        predict(X): Generate multi-label predictions by processing the input sequentially through the chain.\n\n    Attributes:\n        chain (List[Any]): List of individual classifiers fitted in the chain.\n    \"\"\"\n\n    def __init__(self, base_estimator: Any, order: List[int]=None) -> None:\n        self.base_estimator = base_estimator\n        self.order = order\n        self.chain = []\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'ClassifierChainsClassifier':\n        \"\"\"\n        Fit the classifier chain using the provided training data.\n\n        Args:\n            X (pd.DataFrame): Feature set for training.\n            y (pd.DataFrame): Dataframe of multi-label targets.\n\n        Returns:\n            ClassifierChainsClassifier: The fitted classifier chain instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Predict multi-label outputs by applying the classifier chain to the input data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for prediction.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the multi-label predictions.\n        \"\"\"\n        pass\n\nclass MultiLabelDecisionTreeClassifier(BaseAlgorithm):\n    \"\"\"\n    MultiLabelDecisionTreeClassifier implements a decision tree approach adapted for multi-label classification.\n\n    This classifier extends traditional decision tree methodologies to handle cases where instances may belong to multiple\n    classes. It adjusts split criteria and node decisions to accommodate the simultaneous prediction of multiple labels.\n\n    Args:\n        max_depth (Optional[int]): Maximum depth of the decision tree. If None, the tree is expanded until all leaves are pure.\n        min_samples_split (int): Minimum number of samples required to split an internal node. Defaults to 2.\n\n    Methods:\n        fit(X, y): Train the multi-label decision tree using the provided training data.\n        predict(X): Predict multi-label targets for new data instances.\n\n    Attributes:\n        tree_structure (Any): An internal representation of the constructed decision tree.\n    \"\"\"\n\n    def __init__(self, max_depth: Optional[int]=None, min_samples_split: int=2) -> None:\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.tree_structure = None\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'MultiLabelDecisionTreeClassifier':\n        \"\"\"\n        Fit the multi-label decision tree classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for training.\n            y (pd.DataFrame): Dataframe containing multi-label target values.\n\n        Returns:\n            MultiLabelDecisionTreeClassifier: The fitted decision tree classifier.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Predict multi-label outcomes for new data using the trained decision tree.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for which predictions are required.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the predicted multi-label outputs.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Supervised Learning/Classification Algorithms/Multi-label Classification/classifier chains",
                                                        "Supervised Learning/Classification Algorithms/Multi-label Classification/multi-label decision trees",
                                                        "Supervised Learning/Classification Algorithms/Multi-label Classification/multi-label knn"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "__init__.py",
                                                    "path": "src/algorithms/supervised/classification/__init__.py",
                                                    "code": "",
                                                    "feature_paths": [],
                                                    "units": []
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "semi_supervised.py",
                                            "path": "src/algorithms/supervised/semi_supervised.py",
                                            "code": "import pandas as pd\nfrom typing import Any\nfrom algorithms.base_algorithm import BaseAlgorithm\n\nclass CoTrainingClassifier(BaseAlgorithm):\n    \"\"\"\n    A classifier implementing the co-training semi-supervised learning technique.\n\n    This classifier trains two separate models on different views of the labeled data and \n    then iteratively refines each by pseudo-labeling the unlabeled data. The two models \n    are expected to complement each other, leveraging unlabeled samples to enhance learning.\n\n    Methods:\n        __init__(...): Initialize the classifier with hyperparameters and base learners.\n        fit(X, y): Fit the classifier on labeled (and optionally unlabeled) data.\n        predict(X): Generate predictions for the input data.\n\n    Args:\n        base_estimator1 (Any): The first base classifier instance.\n        base_estimator2 (Any): The second base classifier instance.\n        unlabeled_weight (float): Weighting factor for unlabeled data.\n        max_iter (int): Maximum number of iterations for the co-training process.\n\n    Returns:\n        CoTrainingClassifier: An instance of the co-training classifier after fitting.\n    \n    Edge Cases:\n        - Assumes that input data is a pandas DataFrame.\n        - The classifier expects valid and compatible base estimators.\n    \"\"\"\n\n    def __init__(self, base_estimator1: Any, base_estimator2: Any, unlabeled_weight: float=1.0, max_iter: int=10) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'CoTrainingClassifier':\n        \"\"\"\n        Fit the co-training classifier using labeled and unlabeled data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target labels for the labeled data.\n\n        Returns:\n            CoTrainingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions for the given input data.\n\n        Args:\n            X (pd.DataFrame): The data for which predictions are required.\n\n        Returns:\n            pd.Series: The predicted labels.\n        \"\"\"\n        pass\n\nclass SelfTrainingClassifier(BaseAlgorithm):\n    \"\"\"\n    A classifier implementing the self-training semi-supervised learning technique.\n\n    This classifier starts with a base estimator trained on labeled data and then iteratively \n    pseudo-labels the unlabeled data to expand the training set. The self-training mechanism \n    relies on the classifier's confidence to augment the labeled set over several iterations.\n\n    Methods:\n        __init__(...): Initialize the self-training classifier with a base estimator and parameters.\n        fit(X, y): Fit the classifier, leveraging a combination of labeled and pseudo-labeled data.\n        predict(X): Produce predictions using the trained classifier.\n\n    Args:\n        base_estimator (Any): An instance of a base classifier to be used for training.\n        confidence_threshold (float): The threshold above which predictions are considered reliable for pseudo-labeling.\n        max_iter (int): The maximum number of iterations for the self-training process.\n\n    Returns:\n        SelfTrainingClassifier: An instance of the self-training classifier after fitting.\n    \n    Edge Cases:\n        - Assumes input data is a pandas DataFrame.\n        - Relies on the base estimator having a predict_proba method for confidence estimation (if applicable).\n    \"\"\"\n\n    def __init__(self, base_estimator: Any, confidence_threshold: float=0.8, max_iter: int=10) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'SelfTrainingClassifier':\n        \"\"\"\n        Fit the self-training classifier using labeled and unlabeled data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target labels for the labeled portion of the data.\n\n        Returns:\n            SelfTrainingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions for the provided data.\n\n        Args:\n            X (pd.DataFrame): The data for which the predictions are required.\n\n        Returns:\n            pd.Series: The predicted labels.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Supervised Learning/Semi-Supervised Learning/Semi-Supervised Techniques/co-training",
                                                "Supervised Learning/Semi-Supervised Learning/Semi-Supervised Techniques/self-training"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "unclassified_method.py",
                                            "path": "src/algorithms/supervised/unclassified_method.py",
                                            "code": "import pandas as pd\nfrom src.algorithms.base_algorithm import BaseAlgorithm\n\nclass UnclassifiedMethod(BaseAlgorithm):\n    \"\"\"\n    A placeholder class for implementing a miscellaneous supervised learning method (feature '5').\n    \n    This class serves as an unclassified algorithm for supervised learning that does not\n    fit into traditional algorithm categories. It implements the basic estimator interface\n    from BaseAlgorithm, including methods for fitting to training data and making predictions.\n    The specific logic for this method should be provided by further implementation.\n    \n    Methods:\n        __init__(*args, **kwargs):\n            Initializes the algorithm with optional parameters.\n        \n        fit(X: pd.DataFrame, y: pd.Series) -> 'UnclassifiedMethod':\n            Trains the algorithm on the given training data and labels.\n        \n        predict(X: pd.DataFrame) -> pd.Series:\n            Predicts target values for the given input features.\n    \n    Args:\n        *args: Variable length argument list for initialization parameters.\n        **kwargs: Arbitrary keyword arguments for configuration.\n    \n    Returns:\n        UnclassifiedMethod: An instance of the fitted algorithm after calling fit.\n    \n    Raises:\n        NotImplementedError: The methods are not implemented and will raise an error if called.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the UnclassifiedMethod algorithm with optional parameters.\n        \n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments for algorithm configuration.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'UnclassifiedMethod':\n        \"\"\"\n        Fit the unclassified method on the training data.\n        \n        Args:\n            X (pd.DataFrame): Training data features.\n            y (pd.Series): True target values for training.\n        \n        Returns:\n            UnclassifiedMethod: The fitted model instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions from the fitted unclassified method.\n        \n        Args:\n            X (pd.DataFrame): Input data features for which predictions are to be made.\n        \n        Returns:\n            pd.Series: Predicted target values.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Supervised Learning/Additional Supervised Methods/Miscellaneous/5"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/algorithms/supervised/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "unsupervised",
                                    "path": "src/algorithms/unsupervised",
                                    "children": [
                                        {
                                            "type": "directory",
                                            "name": "dimensionality_reduction",
                                            "path": "src/algorithms/unsupervised/dimensionality_reduction",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "pca_methods.py",
                                                    "path": "src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                    "code": "from typing import Optional\nfrom typing import Optional, Iterator\nimport pandas as pd\nfrom general.base import BaseTransformer\n\nclass KernelPCA(BaseTransformer):\n    \"\"\"\n    Perform Kernel Principal Component Analysis (Kernel PCA) for nonlinear dimensionality reduction.\n\n    This interface implements PCA using kernel methods to project data into a higher-dimensional feature space,\n    allowing for the extraction of nonlinear patterns. The transform method applies the learned projection\n    on new data.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'KernelPCA':\n            Fit the Kernel PCA model to the input data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Apply the kernel projection to reduce the dimensionality of X.\n\n    Args:\n        kernel (str): The kernel type to be used (e.g., 'rbf', 'poly', etc.).\n        n_components (int): Number of principal components to extract.\n        **kernel_params: Additional parameters for the kernel function.\n    \"\"\"\n\n    def __init__(self, kernel: str='rbf', n_components: int=2, **kernel_params):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'KernelPCA':\n        \"\"\"\n        Fit the Kernel PCA model using the given data.\n\n        Args:\n            X (pd.DataFrame): The input data for training.\n            y (Optional[pd.Series]): Not used, included for compatibility.\n\n        Returns:\n            KernelPCA: The fitted KernelPCA instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Transform the input data using the kernel PCA mapping.\n\n        Args:\n            X (pd.DataFrame): New input data to transform.\n\n        Returns:\n            pd.DataFrame: The data represented in the reduced dimensional space.\n        \"\"\"\n        pass\n\nclass StandardPCA(BaseTransformer):\n    \"\"\"\n    Perform standard Principal Component Analysis (PCA) using eigen or singular value decomposition.\n\n    This interface implements the classical PCA algorithm for dimensionality reduction. It computes the principal\n    components of the data via eigen decomposition or singular value decomposition, depending on the properties of\n    the input matrix. The resulting components can be used to reduce the dimensionality while preserving variance.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'StandardPCA':\n            Compute the principal components from the input data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Project new data onto the principal component space.\n\n    Args:\n        n_components (int): The number of principal components to compute.\n        method (str): The decomposition method ('eigen' or 'svd') to use; the selection may affect numerical stability.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, method: str='svd'):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'StandardPCA':\n        \"\"\"\n        Fit the PCA model to the training data using the specified decomposition method.\n\n        Args:\n            X (pd.DataFrame): Training data for PCA.\n            y (Optional[pd.Series]): Not used; included for API consistency.\n\n        Returns:\n            StandardPCA: The fitted PCA instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Apply the PCA transformation to reduce the dimensionality of the input data.\n\n        Args:\n            X (pd.DataFrame): Data to transform.\n\n        Returns:\n            pd.DataFrame: Data projected onto the principal components.\n        \"\"\"\n        pass\n\nclass SparsePCA(BaseTransformer):\n    \"\"\"\n    Perform Sparse Principal Component Analysis (Sparse PCA) for dimensionality reduction with sparsity constraints.\n\n    This interface implements Sparse PCA where the principal components are obtained under a sparsity constraint,\n    resulting in components with many zero loadings. This can enhance interpretability in high-dimensional data.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'SparsePCA':\n            Fit the Sparse PCA model using the input data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Transform the data into a sparse principal component space.\n\n    Args:\n        n_components (int): The number of sparse principal components to compute.\n        alpha (float): Sparsity controlling parameter, where a higher value leads to sparser components.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, alpha: float=1.0):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'SparsePCA':\n        \"\"\"\n        Fit the Sparse PCA model to the data.\n\n        Args:\n            X (pd.DataFrame): Input training data.\n            y (Optional[pd.Series]): Not used, for interface compatibility only.\n\n        Returns:\n            SparsePCA: The fitted SparsePCA instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Transform input data into the space spanned by the sparse principal components.\n\n        Args:\n            X (pd.DataFrame): New data to transform.\n\n        Returns:\n            pd.DataFrame: Data represented in the sparse principal component space.\n        \"\"\"\n        pass\n\nclass IncrementalPCA(BaseTransformer):\n    \"\"\"\n    Perform Incremental Principal Component Analysis (Incremental PCA) for large datasets that cannot be processed in memory.\n\n    This interface implements Incremental PCA which allows partial fitting of the model on mini-batches of data,\n    making it suitable for large datasets. The transformation yields principal components that approximate the standard PCA.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'IncrementalPCA':\n            Incrementally fit the PCA model on the input data.\n        partial_fit(X: pd.DataFrame) -> None:\n            Update the PCA model with a mini-batch of data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Transform new data using the incrementally learned principal components.\n\n    Args:\n        n_components (int): The number of principal components to extract incrementally.\n        batch_size (int): Size of the mini-batches to use during incremental fitting.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, batch_size: int=100):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'IncrementalPCA':\n        \"\"\"\n        Fit the Incremental PCA model using the data, processing it in mini-batches.\n\n        Args:\n            X (pd.DataFrame): Input data for incremental fitting.\n            y (Optional[pd.Series]): Not used; provided for compatibility.\n\n        Returns:\n            IncrementalPCA: The fitted IncrementalPCA model.\n        \"\"\"\n        pass\n\n    def partial_fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Update the PCA model with a mini-batch of data.\n\n        Args:\n            X (pd.DataFrame): A mini-batch of input data.\n\n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Transform the input data using the incrementally accumulated principal components.\n\n        Args:\n            X (pd.DataFrame): New input data to be transformed.\n\n        Returns:\n            pd.DataFrame: Data represented in the reduced dimensional space.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/dimensionality reduction",
                                                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/eigen decomposition",
                                                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/incremental pca",
                                                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/kernel pca",
                                                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/singular value decomposition",
                                                        "Unsupervised Learning/Dimensionality Reduction/PCA Methods/sparse pca"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "other_methods.py",
                                                    "path": "src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                    "code": "import pandas as pd\nimport numpy as np\nfrom typing import Union, Optional\n\nclass UMAPReducer:\n    \"\"\"\n    UMAPReducer performs dimensionality reduction using the UMAP algorithm.\n    \n    This class implements the UMAP algorithm with configurable minimum distance and number of neighbors.\n    It is used to project high-dimensional data into a lower-dimensional space while preserving the\n    underlying structure according to the parameters provided.\n\n    Attributes:\n        min_dist (float): The minimum distance between embedded points.\n        n_neighbors (int): The number of neighboring points used to balance local versus global structure.\n    \"\"\"\n\n    def __init__(self, min_dist: float=0.1, n_neighbors: int=15) -> None:\n        \"\"\"\n        Initialize the UMAP reducer with specified parameters.\n\n        Args:\n            min_dist (float): The minimum distance between points in the embedded space.\n            n_neighbors (int): The number of neighbors to consider for each point.\n        \"\"\"\n        self.min_dist = min_dist\n        self.n_neighbors = n_neighbors\n        pass\n\n    def reduce(self, data: Union[pd.DataFrame, np.ndarray], n_components: int=2) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Reduce the dimensionality of input data using UMAP.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): The high-dimensional input data.\n            n_components (int): The target number of dimensions. Defaults to 2.\n\n        Returns:\n            pd.DataFrame or np.ndarray: The dimensionally-reduced representation of the input data.\n        \"\"\"\n        pass\n\nclass TSNEReducer:\n    \"\"\"\n    TSNEReducer performs dimensionality reduction using t-SNE.\n\n    This class encapsulates the t-SNE algorithm for mapping high-dimensional data to a lower-dimensional space.\n    It helps in visualizing clusters and patterns by minimizing the divergence between the high-dimensional and\n    low-dimensional distributions.\n\n    Attributes:\n        n_components (int): The target number of dimensions.\n        perplexity (float): The perplexity parameter balances attention between local and global aspects.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, perplexity: float=30.0) -> None:\n        \"\"\"\n        Initialize the t-SNE reducer with provided parameters.\n\n        Args:\n            n_components (int): The number of dimensions for the output space.\n            perplexity (float): The perplexity parameter affecting the balance of local and global structure.\n        \"\"\"\n        self.n_components = n_components\n        self.perplexity = perplexity\n        pass\n\n    def reduce(self, data: Union[pd.DataFrame, np.ndarray]) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Reduce the dimensionality of input data using t-SNE.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): The high-dimensional data to be reduced.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Low-dimensional representation of the input data.\n        \"\"\"\n        pass\n\nclass LDATopicModeler:\n    \"\"\"\n    LDATopicModeler performs topic modeling using Latent Dirichlet Allocation (LDA).\n\n    This class applies LDA to extract latent topics from a corpus or a feature data matrix and\n    provides functionality to retrieve both the latent topics and the topic distribution across\n    documents/samples. It is primarily used for uncovering underlying topics in text or feature data.\n\n    Attributes:\n        n_topics (int): The number of latent topics to extract.\n        random_state (Optional[int]): Seed used by the random number generator.\n    \"\"\"\n\n    def __init__(self, n_topics: int=10, random_state: Optional[int]=None) -> None:\n        \"\"\"\n        Initialize the LDA modeler with specified parameters.\n\n        Args:\n            n_topics (int): The number of topics to extract.\n            random_state (Optional[int]): Seed for reproducibility.\n        \"\"\"\n        self.n_topics = n_topics\n        self.random_state = random_state\n        pass\n\n    def fit(self, data: Union[pd.DataFrame, np.ndarray]) -> None:\n        \"\"\"\n        Fit the LDA model on the given data.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): Data matrix where rows correspond to samples/documents\n              and columns correspond to features (e.g., word counts or TF-IDF values).\n        \"\"\"\n        pass\n\n    def get_latent_topics(self) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Retrieve the latent topic representation of the training data.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Matrix where each row corresponds to the topic distribution for a sample.\n        \"\"\"\n        pass\n\n    def get_topic_distribution(self) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Obtain the topic distribution over the vocabulary.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Matrix where each row contains the distribution of words for a topic.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/latent topics",
                                                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/lda",
                                                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/min_dist parameter",
                                                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/number of neighbors",
                                                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/t-sne",
                                                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/topic distribution",
                                                        "Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/umap"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "__init__.py",
                                                    "path": "src/algorithms/unsupervised/dimensionality_reduction/__init__.py",
                                                    "code": "",
                                                    "feature_paths": [],
                                                    "units": []
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "clustering",
                                            "path": "src/algorithms/unsupervised/clustering",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "k_means.py",
                                                    "path": "src/algorithms/unsupervised/clustering/k_means.py",
                                                    "code": "from typing import Any, Dict\nimport numpy as np\nfrom typing import Any, List\nfrom typing import Any\n\nclass KMeansInitializer:\n    \"\"\"\n    Provides various centroid initialization methods for K-means clustering.\n    \n    This class encapsulates methods for initializing cluster centroids using different strategies including:\n    - Generic initialization methods\n    - Direct centroid initialization\n    - K-means++ initialization for improved convergence.\n    \n    Methods should be used to set up the initial state for clustering algorithms by selecting starting centroids.\n    \n    Usage:\n        initializer = KMeansInitializer()\n        centroids = initializer.initialize(data, method=\"k-means++\", num_clusters=3)\n    \n    Args:\n        None\n    \n    Returns:\n        Various initialization outputs as numpy.ndarray or list of centroids.\n    \n    Edge Cases:\n        Implementations should handle cases where the input data is insufficient or contains NaN values.\n    \"\"\"\n\n    def initialize(self, data: np.ndarray, method: str='default', num_clusters: int=3) -> Any:\n        \"\"\"\n        Initialize cluster centroids for k-means clustering.\n        \n        Args:\n            data (np.ndarray): The input data array for clustering.\n            method (str): The initialization method (\"default\", \"centroid\", \"k-means++\").\n            num_clusters (int): Number of clusters for which to initialize centroids.\n            \n        Returns:\n            Any: The initialized centroids (format can vary based on method).\n        \"\"\"\n        pass\n\nclass KMeansOptimizer:\n    \"\"\"\n    Provides utilities to assess convergence, evaluate the elbow method, and optimize the number of clusters \n    for k-means based clustering algorithms.\n    \n    This class offers methods to:\n    - Check convergence criteria during iterative updates.\n    - Compute the elbow metric to help in identifying the optimal number of clusters.\n    - Optimize the cluster count automatically based on provided metrics.\n    \n    Usage:\n        optimizer = KMeansOptimizer()\n        is_converged = optimizer.check_convergence(old_centroids, new_centroids, tol=0.001)\n        elbow_score = optimizer.compute_elbow_method(data, max_k=10)\n        optimal_k = optimizer.optimize_clusters(data, max_k=10)\n    \n    Args:\n        None\n    \n    Returns:\n        Convergence flags, elbow scores, or optimal cluster counts as integers or floats.\n    \n    Edge Cases:\n        Methods should effectively handle edge cases such as not reaching convergence within a fixed number of iterations.\n    \"\"\"\n\n    def check_convergence(self, old_centroids: np.ndarray, new_centroids: np.ndarray, tol: float=0.001) -> bool:\n        \"\"\"\n        Checks whether the centroids have converged within a specified tolerance.\n        \n        Args:\n            old_centroids (np.ndarray): Centroid positions from the previous iteration.\n            new_centroids (np.ndarray): Updated centroid positions.\n            tol (float): Tolerance threshold for convergence.\n            \n        Returns:\n            bool: True if the centroids have converged; False otherwise.\n        \"\"\"\n        pass\n\n    def compute_elbow_method(self, data: np.ndarray, max_k: int) -> Dict[int, float]:\n        \"\"\"\n        Computes the elbow metric for a range of clusters to aid in identifying the optimal number of clusters.\n        \n        Args:\n            data (np.ndarray): Input data for computing the metric.\n            max_k (int): Maximum number of clusters to consider.\n            \n        Returns:\n            Dict[int, float]: Mapping of number of clusters to computed elbow values.\n        \"\"\"\n        pass\n\n    def optimize_clusters(self, data: np.ndarray, max_k: int) -> int:\n        \"\"\"\n        Determines the optimal number of clusters based on optimization criteria.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            max_k (int): Maximum number of clusters to be evaluated.\n            \n        Returns:\n            int: The optimal number of clusters.\n        \"\"\"\n        pass\n\nclass KMeansClustering:\n    \"\"\"\n    Implements the standard K-means clustering algorithm.\n    \n    This class is responsible for performing the standard iterative clustering process, including:\n    - Assignment of data points to the nearest centroids.\n    - Updating centroid positions based on cluster membership.\n    \n    Usage:\n        kmeans = KMeansClustering(num_clusters=3, max_iter=300)\n        cluster_labels = kmeans.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters to form.\n        max_iter (int): Maximum iterations for the algorithm.\n    \n    Returns:\n        fit_predict returns cluster labels as a numpy.ndarray.\n    \n    Edge Cases:\n        The implementation should handle cases with empty clusters, convergence conditions, and data containing outliers.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the K-means clustering on the provided data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the cluster labels for the provided data.\n        \n        Args:\n            data (np.ndarray): Data for which to determine cluster membership.\n            \n        Returns:\n            np.ndarray: Array of predicted cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines fit and predict steps for convenience.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            np.ndarray: Cluster labels for the input data.\n        \"\"\"\n        pass\n\nclass MiniBatchKMeansClustering:\n    \"\"\"\n    Implements the Mini-Batch K-means clustering algorithm.\n    \n    This variant follows the standard k-means algorithm but uses mini-batches to reduce computation time.\n    \n    Usage:\n        mbkmeans = MiniBatchKMeansClustering(num_clusters=3, batch_size=100)\n        labels = mbkmeans.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters.\n        batch_size (int): Size of the mini-batch.\n        max_iter (int): Maximum number of iterations.\n    \n    Returns:\n        Cluster labels as a numpy.ndarray from fit_predict.\n    \n    Edge Cases:\n        The implementation should address cases where batch size exceeds dataset length and ensure consistent convergence.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, batch_size: int=100, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the Mini-Batch K-means model on provided data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Assigns data points to the nearest cluster centroid based on the fitted model.\n        \n        Args:\n            data (np.ndarray): Data for clustering prediction.\n        \n        Returns:\n            np.ndarray: Predicted cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines the fit and predict operations.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            \n        Returns:\n            np.ndarray: Cluster labels derived from the model.\n        \"\"\"\n        pass\n\nclass SpectralClustering:\n    \"\"\"\n    Implements the spectral clustering method in the context of k-means clustering based workflows.\n    \n    This algorithm uses eigen decomposition on a similarity matrix to reduce dimensionality before clustering.\n    \n    Usage:\n        spectral = SpectralClustering(num_clusters=3, affinity=\"rbf\")\n        labels = spectral.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Desired number of clusters.\n        affinity (str): Affinity type to construct the similarity matrix.\n        n_neighbors (int, optional): Number of neighbors for graph construction.\n    \n    Returns:\n        Cluster labels as a numpy.ndarray.\n    \n    Edge Cases:\n        The method should handle situations where the similarity matrix fails to capture structure in the data.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, affinity: str='rbf', n_neighbors: int=10) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the spectral clustering model on the input data.\n        \n        Args:\n            data (np.ndarray): Data for clustering.\n            \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Assigns cluster labels based on the spectral clustering.\n        \n        Args:\n            data (np.ndarray): Data for which clustering is to be predicted.\n            \n        Returns:\n            np.ndarray: Predicted cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines fitting and predicting into a single step.\n        \n        Args:\n            data (np.ndarray): Data to be clustered.\n            \n        Returns:\n            np.ndarray: Cluster labels for the input data.\n        \"\"\"\n        pass\n\nclass KMedoidsClustering:\n    \"\"\"\n    Implements the k-medoids clustering algorithm, an alternative to k-means that selects actual data points as centers.\n    \n    Usage:\n        kmedoids = KMedoidsClustering(num_clusters=3, max_iter=300)\n        labels = kmedoids.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters.\n        max_iter (int): Maximum number of iterations.\n    \n    Returns:\n        np.ndarray: Predicted cluster labels.\n    \n    Edge Cases:\n        Should handle data with outliers robustly by selecting medoids that minimize dissimilarity.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the k-medoids model to the input data.\n        \n        Args:\n            data (np.ndarray): Data used for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts cluster membership for input data based on medoid distances.\n        \n        Args:\n            data (np.ndarray): Input data.\n            \n        Returns:\n            np.ndarray: Cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Executes fitting and prediction in a single step.\n        \n        Args:\n            data (np.ndarray): Data for clustering.\n            \n        Returns:\n            np.ndarray: Cluster assignments.\n        \"\"\"\n        pass\n\nclass FuzzyKMeansClustering:\n    \"\"\"\n    Implements the fuzzy k-means (soft clustering) algorithm where each data point can belong\n    to multiple clusters with varying degrees of membership.\n    \n    Usage:\n        fuzzy_kmeans = FuzzyKMeansClustering(num_clusters=3, m=2.0, max_iter=300)\n        memberships = fuzzy_kmeans.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters.\n        m (float): Fuzziness parameter that controls the degree of membership sharing.\n        max_iter (int): Maximum number of iterations for convergence.\n    \n    Returns:\n        np.ndarray: Membership matrix indicating the degree of belonging to each cluster.\n    \n    Edge Cases:\n        Should handle cases where the fuzziness parameter may lead to degenerate membership assignments.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, m: float=2.0, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the fuzzy k-means model to the input data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the fuzzy cluster memberships for the provided data.\n        \n        Args:\n            data (np.ndarray): Data for which to compute cluster memberships.\n            \n        Returns:\n            np.ndarray: A membership matrix where each row sums to 1.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines fitting and predicting of fuzzy cluster memberships.\n        \n        Args:\n            data (np.ndarray): Input data for fuzzy clustering.\n            \n        Returns:\n            np.ndarray: The membership matrix.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Unsupervised Learning/Clustering/K-means Clustering/centroid initialization",
                                                        "Unsupervised Learning/Clustering/K-means Clustering/convergence criteria",
                                                        "Unsupervised Learning/Clustering/K-means Clustering/elbow method",
                                                        "Unsupervised Learning/Clustering/K-means Clustering/fuzzy k-means",
                                                        "Unsupervised Learning/Clustering/K-means Clustering/initialization methods",
                                                        "Unsupervised Learning/Clustering/K-means Clustering/k-means clustering",
                                                        "Unsupervised Learning/Clustering/K-means Clustering/k-means++ initialization",
                                                        "Unsupervised Learning/Clustering/K-means Clustering/k-medoids",
                                                        "Unsupervised Learning/Clustering/K-means Clustering/mini-batch k-means",
                                                        "Unsupervised Learning/Clustering/K-means Clustering/optimize clusters",
                                                        "Unsupervised Learning/Clustering/K-means Clustering/spectral clustering"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "hierarchical_density.py",
                                                    "path": "src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                    "code": "from typing import Any\nimport numpy as np\n\nclass DensityBasedClustering:\n    \"\"\"\n    Interface for performing density-based clustering.\n\n    This class provides a framework to implement density-based clustering techniques,\n    such as DBSCAN or similar algorithms that group data based on the local density of points.\n    It encapsulates configuration parameters such as the neighborhood radius and the minimum number\n    of points required to form a cluster.\n\n    Args:\n        eps (float): The radius within which to search for neighboring points.\n        min_samples (int): The minimum number of points required to form a dense region.\n\n    Methods:\n        fit(data: np.ndarray) -> None:\n            Compute the clustering structure from the input data.\n        predict(data: np.ndarray) -> np.ndarray:\n            Assign cluster labels to new or existing data points based on the learned clustering structure.\n    \"\"\"\n\n    def __init__(self, eps: float=0.5, min_samples: int=5) -> None:\n        self.eps = eps\n        self.min_samples = min_samples\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fit the density-based clustering model on the provided data.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            The method should compute the clustering structure by analyzing point density.\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict cluster labels for the provided data with the density-based clustering model.\n\n        Args:\n            data (np.ndarray): New or training data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            np.ndarray: An array of cluster labels, where noise points may be labeled as -1.\n\n        Notes:\n            The output labels should be consistent with the clustering computed in the fit method.\n        \"\"\"\n        pass\n\nclass AgglomerativeClustering:\n    \"\"\"\n    Interface for performing agglomerative hierarchical clustering.\n\n    This class implements the agglomerative (bottom-up) clustering strategy by merging the\n    closest clusters iteratively based on a chosen linkage criterion. It supports extracting\n    the final cluster labels as well as generating a dendrogram representation of the hierarchical structure.\n\n    Args:\n        linkage (str): The linkage criterion to use (e.g., 'ward', 'complete', 'average').\n        distance_metric (str, optional): The metric to measure distance between data points. Defaults to 'euclidean'.\n\n    Methods:\n        fit(data: np.ndarray) -> None:\n            Perform the agglomerative clustering on the input data.\n        predict(data: np.ndarray) -> np.ndarray:\n            Return cluster labels for the input data based on the fitted model.\n        get_dendrogram() -> Any:\n            Retrieve a dendrogram structure representing the hierarchical cluster merge history.\n    \"\"\"\n\n    def __init__(self, linkage: str='ward', distance_metric: str='euclidean') -> None:\n        self.linkage = linkage\n        self.distance_metric = distance_metric\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fit the agglomerative clustering model on the provided dataset.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            This method should execute the bottom-up merging process based on the specified linkage.\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict cluster labels for the input data using the agglomerative clustering model.\n\n        Args:\n            data (np.ndarray): Data as a NumPy array for which cluster labels are required.\n\n        Returns:\n            np.ndarray: An array of cluster labels corresponding to each sample.\n        \"\"\"\n        pass\n\n    def get_dendrogram(self) -> Any:\n        \"\"\"\n        Retrieve the dendrogram structure representing the hierarchical clustering tree.\n\n        Returns:\n            Any: An object representing the dendrogram, which may be used for visualization or further analysis.\n        \"\"\"\n        pass\n\nclass DivisiveClustering:\n    \"\"\"\n    Interface for performing divisive hierarchical clustering.\n\n    This class implements the divisive (top-down) clustering strategy by recursively splitting\n    the dataset into clusters based on specified criteria. It provides methods to obtain cluster\n    labels and to generate a dendrogram that represents the recursive division of the data.\n\n    Args:\n        criterion (Any, optional): The criterion for splitting clusters (e.g., maximization of between-cluster variance).\n        distance_metric (str, optional): The metric used for evaluating splits. Defaults to 'euclidean'.\n\n    Methods:\n        fit(data: np.ndarray) -> None:\n            Perform the divisive clustering on the input data.\n        predict(data: np.ndarray) -> np.ndarray:\n            Return cluster labels for the input data based on the fitted model.\n        get_dendrogram() -> Any:\n            Retrieve a dendrogram that illustrates the hierarchical splits made by the model.\n    \"\"\"\n\n    def __init__(self, criterion: Any=None, distance_metric: str='euclidean') -> None:\n        self.criterion = criterion\n        self.distance_metric = distance_metric\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fit the divisive clustering model on the provided dataset by recursively splitting the data.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            This method should implement a top-down clustering approach, recursively dividing the data.\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict cluster labels for the given data using the divisive clustering model.\n\n        Args:\n            data (np.ndarray): Data as a NumPy array for which cluster labels are to be computed.\n\n        Returns:\n            np.ndarray: An array of cluster labels reflecting the outcome of the divisive clustering.\n        \"\"\"\n        pass\n\n    def get_dendrogram(self) -> Any:\n        \"\"\"\n        Retrieve the hierarchical dendrogram representing the splits generated by the divisive clustering process.\n\n        Returns:\n            Any: A dendrogram object that can be used to analyze or visualize the hierarchical structure.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Unsupervised Learning/Clustering/Hierarchical & Density Clustering/agglomerative",
                                                        "Unsupervised Learning/Clustering/Hierarchical & Density Clustering/density-based clustering",
                                                        "Unsupervised Learning/Clustering/Hierarchical & Density Clustering/divisive"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "__init__.py",
                                                    "path": "src/algorithms/unsupervised/clustering/__init__.py",
                                                    "code": "",
                                                    "feature_paths": [],
                                                    "units": []
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "anomaly_detection.py",
                                            "path": "src/algorithms/unsupervised/anomaly_detection.py",
                                            "code": "import pandas as pd\nimport numpy as np\n\nclass OneClassSVMAnomalyDetector:\n    \"\"\"\n    Anomaly detector using the One-Class SVM algorithm.\n    \n    This class implements the one-class SVM approach for detecting anomalies in an unsupervised\n    learning setting. It is designed to learn the boundary of normal data points during training\n    and subsequently predict whether a new observation is an anomaly.\n    \n    Methods:\n        __init__(kernel: str = 'rbf', nu: float = 0.5, gamma: Optional[float] = None)\n            Initializes the anomaly detector with parameters for the SVM kernel, the nu parameter,\n            and the gamma value.\n\n        fit(X: pd.DataFrame) -> None:\n            Train the anomaly detector on the provided dataset.\n\n        predict(X: pd.DataFrame) -> np.ndarray:\n            Predict if the samples in the dataset are anomalies. Returns an array where typically\n            -1 indicates an anomaly and 1 indicates a normal data point.\n    \n    Args:\n        kernel (str): Specifies the kernel type to be used in the algorithm (default is 'rbf').\n        nu (float): An upper bound on the fraction of training errors and a lower bound of the fraction\n                    of support vectors (default is 0.5).\n        gamma (Optional[float]): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. If None, it is set\n                                 to 'scale' by convention.\n    \"\"\"\n\n    def __init__(self, kernel: str='rbf', nu: float=0.5, gamma: Optional[float]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Fit the One-Class SVM model on the training data.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict anomalies in the provided data.\n        \n        Args:\n            X (pd.DataFrame): Data to be evaluated for anomalies.\n        \n        Returns:\n            np.ndarray: An array with predicted labels (e.g., 1 for normal, -1 for anomaly).\n        \"\"\"\n        pass\n\nclass IsolationForestAnomalyDetector:\n    \"\"\"\n    Anomaly detector using the Isolation Forest algorithm.\n    \n    This class implements the Isolation Forest approach for unsupervised anomaly detection.\n    The algorithm isolates anomalies instead of profiling normal data points, making it effective\n    for handling high-dimensional datasets.\n    \n    Methods:\n        __init__(n_estimators: int = 100, contamination: float = 0.1, max_samples: Optional[int] = None)\n            Initializes the Isolation Forest with the specified number of trees (estimators), expected\n            contamination (proportion of anomalies), and the number of samples to draw from X to train each tree.\n\n        fit(X: pd.DataFrame) -> None:\n            Build the isolation forest on the training data.\n\n        predict(X: pd.DataFrame) -> np.ndarray:\n            Predict whether the samples are anomalies, returning an array where anomalies are usually\n            marked with -1.\n    \n    Args:\n        n_estimators (int): The number of base estimators in the ensemble (default is 100).\n        contamination (float): The expected proportion of outliers in the data (default is 0.1).\n        max_samples (Optional[int]): The number of samples to draw to train each base estimator. If None,\n                                     the maximum available samples are used.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, contamination: float=0.1, max_samples: Optional[int]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Fit the Isolation Forest model on the training data.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict anomalies using the Isolation Forest model.\n        \n        Args:\n            X (pd.DataFrame): Data for which anomaly detection is to be performed.\n        \n        Returns:\n            np.ndarray: An array with predictions where typically -1 indicates an anomaly and 1 indicates normal.\n        \"\"\"\n        pass\n\nclass LocalOutlierFactorAnomalyDetector:\n    \"\"\"\n    Anomaly detector using the Local Outlier Factor (LOF) algorithm.\n    \n    This class implements the Local Outlier Factor method for detecting anomalies by measuring the\n    local deviation of density of a given data point with respect to its neighbors. A lower density\n    compared to its neighbors indicates a potential outlier.\n    \n    Methods:\n        __init__(n_neighbors: int = 20, contamination: float = 0.1)\n            Initializes the LOF detector with the number of neighbors to use and the expected\n            proportion of anomalies.\n\n        fit(X: pd.DataFrame) -> None:\n            Compute the LOF scores from the training data.\n\n        predict(X: pd.DataFrame) -> np.ndarray:\n            Predict whether data instances are anomalies based on their LOF scores, with an output\n            array where -1 indicates an anomaly.\n    \n    Args:\n        n_neighbors (int): The number of neighbors to use for computing the local density (default is 20).\n        contamination (float): The expected proportion of anomalies in the data (default is 0.1).\n    \"\"\"\n\n    def __init__(self, n_neighbors: int=20, contamination: float=0.1) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Fit the Local Outlier Factor model by computing neighbor densities.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict anomalies using the LOF model.\n        \n        Args:\n            X (pd.DataFrame): Data for which anomaly detection is performed.\n        \n        Returns:\n            np.ndarray: An array with predictions where -1 indicates an anomaly and 1 indicates a normal point.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Unsupervised Learning/Anomaly and Self-Organizing/Anomaly Detection/isolation forest",
                                                "Unsupervised Learning/Anomaly and Self-Organizing/Anomaly Detection/local outlier factor",
                                                "Unsupervised Learning/Anomaly and Self-Organizing/Anomaly Detection/one-class svm"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "self_organizing_maps.py",
                                            "path": "src/algorithms/unsupervised/self_organizing_maps.py",
                                            "code": "from typing import Optional\nimport numpy as np\n\nclass SelfOrganizingMap:\n    \"\"\"\n    Represents a Self-Organizing Map (SOM) with adjustable learning rate and topology preservation mechanisms.\n\n    This class encapsulates the functionality required to train a Self-Organizing Map. It allows the\n    specification of an initial learning rate, which controls the magnitude of weight updates during training, \n    and includes functionality to maintain and assess topology preservation, ensuring that the spatial relationships \n    in the input data are maintained in the resulting map.\n\n    Attributes:\n        learning_rate (float): The initial learning rate to be used during training.\n        topology_preservation (bool): A flag indicating whether topology preservation mechanisms should be applied.\n        map_dimensions (tuple): The dimensions of the SOM grid (e.g., number of rows and columns).\n    \"\"\"\n\n    def __init__(self, learning_rate: float, topology_preservation: bool, map_dimensions: Optional[tuple]=(10, 10)) -> None:\n        \"\"\"\n        Initialize the Self-Organizing Map with specified learning rate and topology preservation settings.\n\n        Args:\n            learning_rate (float): The initial learning rate for the training process. Must be a positive float.\n            topology_preservation (bool): Determines if topology preservation is enforced during training.\n            map_dimensions (tuple, optional): Dimensions of the SOM grid as (rows, columns). Defaults to (10, 10).\n\n        Raises:\n            ValueError: If learning_rate is not positive or if map_dimensions is not a valid tuple.\n        \"\"\"\n        pass\n\n    def fit(self, data: np.ndarray, num_iterations: int) -> None:\n        \"\"\"\n        Train the Self-Organizing Map on the provided dataset.\n\n        The fit method updates the internal map (i.e., weights) based on the input data over a number of iterations.\n        The learning rate may be adjusted during the training, and topology preservation is maintained through\n        appropriate update rules.\n\n        Args:\n            data (np.ndarray): A 2D numpy array representing the training input features.\n            num_iterations (int): The total number of iterations for training the SOM.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the input data is not in the expected format or if num_iterations is not a positive integer.\n        \"\"\"\n        pass\n\n    def get_topology_preservation_metric(self) -> float:\n        \"\"\"\n        Compute and return a metric measuring the quality of topology preservation in the SOM.\n\n        This method evaluates how well the SOM maintains the relative spatial relationships of the input data.\n        A higher value typically indicates better preservation of the topological structure.\n\n        Returns:\n            float: A numerical value representing the quality of topology preservation.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Unsupervised Learning/Anomaly and Self-Organizing/Self-Organizing Maps/learning rate",
                                                "Unsupervised Learning/Anomaly and Self-Organizing/Self-Organizing Maps/topology preservation"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/algorithms/unsupervised/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "optimization",
                                    "path": "src/algorithms/optimization",
                                    "children": [
                                        {
                                            "type": "directory",
                                            "name": "gradient_descent",
                                            "path": "src/algorithms/optimization/gradient_descent",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "adaptive_batch.py",
                                                    "path": "src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                    "code": "from typing import Any\nimport numpy as np\nfrom typing import Any, Callable\n\nclass AdagradOptimizer:\n    \"\"\"\n    Optimizer implementing the Adagrad algorithm which adapts learning rate for each parameter.\n    \n    This optimizer adjusts the learning rate for each parameter based on the history of gradients.\n    It is typically used when dealing with sparse data or scenarios where different parameters\n    require different updates.\n\n    Args:\n        learning_rate (float): The initial step size for gradient updates.\n        epsilon (float): A small constant to prevent division by zero, typically 1e-8.\n\n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Updates the parameters using the Adagrad algorithm.\n            \n            Args:\n                params (np.ndarray): Current parameters to be updated.\n                grads (np.ndarray): Gradients computed from the loss function.\n            \n            Returns:\n                np.ndarray: Updated parameters.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, epsilon: float=1e-08) -> None:\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon\n        self.grad_squared_accum = None\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass\n\nclass NesterovAcceleratedGradient:\n    \"\"\"\n    Optimizer implementing Nesterov Accelerated Gradient (NAG) method.\n\n    This optimizer improves upon the standard momentum method by computing the gradient \n    at the approximate future position of the parameters, leading to faster convergence.\n\n    Args:\n        learning_rate (float): The step size for updating parameters.\n        momentum (float): The momentum factor (typically between 0 and 1).\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Perform a parameter update using the Nesterov accelerated gradient method.\n            \n            Args:\n                params (np.ndarray): The current model parameters.\n                grads (np.ndarray): The computed gradients based on the lookahead position.\n            \n            Returns:\n                np.ndarray: Updated parameters after applying the NAG update.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, momentum: float) -> None:\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.velocity = None\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass\n\nclass RMSPropOptimizer:\n    \"\"\"\n    Optimizer implementing the RMSProp algorithm.\n\n    RMSProp adapts the learning rate for each parameter by dividing the learning rate\n    by a running average of the magnitudes of recent gradients, which helps with faster and \n    more stable convergence.\n\n    Args:\n        learning_rate (float): Initial learning rate for the optimizer.\n        decay_rate (float): Decay rate for the moving average of squared gradients.\n        epsilon (float): A small constant to prevent division by zero, typically set to 1e-8.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Updates parameters using RMSProp optimization.\n            \n            Args:\n                params (np.ndarray): Array of current parameters.\n                grads (np.ndarray): Array of gradients computed from the loss.\n            \n            Returns:\n                np.ndarray: Updated parameters after applying the RMSProp step.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, decay_rate: float, epsilon: float=1e-08) -> None:\n        self.learning_rate = learning_rate\n        self.decay_rate = decay_rate\n        self.epsilon = epsilon\n        self.squared_avg = None\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass\n\nclass GDWithLineSearch:\n    \"\"\"\n    Optimizer that uses gradient descent augmented with line search.\n\n    This method performs standard gradient descent updates while dynamically determining\n    an optimal learning rate through a line search strategy. The line search procedure iteratively\n    selects a step size that sufficiently decreases the loss function.\n\n    Args:\n        initial_learning_rate (float): The starting step size before line search adjustment.\n        line_search_fn (Callable[[np.ndarray, np.ndarray], float]): A callable that computes the optimal \n            step size given the current parameters and gradient. This function should accept the\n            current parameter vector and gradient and return a suitable learning rate.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray, loss_fn: Callable[[np.ndarray], float]) -> np.ndarray:\n            Updates the parameters using gradient descent with a dynamically determined step size.\n            \n            Args:\n                params (np.ndarray): Current model parameters.\n                grads (np.ndarray): Gradients computed for the current parameters.\n                loss_fn (Callable[[np.ndarray], float]): The loss function to evaluate parameter updates.\n            \n            Returns:\n                np.ndarray: Updated parameters after applying the line search-based gradient descent step.\n    \"\"\"\n\n    def __init__(self, initial_learning_rate: float, line_search_fn: Callable[[np.ndarray, np.ndarray], float]) -> None:\n        self.initial_learning_rate = initial_learning_rate\n        self.line_search_fn = line_search_fn\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray, loss_fn: Callable[[np.ndarray], float]) -> np.ndarray:\n        pass\n\nclass AdamOptimizer:\n    \"\"\"\n    Optimizer implementing the Adam algorithm, which combines adaptive learning rates and momentum.\n\n    Adam maintains exponential moving averages of both the gradient and its square, and it\n    includes bias-correction terms. It is widely used for training deep learning models due\n    to its computational efficiency and robust performance on large datasets.\n\n    Args:\n        learning_rate (float): The initial step size for parameter updates.\n        beta1 (float): Exponential decay rate for the first moment estimates.\n        beta2 (float): Exponential decay rate for the second moment estimates.\n        epsilon (float): A small constant to prevent division by zero (usually around 1e-8).\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Performs an update on the parameters using the Adam optimization algorithm.\n            \n            Args:\n                params (np.ndarray): Current parameters as a numpy array.\n                grads (np.ndarray): Gradients calculated from the loss function.\n            \n            Returns:\n                np.ndarray: Updated parameters after the Adam step.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, beta1: float=0.9, beta2: float=0.999, epsilon: float=1e-08) -> None:\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.m = None\n        self.v = None\n        self.t = 0\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass\n",
                                                    "feature_paths": [
                                                        "Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/adagrad",
                                                        "Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/adam optimizer",
                                                        "Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/gd with line search",
                                                        "Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/nesterov accelerated gd",
                                                        "Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/rmsprop"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "stochastic.py",
                                                    "path": "src/algorithms/optimization/gradient_descent/stochastic.py",
                                                    "code": "import numpy as np\n\nclass SGDWithMomentum:\n    \"\"\"\n    Implements Stochastic Gradient Descent with Momentum.\n    \n    This optimizer updates model parameters using a stochastic gradient descent approach\n    enhanced with momentum to accelerate convergence and dampen oscillations.\n    \n    Attributes:\n        learning_rate (float): The step size used for each parameter update.\n        momentum (float): The momentum factor, typically between 0 and 1, that determines\n                          the contribution of past gradients.\n        velocity (np.ndarray): The accumulated velocity vector, initialized during the first call to step.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Computes the update for parameters based on gradients and momentum.\n    \n    Args:\n        learning_rate (float): The learning rate for parameter updates.\n        momentum (float): Momentum coefficient for smoothing updates.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, momentum: float) -> None:\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.velocity = None\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Perform a single optimization step using SGD with momentum.\n        \n        Args:\n            params (np.ndarray): The current model parameters.\n            grads (np.ndarray): The gradients computed for the parameters.\n            \n        Returns:\n            np.ndarray: Updated model parameters after applying momentum-based SGD step.\n            \n        Edge Cases:\n            - If velocity is uninitialized, it should be set to zeros with the same shape as params.\n            - The method assumes that params and grads have identical shapes.\n        \"\"\"\n        pass\n\nclass MiniBatchSGD:\n    \"\"\"\n    Implements Mini-Batch Stochastic Gradient Descent.\n    \n    This optimizer updates model parameters using mini-batch gradient descent, which combines \n    the advantages of both stochastic and batch gradient descent. It processes a subset of the \n    training data (mini-batch) to compute the gradients for each update.\n    \n    Attributes:\n        learning_rate (float): The step size used for each parameter update.\n        batch_size (int): The number of training examples used to compute a single gradient update.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Performs an optimization step using the computed mini-batch gradients.\n    \n    Args:\n        learning_rate (float): The learning rate for parameter updates.\n        batch_size (int): The number of samples in each mini-batch.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, batch_size: int) -> None:\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Perform a single optimization step using mini-batch SGD.\n        \n        Args:\n            params (np.ndarray): The current model parameters.\n            grads (np.ndarray): The gradients computed from a mini-batch.\n        \n        Returns:\n            np.ndarray: Updated model parameters after applying the mini-batch SGD step.\n        \n        Assumptions:\n            - The gradients are computed from a mini-batch of the dataset.\n            - The shapes of params and grads are identical.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Optimization Techniques/Gradient Descent Methods/Stochastic Methods/mini-batch sgd",
                                                        "Optimization Techniques/Gradient Descent Methods/Stochastic Methods/sgd with momentum"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "__init__.py",
                                                    "path": "src/algorithms/optimization/gradient_descent/__init__.py",
                                                    "code": "",
                                                    "feature_paths": [],
                                                    "units": []
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "svm_optimization.py",
                                            "path": "src/algorithms/optimization/svm_optimization.py",
                                            "code": "import numpy as np\nfrom typing import Any, Optional\n\nclass SVMStochasticGradientDescentOptimizer:\n    \"\"\"\n    Optimizer for SVM classification using stochastic gradient descent.\n    \n    This class implements the optimization of the SVM objective function using\n    stochastic gradient descent (SGD). It is designed to handle large datasets by\n    iteratively updating the model weights based on mini-batches or single samples.\n    \n    Attributes:\n        learning_rate (float): The step size used for each update.\n        max_iter (int): The maximum number of iterations to run the optimizer.\n        tolerance (float): The threshold for stopping criterion based on weight change.\n    \n    Methods:\n        optimize(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n            Performs the optimization to determine the optimal parameters (weights)\n            for the SVM model given the training data and labels.\n    \n    Args:\n        learning_rate (float): Learning rate for the SGD updates.\n        max_iter (int): Maximum number of iterations to perform.\n        tolerance (float): Tolerance for convergence; if updates fall below this value,\n            optimization stops.\n    \"\"\"\n\n    def __init__(self, learning_rate: float=0.01, max_iter: int=1000, tolerance: float=0.0001) -> None:\n        self.learning_rate = learning_rate\n        self.max_iter = max_iter\n        self.tolerance = tolerance\n\n    def optimize(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Optimize the SVM objective function using stochastic gradient descent.\n        \n        Processes the feature matrix X and associated labels y, iteratively updating\n        the model's weights to minimize the SVM loss function.\n        \n        Args:\n            X (np.ndarray): A 2D array representing the input features.\n            y (np.ndarray): A 1D array of target labels corresponding to X.\n        \n        Returns:\n            np.ndarray: The optimized weight vector after convergence or after\n            reaching the maximum number of iterations.\n        \"\"\"\n        pass\n\nclass SMOSVMOptimizer:\n    \"\"\"\n    Optimizer for SVM classification using the Sequential Minimal Optimization (SMO) algorithm.\n    \n    This class provides an interface for optimizing SVM quadratic programming problems\n    by applying the SMO algorithm. It incrementally adjusts the Lagrange multipliers to\n    find the optimal decision boundary. The implementation is designed to ensure efficient\n    handling of constraints and calculation of the optimum parameters.\n    \n    Attributes:\n        C (float): Regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n        tolerance (float): Tolerance for the optimization to determine convergence.\n        max_iter (int): Maximum number of iterations to attempt optimization.\n    \n    Methods:\n        optimize(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n            Executes the SMO algorithm on the provided training data to compute the optimal\n            support vectors and corresponding model parameters.\n    \n    Args:\n        C (float): The regularization parameter for SVM.\n        tolerance (float): A small value to detect convergence of the algorithm.\n        max_iter (int): The limit on the number of iterations for the optimization process.\n    \"\"\"\n\n    def __init__(self, C: float=1.0, tolerance: float=0.001, max_iter: int=1000) -> None:\n        self.C = C\n        self.tolerance = tolerance\n        self.max_iter = max_iter\n\n    def optimize(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Optimize the SVM model parameters using the SMO algorithm.\n\n        Args:\n            X (np.ndarray): A 2D numpy array containing the input features.\n            y (np.ndarray): A 1D numpy array of target labels corresponding to X.\n\n        Returns:\n            np.ndarray: The vector of optimized parameters (e.g., Lagrange multipliers or weight vector)\n            resulting from the SMO optimization process.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Optimization Techniques/SVM Optimization/Optimization Algorithms/smo algorithm",
                                                "Optimization Techniques/SVM Optimization/Optimization Algorithms/stochastic gradient descent",
                                                "Supervised Learning/Classification Algorithms/SVM Classification/smo algorithm",
                                                "Supervised Learning/Classification Algorithms/SVM Classification/stochastic gradient descent"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/algorithms/optimization/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "tree_ensemble",
                                    "path": "src/algorithms/tree_ensemble",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "decision_tree_methods.py",
                                            "path": "src/algorithms/tree_ensemble/decision_tree_methods.py",
                                            "code": "import pandas as pd\nfrom typing import Any, Optional, Union\nfrom typing import Any\n\ndef apply_reduced_error_pruning(tree_model: Any, X_val: pd.DataFrame, y_val: pd.Series, tolerance: Optional[float]=None) -> Any:\n    \"\"\"\n    Apply reduced error pruning to a decision tree model using a validation dataset.\n\n    This function prunes the given decision tree model by iteratively removing nodes\n    and evaluating the impact on validation accuracy. The pruning stops when the pruning\n    does not yield an improvement greater than a specified tolerance.\n\n    Args:\n        tree_model (Any): The decision tree model to be pruned.\n        X_val (pd.DataFrame): The validation features used to evaluate pruning impact.\n        y_val (pd.Series): The validation labels corresponding to X_val.\n        tolerance (Optional[float]): The minimum improvement threshold required to continue pruning.\n                                     If None, a default threshold may be used by the underlying algorithm.\n\n    Returns:\n        Any: The pruned decision tree model.\n    \"\"\"\n    pass\n\ndef apply_node_limited_pre_pruning(tree_model: Any, max_nodes: int) -> Any:\n    \"\"\"\n    Perform node-limited pre-pruning on a decision tree model by restricting the total number of nodes.\n\n    This function applies a pre-pruning strategy where the tree growth is halted once the number\n    of nodes reaches a specified maximum. This helps in controlling model complexity and overfitting.\n\n    Args:\n        tree_model (Any): The decision tree model to be pruned.\n        max_nodes (int): The maximum number of nodes allowed in the tree.\n\n    Returns:\n        Any: The pruned decision tree model with node limitation.\n    \"\"\"\n    pass\n\ndef apply_early_stopping(tree_model: Any, X_val: pd.DataFrame, y_val: pd.Series, patience: int=10) -> Any:\n    \"\"\"\n    Apply early stopping as a pruning strategy during decision tree training.\n\n    This function monitors the performance of a decision tree model on a validation dataset\n    and stops further growth when the performance fails to improve for a defined number of iterations\n    (patience). This prevents the model from overfitting.\n\n    Args:\n        tree_model (Any): The decision tree model under training or intermediate stage.\n        X_val (pd.DataFrame): The validation features used to monitor performance improvements.\n        y_val (pd.Series): The validation labels corresponding to X_val.\n        patience (int): The number of iterations with no improvement after which training is stopped.\n\n    Returns:\n        Any: The decision tree model that has been halted early to avoid overfitting.\n    \"\"\"\n    pass\n\ndef apply_cost_complexity_pruning(tree_model: Any, ccp_alpha: float) -> Any:\n    \"\"\"\n    Apply cost complexity pruning to a decision tree model to balance complexity versus predictive accuracy.\n\n    This function prunes the decision tree based on a complexity parameter (ccp_alpha). The method\n    computes a trade-off between the model complexity and its performance, removing branches that incur\n    a cost higher than the improvement in error reduction.\n\n    Args:\n        tree_model (Any): The decision tree model to be pruned.\n        ccp_alpha (float): The complexity parameter used to control the trade-off between tree size and accuracy.\n                           A larger value leads to more pruning.\n\n    Returns:\n        Any: The pruned decision tree model after applying cost complexity adjustments.\n    \"\"\"\n    pass\n\ndef apply_depth_limitation(tree_model: Any, max_depth: int) -> Any:\n    \"\"\"\n    Limit the depth of a decision tree model to prevent overfitting and excessive complexity.\n\n    This function enforces a maximum tree depth by pruning nodes that exceed the specified depth limit,\n    ensuring that the final tree maintains a controlled level of granularity.\n\n    Args:\n        tree_model (Any): The decision tree model to which the depth limitation is applied.\n        max_depth (int): The maximum allowed depth for the tree. Nodes beyond this depth will be pruned.\n\n    Returns:\n        Any: The decision tree model with enforced depth limitation.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/cost complexity pruning",
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/depth limitation",
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/early stopping",
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/node-limited pre-pruning",
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/reduced error pruning"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "forest_voting_methods.py",
                                            "path": "src/algorithms/tree_ensemble/forest_voting_methods.py",
                                            "code": "from typing import Any, List\nfrom algorithms.base_algorithm import BaseAlgorithm\nimport numpy as np\nfrom typing import Any, List, Optional\nfrom typing import Any, Optional\nimport pandas as pd\n\nclass VotingEnsemble(BaseAlgorithm):\n    \"\"\"\n    VotingEnsemble provides an ensemble method that aggregates predictions from multiple models\n    using a voting mechanism. It supports both hard voting, where predictions are determined by\n    majority rule, and soft voting, where averaged probabilities determine the final outcome.\n\n    Attributes:\n        estimators (List[Any]): A list of fitted estimators whose predictions will be aggregated.\n        voting (str): The voting type, either 'hard' for majority vote or 'soft' for probability-weighted vote.\n        weights (Optional[List[float]]): Optional list of weights corresponding to each estimator.\n    \"\"\"\n\n    def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'VotingEnsemble':\n        \"\"\"\n        Fit the ensemble using the provided training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): True labels for training.\n\n        Returns:\n            VotingEnsemble: The fitted ensemble instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict the class labels for the given input data using the voting mechanism.\n\n        Args:\n            X (pd.DataFrame): Input data on which to predict.\n\n        Returns:\n            np.ndarray: Predicted class labels.\n        \"\"\"\n        pass\n\nclass GradientBoostingEnsemble(BaseAlgorithm):\n    \"\"\"\n    GradientBoostingEnsemble implements a gradient boosting ensemble method where\n    models are trained sequentially to correct the errors of prior models. This interface\n    is designed to support boosting with decision trees for improved predictive performance.\n\n    Attributes:\n        n_estimators (int): The number of boosting stages to perform.\n        learning_rate (float): The contribution weight of each model.\n        max_depth (int): The maximum depth of individual regression estimators.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, learning_rate: float=0.1, max_depth: int=3) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'GradientBoostingEnsemble':\n        \"\"\"\n        Fit the gradient boosting ensemble to the training data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Target values.\n\n        Returns:\n            GradientBoostingEnsemble: The fitted boosting ensemble instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Generate predictions for input data using the ensemble of boosted models.\n\n        Args:\n            X (pd.DataFrame): Data for prediction.\n\n        Returns:\n            np.ndarray: Predicted values or class labels.\n        \"\"\"\n        pass\n\nclass MultiClassVotingClassifier(BaseAlgorithm):\n    \"\"\"\n    MultiClassVotingClassifier is designed to perform ensemble classification specifically\n    for multi-class problems using voting mechanisms. It aggregates predictions from several\n    base classifiers to determine the most likely class among multiple possible outcomes.\n\n    Attributes:\n        estimators (List[Any]): A list of classifier estimators.\n        voting (str): Type of voting to use ('hard' or 'soft').\n        weights (Optional[List[float]]): Optional weightings for each estimator.\n    \"\"\"\n\n    def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultiClassVotingClassifier':\n        \"\"\"\n        Fit the multi-class voting classifier using training data.\n\n        Args:\n            X (pd.DataFrame): Input feature data.\n            y (pd.Series): Multi-class target labels.\n\n        Returns:\n            MultiClassVotingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict class labels for multi-class classification using a voting scheme.\n\n        Args:\n            X (pd.DataFrame): Data for which to predict class labels.\n\n        Returns:\n            np.ndarray: Predicted class labels.\n        \"\"\"\n        pass\n\nclass RandomForestEnsemble(BaseAlgorithm):\n    \"\"\"\n    RandomForestEnsemble implements the random forest algorithm, an ensemble method that\n    builds a multitude of decision trees at training time and outputs the mode of the classes\n    for classification or mean prediction for regression tasks.\n\n    Attributes:\n        n_estimators (int): Number of trees in the forest.\n        max_features (Optional[int]): The number of features to consider when looking for the best split.\n        bootstrap (bool): Whether bootstrap samples are used when building trees.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, max_features: Optional[int]=None, bootstrap: bool=True) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'RandomForestEnsemble':\n        \"\"\"\n        Fit the random forest ensemble on the training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): Training target labels.\n\n        Returns:\n            RandomForestEnsemble: The fitted ensemble model.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict class labels or regression outputs using the random forest ensemble.\n\n        Args:\n            X (pd.DataFrame): Input data for prediction.\n\n        Returns:\n            np.ndarray: Predicted outcomes.\n        \"\"\"\n        pass\n\nclass BaggingEnsemble(BaseAlgorithm):\n    \"\"\"\n    BaggingEnsemble implements the bootstrap aggregating (bagging) method for ensemble learning.\n    This technique trains multiple instances of a base estimator on random subsets of the training data\n    and aggregates their predictions to enhance stability and accuracy.\n\n    Attributes:\n        base_estimator (Any): The base model used for bagging.\n        n_estimators (int): The number of base estimators to train.\n        max_samples (Optional[int]): The number of samples drawn from the training set for each estimator.\n    \"\"\"\n\n    def __init__(self, base_estimator: Any, n_estimators: int=10, max_samples: Optional[int]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'BaggingEnsemble':\n        \"\"\"\n        Fit the bagging ensemble by training multiple instances of the base estimator on subsets of the data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Training target labels.\n\n        Returns:\n            BaggingEnsemble: The fitted bagging ensemble model.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict outcomes by aggregating the predictions of individual base estimators.\n\n        Args:\n            X (pd.DataFrame): Data for which predictions are required.\n\n        Returns:\n            np.ndarray: Aggregated predictions.\n        \"\"\"\n        pass\n\nclass BinaryVotingClassifier(BaseAlgorithm):\n    \"\"\"\n    BinaryVotingClassifier specializes in ensemble classification for binary outcomes using a voting mechanism.\n    It aggregates predictions from multiple binary classifiers to determine the final binary label.\n\n    Attributes:\n        estimators (List[Any]): A list of binary classifier estimators.\n        voting (str): The voting strategy ('hard' or 'soft').\n        weights (Optional[List[float]]): Optional weights for each classifier.\n    \"\"\"\n\n    def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'BinaryVotingClassifier':\n        \"\"\"\n        Fit the binary voting classifier with the training data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Binary target labels.\n\n        Returns:\n            BinaryVotingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict binary class labels for the given input data using a voting mechanism.\n\n        Args:\n            X (pd.DataFrame): Data for which to predict binary labels.\n\n        Returns:\n            np.ndarray: Predicted binary class labels.\n        \"\"\"\n        pass\n\nclass QuantileRegressionForest(BaseAlgorithm):\n    \"\"\"\n    QuantileRegressionForest implements an ensemble method for quantile regression using a collection \n    of decision trees. This model is designed to estimate conditional quantiles, providing insight into \n    the distribution of the target variable rather than just the mean.\n\n    Attributes:\n        n_estimators (int): The number of trees in the forest.\n        quantiles (List[float]): The list of quantiles to estimate (e.g., [0.1, 0.5, 0.9]).\n        max_depth (int): The maximum depth of each tree.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, quantiles: List[float]=[0.1, 0.5, 0.9], max_depth: int=5) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'QuantileRegressionForest':\n        \"\"\"\n        Fit the quantile regression forest model on the training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): Target values.\n\n        Returns:\n            QuantileRegressionForest: The fitted quantile regression model.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> dict:\n        \"\"\"\n        Predict quantiles for the given input data. Returns a dictionary mapping each quantile to an array\n        of predicted values.\n\n        Args:\n            X (pd.DataFrame): Data for prediction.\n\n        Returns:\n            dict: A dictionary where keys are quantile levels and values are arrays of predictions.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/bagging",
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/binary classification",
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/gradient boosting",
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/multi-class classification",
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/quantile regression",
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/random forest",
                                                "Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/voting"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/algorithms/tree_ensemble/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "file",
                                    "name": "__init__.py",
                                    "path": "src/algorithms/__init__.py",
                                    "code": "",
                                    "feature_paths": [],
                                    "units": []
                                }
                            ]
                        },
                        {
                            "type": "directory",
                            "name": "workflow",
                            "path": "src/workflow",
                            "children": [
                                {
                                    "type": "directory",
                                    "name": "preprocessing",
                                    "path": "src/workflow/preprocessing",
                                    "children": [
                                        {
                                            "type": "directory",
                                            "name": "feature_engineering",
                                            "path": "src/workflow/preprocessing/feature_engineering",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "data_transformation.py",
                                                    "path": "src/workflow/preprocessing/feature_engineering/data_transformation.py",
                                                    "code": "from scipy import sparse\nfrom typing import Optional, List, Union\nimport pandas as pd\n\ndef apply_onehot_encoding(df: pd.DataFrame, columns: Optional[List[str]]=None, sparse_output: bool=False) -> Union[pd.DataFrame, sparse.csr_matrix]:\n    \"\"\"\n    Apply one-hot encoding to the specified categorical columns of the input DataFrame.\n    \n    This function transforms categorical columns into one-hot encoded representations.\n    If 'columns' are not specified, all columns with object or category dtypes will be encoded.\n    The output can be returned as a dense DataFrame or as a sparse CSR matrix based on the\n    'sparse_output' flag.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing data to encode.\n        columns (Optional[List[str]]): List of column names to one-hot encode. If None, infer categorical columns.\n        sparse_output (bool): Flag indicating whether the output should be a sparse CSR matrix. Defaults to False.\n\n    Returns:\n        Union[pd.DataFrame, sparse.csr_matrix]: One-hot encoded data in dense or sparse format.\n\n    Edge Cases:\n        - If no categorical columns exist or the specified columns are not present, the function returns the original DataFrame.\n        - If an empty DataFrame is passed, an empty DataFrame (or corresponding sparse matrix) is returned.\n\n    Assumptions:\n        - The input DataFrame is properly formatted.\n    \"\"\"\n    pass\n\ndef scale_and_normalize_features(df: pd.DataFrame, feature_columns: Optional[List[str]]=None, scaling_range: Optional[tuple]=(0, 1), normalization_method: str='zscore') -> pd.DataFrame:\n    \"\"\"\n    Scale and normalize the specified features in the input DataFrame.\n    \n    This function performs two transformations:\n      1. Scaling: Rescales numerical features to a specified range (default is [0, 1]).\n      2. Normalization: Applies normalization to standardize features. The default normalization\n         method is 'zscore' (transforming data to have zero mean and unit variance).\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing features to be transformed.\n        feature_columns (Optional[List[str]]): List of columns to be scaled and normalized. \n                                               If None, all numeric columns are processed.\n        scaling_range (Optional[tuple]): Desired range for scaling as (min, max). Defaults to (0, 1).\n        normalization_method (str): The normalization method to apply. Supported:\n                                    'zscore' for standard normalization.\n                                    Additional methods can be incorporated if needed.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with scaled and normalized feature values.\n\n    Edge Cases:\n        - If the specified feature_columns list is empty or none of the columns are numeric,\n          the original DataFrame is returned.\n        - If an invalid normalization method is specified, behavior is undefined (implementation should handle errors).\n\n    Assumptions:\n        - The input DataFrame is valid and contains numeric types for the scaling and normalization.\n        - The normalization_method adheres to supported methods which are validated elsewhere.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Preprocessing/Feature Engineering/Data Transformation/one-hot encoding",
                                                        "Preprocessing/Feature Engineering/Data Transformation/scaling and normalization"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "feature_extraction.py",
                                                    "path": "src/workflow/preprocessing/feature_engineering/feature_extraction.py",
                                                    "code": "import pandas as pd\nfrom typing import Optional\n\ndef extract_pca_features(data: pd.DataFrame, n_components: int=2, method: str='svd') -> pd.DataFrame:\n    \"\"\"\n    Apply Principal Component Analysis (PCA) to extract principal components as features from the input data.\n    \n    This function is designed for the preprocessing pipeline's feature extraction stage, enabling dimensionality\n    reduction via PCA. It provides an interface for transforming the input DataFrame into a reduced set of features\n    while preserving as much variance as possible from the original dataset.\n    \n    Args:\n        data (pd.DataFrame): The input data containing the features to be transformed.\n        n_components (int, optional): The number of principal components to extract. Defaults to 2.\n        method (str, optional): The PCA computation method to use. Accepted values may include 'svd' for singular value decomposition,\n                                or other method identifiers as defined in the underlying PCA implementation. Defaults to 'svd'.\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the extracted principal components, with at most n_components columns.\n    \n    Edge Cases & Assumptions:\n        - It is assumed that the input DataFrame 'data' is preprocessed (e.g., missing values handled) prior to PCA.\n        - The function does not handle scaling; any required scaling should occur in an earlier pipeline stage.\n        - The 'method' parameter allows for future extension to different PCA computation algorithms.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Preprocessing/Feature Engineering/Feature Extraction/pca"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "data_augmentation.py",
                                                    "path": "src/workflow/preprocessing/feature_engineering/data_augmentation.py",
                                                    "code": "from typing import List, Optional\nimport pandas as pd\nimport numpy as np\nfrom typing import Optional, Dict\n\ndef augment_text_data(text_data: pd.DataFrame, augmentation_methods: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Augment textual data using specified augmentation methods.\n    \n    This function applies one or more text augmentation techniques to the input\n    DataFrame containing textual data. Typical augmentation methods may include\n    synonym replacement, random insertion, random swap, and deletion. This interface\n    enables users to enrich the dataset for downstream machine learning tasks,\n    especially in scenarios with limited text samples.\n    \n    Args:\n        text_data (pd.DataFrame): A DataFrame containing text data to augment.\n        augmentation_methods (Optional[List[str]]): A list of augmentation method names \n            to apply (e.g., ['synonym_replacement', 'random_insertion']). If None, default \n            augmentation methods may be used.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with the augmented text data.\n    \n    Edge Cases:\n        - If the input DataFrame is empty, the function returns an empty DataFrame.\n        - If augmentation_methods is None or empty, the function should handle it gracefully,\n          potentially applying a default method or returning the original DataFrame.\n    \n    Assumptions:\n        - The DataFrame has a column (or columns) that can be interpreted as textual data.\n    \"\"\"\n    pass\n\ndef augment_image_data(image_data: np.ndarray, augmentation_params: Optional[Dict[str, any]]=None) -> np.ndarray:\n    \"\"\"\n    Apply augmentation techniques to image data.\n    \n    This function performs image augmentation operations such as rotation, scaling,\n    flipping, and cropping on the input numpy array representing image(s). The optional \n    augmentation parameters allow for configuring specific details of the augmentation \n    process. This is particularly useful for expanding image datasets and improving\n    model robustness in computer vision tasks.\n    \n    Args:\n        image_data (np.ndarray): A numpy array representing one or more images.\n        augmentation_params (Optional[Dict[str, any]]): A dictionary containing augmentation \n            parameter settings (e.g., rotation angle, scaling factors, crop dimensions). If \n            None, default augmentation parameters should be applied.\n    \n    Returns:\n        np.ndarray: A numpy array of the augmented image data.\n    \n    Edge Cases:\n        - If the image_data array is empty, the function returns an empty array.\n        - If augmentation_params is None, default augmentation transformations are applied.\n    \n    Assumptions:\n        - The input numpy array conforms to the expected shape and data type for image data.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Preprocessing/Feature Engineering/Data Augmentation/image augmentation",
                                                        "Preprocessing/Feature Engineering/Data Augmentation/text augmentation"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "__init__.py",
                                                    "path": "src/workflow/preprocessing/feature_engineering/__init__.py",
                                                    "code": "",
                                                    "feature_paths": [],
                                                    "units": []
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "normalization",
                                            "path": "src/workflow/preprocessing/normalization",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "z_score.py",
                                                    "path": "src/workflow/preprocessing/normalization/z_score.py",
                                                    "code": "from typing import Optional, List\nimport pandas as pd\n\ndef normalize_z_score(df: pd.DataFrame, columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Normalize the input dataframe using Z-score normalization.\n\n    This function transforms specified numerical columns of the input dataframe so that they have a mean of 0 \n    and a standard deviation of 1. If the 'columns' parameter is None, the function will apply normalization \n    to all numeric columns present in the dataframe.\n\n    Args:\n        df (pd.DataFrame): The input dataframe containing the data to be normalized.\n        columns (Optional[List[str]]): A list of column names to normalize. If None, all numeric columns will be processed.\n\n    Returns:\n        pd.DataFrame: A new dataframe with the selected columns normalized using Z-score scaling.\n\n    Notes:\n        - The function assumes that the specified columns contain numeric data.\n        - It does not modify the original dataframe.\n        - Columns with zero standard deviation may lead to division by zero errors; handling such cases is left to the implementer.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Preprocessing/Normalization/Z-score Normalization/normalize to mean 0 and std 1"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "__init__.py",
                                                    "path": "src/workflow/preprocessing/normalization/__init__.py",
                                                    "code": "",
                                                    "feature_paths": [],
                                                    "units": []
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/workflow/preprocessing/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "evaluation",
                                    "path": "src/workflow/evaluation",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "accuracy.py",
                                            "path": "src/workflow/evaluation/accuracy.py",
                                            "code": "from typing import Optional\nimport numpy as np\n\ndef compute_f1_score(y_true: np.ndarray, y_pred: np.ndarray, average: Optional[str]='binary') -> float:\n    \"\"\"\n    Compute the F1 score, which is the harmonic mean of precision and recall.\n    \n    The F1 score is a widely used accuracy metric for classification tasks,\n    combining the precision and recall into a single score by taking their harmonic mean.\n    This function serves as a public-facing interface for computing the F1 score\n    for a given set of true labels and predicted labels.\n    \n    Args:\n        y_true (np.ndarray): Array of true class labels.\n        y_pred (np.ndarray): Array of predicted class labels.\n        average (str, optional): Defines the type of averaging performed on the data.\n             - 'binary': Only report results for the class specified by pos_label.\n             - 'micro', 'macro', 'weighted': Different averaging methods applicable to multi-class problems.\n             Defaults to 'binary'.\n    \n    Returns:\n        float: The computed F1 score as a floating-point value.\n    \n    Edge Cases:\n        - The function assumes that y_true and y_pred are valid numpy arrays of the same shape.\n        - For binary classification, it is assumed that the labels are encoded in a consistent manner.\n        - If the input arrays are empty or the metric is ill-defined, the behavior should be handled gracefully.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [
                                                "Evaluation/Accuracy Metrics/F1 Score/f1 score"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "performance.py",
                                            "path": "src/workflow/evaluation/performance.py",
                                            "code": "import numpy as np\nfrom typing import Optional, List, Any\n\ndef compute_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, labels: Optional[List[Any]]=None) -> np.ndarray:\n    \"\"\"\n    Compute the confusion matrix to evaluate the accuracy of a classification.\n\n    The confusion matrix is a summary table used to assess the performance of a classification\n    algorithm, by displaying the count of true positive, false positive, true negative, and false negative predictions.\n    This implementation compares the actual labels (y_true) to the predicted labels (y_pred).\n\n    Args:\n        y_true (np.ndarray): Array of true labels.\n        y_pred (np.ndarray): Array of predicted labels.\n        labels (Optional[List[Any]]): List of unique labels used in the classification to define the mapping\n            of matrix rows and columns. If None, this will be inferred from the input data.\n\n    Returns:\n        np.ndarray: A 2D numpy array where the element at index [i, j] represents the count of samples \n        with true label corresponding to the i-th class and predicted label corresponding to the j-th class.\n\n    Edge Cases:\n        - If the input arrays are empty or their lengths do not match, the behavior is undefined.\n        - If labels are not provided, they must be inferred correctly from y_true and y_pred to maintain consistency.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [
                                                "Evaluation/Performance Metrics/Confusion Matrix/confusion matrix"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/workflow/evaluation/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "file",
                                    "name": "__init__.py",
                                    "path": "src/workflow/__init__.py",
                                    "code": "",
                                    "feature_paths": [],
                                    "units": []
                                }
                            ]
                        },
                        {
                            "type": "directory",
                            "name": "data_engineering",
                            "path": "src/data_engineering",
                            "children": [
                                {
                                    "type": "directory",
                                    "name": "data_preparation",
                                    "path": "src/data_engineering/data_preparation",
                                    "children": [
                                        {
                                            "type": "directory",
                                            "name": "cleaning",
                                            "path": "src/data_engineering/data_preparation/cleaning",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "initial_cleaning.py",
                                                    "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                    "code": "import pandas as pd\n\ndef dropna_values(df: pd.DataFrame, axis: int=0, how: str='any') -> pd.DataFrame:\n    \"\"\"\n    Drop NA/null values from a DataFrame.\n\n    This function removes missing values from the DataFrame along the specified axis.\n    It uses pandas' dropna functionality.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        axis (int, optional): The axis along which to drop NA values (0 for rows, 1 for columns). Defaults to 0.\n        how (str, optional): Determine if row or column is removed based on 'any' or 'all' NA values. Defaults to 'any'.\n\n    Returns:\n        pd.DataFrame: DataFrame with NA values dropped.\n\n    Edge Cases:\n        Returns the original DataFrame if there are no NA values.\n    \"\"\"\n    pass\n\ndef convert_to_appropriate_types(df: pd.DataFrame, type_mapping: dict) -> pd.DataFrame:\n    \"\"\"\n    Convert DataFrame columns to appropriate types.\n\n    This function casts DataFrame columns based on a provided mapping from column names\n    to desired data types.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        type_mapping (dict): Dictionary mapping column names (str) to target data types.\n\n    Returns:\n        pd.DataFrame: DataFrame with columns converted to the specified types.\n\n    Edge Cases:\n        Columns not present in the mapping remain unchanged.\n    \"\"\"\n    pass\n\ndef clip_dataframe_values(df: pd.DataFrame, lower: float=None, upper: float=None) -> pd.DataFrame:\n    \"\"\"\n    Clip the values of a DataFrame to specified lower and/or upper bounds.\n\n    This function limits all numerical values in the DataFrame to be within\n    the specified range.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        lower (float, optional): Lower bound for clipping. Defaults to None.\n        upper (float, optional): Upper bound for clipping. Defaults to None.\n\n    Returns:\n        pd.DataFrame: DataFrame with values clipped within the specified bounds.\n\n    Edge Cases:\n        If both bounds are None, the original DataFrame is returned unchanged.\n    \"\"\"\n    pass\n\ndef detect_outliers_using_clustering(df: pd.DataFrame, clustering_params: dict) -> pd.DataFrame:\n    \"\"\"\n    Detect outliers in a DataFrame using clustering techniques.\n\n    This function applies a clustering algorithm to the DataFrame to identify\n    data points that are considered outliers based on cluster assignments.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        clustering_params (dict): Parameters for the clustering algorithm (e.g., number of clusters, distance metrics).\n\n    Returns:\n        pd.DataFrame: DataFrame with an additional indicator column for detected outliers.\n\n    Edge Cases:\n        If the DataFrame is empty or clustering fails, an unmodified DataFrame may be returned.\n    \"\"\"\n    pass\n\ndef drop_columns_with_missing(df: pd.DataFrame, threshold: float=0.5) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from a DataFrame that have a proportion of missing values above a threshold.\n\n    This function evaluates each column for the proportion of missing values\n    and removes those columns that exceed the defined threshold.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Maximum allowed fraction of missing values per column. Defaults to 0.5.\n\n    Returns:\n        pd.DataFrame: DataFrame after dropping columns with excessive missing data.\n\n    Edge Cases:\n        If no columns meet the dropping criteria, the original DataFrame is returned intact.\n    \"\"\"\n    pass\n\ndef tag_outliers(df: pd.DataFrame, method: str='IQR', factor: float=1.5) -> pd.DataFrame:\n    \"\"\"\n    Tag outliers in a DataFrame based on a specified statistical method.\n\n    This function identifies and tags outlier data points using either the IQR method\n    or another specified method and adds a flag column indicating outlier status.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        method (str, optional): Statistical method to use ('IQR' or other). Defaults to 'IQR'.\n        factor (float, optional): Multiplicative factor for determining thresholds. Defaults to 1.5.\n\n    Returns:\n        pd.DataFrame: DataFrame with an added column that flags outlier rows.\n\n    Edge Cases:\n        If the method is unsupported or no outliers are found, a flag column with default values is added.\n    \"\"\"\n    pass\n\ndef remove_by_standard_deviation(df: pd.DataFrame, threshold: float=3.0) -> pd.DataFrame:\n    \"\"\"\n    Remove rows from a DataFrame that deviate from the mean by more than a specified number of standard deviations.\n\n    This function calculates the standard deviation for numerical columns and removes rows\n    that fall outside the defined threshold range.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Number of standard deviations from the mean to consider as the limit. Defaults to 3.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with extreme outlier rows removed.\n\n    Edge Cases:\n        Returns the original DataFrame if no rows exceed the threshold.\n    \"\"\"\n    pass\n\ndef remove_outlier_rows(df: pd.DataFrame, outlier_indicator_column: str='is_outlier') -> pd.DataFrame:\n    \"\"\"\n    Remove rows identified as outliers from a DataFrame.\n\n    This function drops rows that have been flagged as outliers based on a specified indicator column.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        outlier_indicator_column (str, optional): Column name that indicates outlier status. Defaults to 'is_outlier'.\n\n    Returns:\n        pd.DataFrame: DataFrame with outlier rows removed.\n\n    Edge Cases:\n        If the indicator column does not exist, the original DataFrame is returned unchanged.\n    \"\"\"\n    pass\n\ndef remove_top_percent(df: pd.DataFrame, percent: float=1.0) -> pd.DataFrame:\n    \"\"\"\n    Remove the top percentage of extreme values from a DataFrame.\n\n    This function removes the rows corresponding to the top specified percentage \n    (by value) of a target numerical column, often used to exclude extreme outliers.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        percent (float, optional): The percentage of top extreme values to remove. Defaults to 1.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with the top percent of extreme values removed.\n\n    Edge Cases:\n        If the computed cutoff excludes no rows, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef standardize_string_data(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n    Standardize string data in specified DataFrame columns.\n\n    This function ensures that string data in the selected columns is standardized,\n    which may include trimming, lowercasing, or other normalization.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list): List of column names (str) to standardize.\n\n    Returns:\n        pd.DataFrame: DataFrame with standardized string data in specified columns.\n\n    Edge Cases:\n        If a specified column is not of string type, it will be skipped.\n    \"\"\"\n    pass\n\ndef standardize_date_formats(df: pd.DataFrame, date_columns: list, date_format: str='%Y-%m-%d') -> pd.DataFrame:\n    \"\"\"\n    Standardize the date formats in specified DataFrame columns.\n\n    This function converts dates in given columns to a uniform date format.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        date_columns (list): List of column names (str) containing date values.\n        date_format (str, optional): The target format for dates. Defaults to \"%Y-%m-%d\".\n\n    Returns:\n        pd.DataFrame: DataFrame with dates in standardized format.\n\n    Edge Cases:\n        If a date conversion fails, the original value is retained.\n    \"\"\"\n    pass\n\ndef convert_units(df: pd.DataFrame, conversion_mapping: dict) -> pd.DataFrame:\n    \"\"\"\n    Convert units of measurement for specified columns in a DataFrame.\n\n    This function applies unit conversions to DataFrame columns based on a provided mapping,\n    where each key is a column and the value is a conversion function or factor.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        conversion_mapping (dict): A dictionary mapping column names to conversion parameters.\n\n    Returns:\n        pd.DataFrame: DataFrame with converted unit values.\n\n    Edge Cases:\n        If a column is not present or conversion fails, the column remains unchanged.\n    \"\"\"\n    pass\n\ndef use_forward_fill(df: pd.DataFrame, columns: list=None) -> pd.DataFrame:\n    \"\"\"\n    Fill missing values in a DataFrame using forward fill method.\n\n    This function applies a forward-fill strategy to impute missing values,\n    propagating the last valid observation forward.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list, optional): List of column names (str) to forward fill. \n                                  If None, applies to all columns.\n\n    Returns:\n        pd.DataFrame: DataFrame with missing values filled using forward fill.\n\n    Edge Cases:\n        If there are no missing values, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef use_z_score_method(df: pd.DataFrame, threshold: float=3.0) -> pd.DataFrame:\n    \"\"\"\n    Identify outliers in a DataFrame using the Z-score method.\n\n    This function calculates the Z-score for numerical columns and flags rows\n    where the absolute Z-score exceeds the defined threshold.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Z-score threshold to identify outliers. Defaults to 3.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with an additional indicator for outliers based on the Z-score.\n\n    Edge Cases:\n        If no outliers are present according to the threshold, the DataFrame is returned unmodified.\n    \"\"\"\n    pass\n\ndef use_iqr_method(df: pd.DataFrame, factor: float=1.5) -> pd.DataFrame:\n    \"\"\"\n    Identify outliers in a DataFrame using the IQR method.\n\n    This function computes the interquartile range for numerical columns and flags rows\n    that lie outside the acceptable range defined by a multiplier factor.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        factor (float, optional): Multiplier for the IQR to set thresholds for outlier detection. Defaults to 1.5.\n\n    Returns:\n        pd.DataFrame: DataFrame with an indicator marking rows considered outliers based on the IQR method.\n\n    Edge Cases:\n        If the IQR cannot be computed (e.g., insufficient data), the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef standardize_numeric_formats(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n    Standardize the numeric formats in specified DataFrame columns.\n\n    This function ensures that numerical columns are formatted uniformly,\n    which may include rounding, setting precision, or applying consistent scaling.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list): List of column names (str) to standardize numerically.\n\n    Returns:\n        pd.DataFrame: DataFrame with standardized numeric columns.\n\n    Edge Cases:\n        If a non-numeric column is included in the columns list, it is skipped.\n    \"\"\"\n    pass\n\ndef fill_with_constant(df: pd.DataFrame, value, columns: list=None) -> pd.DataFrame:\n    \"\"\"\n    Fill missing or undesirable values in a DataFrame with a constant value.\n\n    This function replaces missing values or outlier values with a specified constant across\n    all or selected columns.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        value: The constant value to fill in.\n        columns (list, optional): List of column names (str) to apply the fill.\n                                  If None, applies to entire DataFrame.\n\n    Returns:\n        pd.DataFrame: DataFrame with specified columns filled with the constant value.\n\n    Edge Cases:\n        If no columns are specified and the DataFrame has no missing values, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef reset_index_to_default(df: pd.DataFrame, drop: bool=True) -> pd.DataFrame:\n    \"\"\"\n    Reset the index of a DataFrame to the default integer index.\n\n    This function reinitializes the DataFrame index, optionally dropping the old index.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        drop (bool, optional): Whether to drop the existing index. Defaults to True.\n\n    Returns:\n        pd.DataFrame: DataFrame with the index reset to the default integer index.\n\n    Edge Cases:\n        If the index is already the default range index, the DataFrame is returned unchanged.\n    \"\"\"\n    pass\n\ndef convert_text_to_number(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n    Convert textual representations of numbers in specified DataFrame columns to numeric types.\n\n    This function attempts to cast string representations of numbers to numeric values.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list): List of column names (str) that contain textual numbers.\n\n    Returns:\n        pd.DataFrame: DataFrame with the specified columns converted to numeric types.\n\n    Edge Cases:\n        Improperly formatted strings that cannot be converted will remain unchanged or be set to NaN.\n    \"\"\"\n    pass\n\ndef correct_data_inconsistencies(df: pd.DataFrame, correction_rules: dict) -> pd.DataFrame:\n    \"\"\"\n    Correct data inconsistencies in a DataFrame based on predefined rules.\n\n    This function applies a set of rules to standardize and correct inconsistent data entries,\n    ensuring data quality and uniformity.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        correction_rules (dict): A mapping from column names to correction functions or rules.\n\n    Returns:\n        pd.DataFrame: DataFrame with corrected data inconsistencies.\n\n    Edge Cases:\n        If a column does not have a corresponding rule, it is left unchanged.\n    \"\"\"\n    pass\n\ndef drop_null_columns(df: pd.DataFrame, threshold: float=1.0) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from a DataFrame that consist entirely of null values.\n\n    This function examines each column and drops those that have a fraction of non-null entries\n    lower than the specified threshold (typically 1.0 to indicate all values are null).\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Fraction of non-null values required to retain the column.\n                                     Defaults to 1.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with null-only columns removed.\n\n    Edge Cases:\n        If no columns meet the criteria for dropping, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef reset_and_drop_old_index(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Reset the index of a DataFrame to default and drop the old index column if it exists.\n\n    This function reinitializes the DataFrame index and ensures any previous index column is discarded.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n\n    Returns:\n        pd.DataFrame: DataFrame with a new default index and without the old index column.\n\n    Edge Cases:\n        If there is no old index column to drop, the function behaves like a simple index reset.\n    \"\"\"\n    pass\n\ndef replace_with_default_value(df: pd.DataFrame, default_mapping: dict) -> pd.DataFrame:\n    \"\"\"\n    Replace specified values in a DataFrame with default values.\n\n    This function uses a mapping of column names to default replacement values to\n    substitute certain data entries in a DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        default_mapping (dict): Dictionary where keys are column names and values are the default value to apply.\n\n    Returns:\n        pd.DataFrame: DataFrame with specified values replaced by defaults.\n\n    Edge Cases:\n        If a column in the mapping does not exist in the DataFrame, it is ignored.\n    \"\"\"\n    pass\n\ndef drop_rows(df: pd.DataFrame, condition) -> pd.DataFrame:\n    \"\"\"\n    Drop rows from a DataFrame that meet a specified condition.\n\n    This function removes rows based on a provided condition, which can be a boolean mask or a callable\n    that takes a row and returns a boolean value.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        condition: A boolean array, boolean Series, or callable that defines the drop condition.\n\n    Returns:\n        pd.DataFrame: DataFrame with rows dropped based on the condition.\n\n    Edge Cases:\n        If the condition does not match any rows, the original DataFrame is returned.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/clip values",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/convert text to number",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/convert to appropriate types",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/convert units",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/correct data inconsistencies",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/detect using clustering",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/drop columns with missing",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/drop null columns",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/drop rows",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/dropna",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/fill with constant",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/remove by standard deviation",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/remove outlier rows",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/remove top 1%",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/replace with default value",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/reset and drop old index",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/reset index to default",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/standardize date formats",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/standardize numeric formats",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/standardize string data",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/tag outliers",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/use forward fill",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/use iqr method",
                                                        "Data Preparation/Data Cleaning/Initial Cleaning/use z-score method"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "supplementary_cleaning.py",
                                                    "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                    "code": "import pandas as pd\n\nclass OutlierFilter:\n    \"\"\"\n    Provides methods for identifying and handling outliers in a DataFrame.\n\n    This class supports multiple techniques including filtering by percentile, identifying high outliers,\n    threshold based filtering, IQR-based removal, and generalized outlier identification.\n    \"\"\"\n\n    def filter_by_percentile(self, df: pd.DataFrame, percentile: float) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Filters rows in the DataFrame based on a specified percentile threshold.\n        \n        Extended Explanation:\n            This method removes or marks rows in the DataFrame that fall below or above a given percentile value,\n            depending on the implementation of thresholding logic. It is useful for mitigating the influence of extreme values.\n        \n        Args:\n            df (pd.DataFrame): The input DataFrame to filter.\n            percentile (float): The percentile threshold (between 0 and 100) to use for filtering.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with rows filtered based on the percentile.\n        \n        Raises:\n            ValueError: If the percentile is not between 0 and 100.\n        \"\"\"\n        pass\n\n    def filter_high_outliers(self, df: pd.DataFrame, high_threshold: float) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Filters out rows that contain exceptionally high outlier values.\n        \n        Extended Explanation:\n            This method detects and filters rows where the values exceed a specified high threshold.\n            It can be applied to a specific column or across the DataFrame if aggregated.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            high_threshold (float): The threshold above which values are considered high outliers.\n        \n        Returns:\n            pd.DataFrame: DataFrame with high outliers removed or flagged.\n        \n        Raises:\n            ValueError: If high_threshold is not a positive number.\n        \"\"\"\n        pass\n\n    def filter_by_threshold(self, df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Filters rows based on a fixed numeric threshold.\n        \n        Extended Explanation:\n            This method removes rows where specified numeric values exceed or fall short of a given threshold,\n            depending on business requirements.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            threshold (float): Numeric threshold for filtering.\n        \n        Returns:\n            pd.DataFrame: DataFrame filtered according to the threshold.\n        \n        Raises:\n            ValueError: If threshold is non-numeric.\n        \"\"\"\n        pass\n\n    def remove_based_on_iqr(self, df: pd.DataFrame, factor: float=1.5) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes outliers using the IQR method.\n        \n        Extended Explanation:\n            This method calculates the Interquartile Range (IQR) and filters out rows that\n            lie beyond the range defined by the specified factor multiplied by the IQR.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            factor (float): The multiplier for the IQR to define the acceptable range.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with outliers removed based on the IQR method.\n        \n        Raises:\n            ValueError: If factor is not a positive number.\n        \"\"\"\n        pass\n\n    def identify_outliers(self, df: pd.DataFrame, method: str='standard') -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Identifies outliers in the DataFrame using a specified detection method.\n        \n        Extended Explanation:\n            This method labels or flags rows that are potential outliers following the specified \n            detection strategy (e.g., standard deviation, modified z-score, etc.). No rows are removed.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            method (str): The method used for detecting outliers. Defaults to 'standard'.\n        \n        Returns:\n            pd.DataFrame: DataFrame with an additional column flagging outliers.\n        \n        Raises:\n            ValueError: If the specified method is not supported.\n        \"\"\"\n        pass\n\nclass DuplicateCleaner:\n    \"\"\"\n    Implements methods for dealing with duplicate data in a DataFrame.\n\n    This class covers keeping the first occurrence of duplicate records, dropping duplicate columns,\n    removing exact duplicate rows, and handling near-duplicate rows through fuzzy deduplication techniques.\n    \"\"\"\n\n    def keep_first(self, df: pd.DataFrame, subset: list=None) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Keeps the first occurrence of duplicate rows and removes subsequent duplicates.\n        \n        Extended Explanation:\n            This method checks for duplicate rows based on all columns or a specified subset of columns,\n            and retains only the first occurrence while removing others.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing possible duplicate rows.\n            subset (list, optional): List of column names to consider for identifying duplicates.\n        \n        Returns:\n            pd.DataFrame: DataFrame with duplicates removed, preserving the first occurrence.\n        \n        Raises:\n            ValueError: If 'df' is not a valid DataFrame.\n        \"\"\"\n        pass\n\n    def drop_duplicate_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes columns that are exact duplicates of each other.\n        \n        Extended Explanation:\n            This method examines the DataFrame for columns that have identical content and drops the redundant ones.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n        \n        Returns:\n            pd.DataFrame: DataFrame with duplicate columns removed.\n        \n        Raises:\n            ValueError: If the DataFrame is empty or improperly formatted.\n        \"\"\"\n        pass\n\n    def remove_exact_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes rows that are exactly duplicated.\n        \n        Extended Explanation:\n            This method identifies rows that are completely identical across all columns and removes the duplicates.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to be de-duplicated.\n        \n        Returns:\n            pd.DataFrame: DataFrame with exact duplicate rows removed.\n        \n        Raises:\n            ValueError: If 'df' has unexpected structure.\n        \"\"\"\n        pass\n\n    def fuzzy_deduplication(self, df: pd.DataFrame, similarity_threshold: float=0.8) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Applies fuzzy matching to identify and remove near-duplicate rows.\n        \n        Extended Explanation:\n            This method uses a similarity threshold to determine which rows are sufficiently similar \n            to be considered duplicates and then removes or flags them.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame to process.\n            similarity_threshold (float): Similarity threshold (between 0 and 1); values closer to 1 denote stricter matching.\n        \n        Returns:\n            pd.DataFrame: DataFrame with near-duplicates handled.\n        \n        Raises:\n            ValueError: If similarity_threshold is outside the [0, 1] range.\n        \"\"\"\n        pass\n\nclass DataConversionScaler:\n    \"\"\"\n    Provides conversion and scaling operations for numeric data in a DataFrame.\n\n    This class includes methods for converting data types to integers and ordinals,\n    applying decimal scaling, and standardizing data using robust scaling techniques.\n    \"\"\"\n\n    def convert_to_integer(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Converts specified columns in a DataFrame to integer type.\n        \n        Extended Explanation:\n            This method is used to ensure that numeric data is stored as integers. It processes\n            the provided columns and converts any float or string representations of numbers to integers.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing the columns to convert.\n            columns (list): List of column names to be converted to integer type.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with the specified columns converted to integer.\n        \n        Raises:\n            TypeError: If conversion cannot be performed due to incompatible data types.\n        \"\"\"\n        pass\n\n    def convert_to_ordinal(self, df: pd.DataFrame, columns: list, mapping: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Converts specified categorical columns to ordinal values.\n        \n        Extended Explanation:\n            This method applies a mapping dictionary to convert nominal or categorical data into ordinal form,\n            which can facilitate further numeric processing.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing categorical columns.\n            columns (list): List of column names to convert.\n            mapping (dict): Dictionary mapping original values to ordinal integers.\n        \n        Returns:\n            pd.DataFrame: DataFrame with specified columns converted to ordinal values.\n        \n        Raises:\n            KeyError: If a column value is missing from the mapping.\n        \"\"\"\n        pass\n\n    def apply_decimal_scaling(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Scales numeric columns using decimal scaling.\n        \n        Extended Explanation:\n            This method normalizes numeric data by shifting the decimal point. This is useful for adjusting the magnitude\n            of numbers without changing their distribution.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing numerical columns.\n            columns (list): List of columns to be scaled.\n        \n        Returns:\n            pd.DataFrame: DataFrame with scaled numeric columns.\n        \n        Raises:\n            ValueError: If columns contain non-numeric values.\n        \"\"\"\n        pass\n\n    def standardize_using_robust_scaling(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Standardizes data using techniques that are robust to outliers.\n        \n        Extended Explanation:\n            Rather than using mean and standard deviation, this method employs robust statistical measures\n            such as the median and IQR to reduce the influence of extreme values.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing columns to be standardized.\n            columns (list): List of column names to standardize.\n        \n        Returns:\n            pd.DataFrame: DataFrame with robust-scaled values.\n        \n        Raises:\n            ValueError: If provided columns contain non-numeric data.\n        \"\"\"\n        pass\n\nclass CategoricalCleaner:\n    \"\"\"\n    Provides methods for cleaning and standardizing categorical or textual data in a DataFrame.\n\n    This includes operations to standardize categorical formats, remove special characters using regex,\n    and fill missing or problematic entries with either custom values or zero.\n    \"\"\"\n\n    def standardize_categorical_data(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Standardizes the representation of categorical data in the DataFrame.\n        \n        Extended Explanation:\n            This method adjusts the string formats in the specified columns to a consistent standard, which\n            may involve trimming spaces, changing case, or other normalization procedures.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            columns (list): List of columns containing categorical data to standardize.\n        \n        Returns:\n            pd.DataFrame: DataFrame with standardized categorical data.\n        \n        Raises:\n            ValueError: If any of the specified columns are not of a string type.\n        \"\"\"\n        pass\n\n    def remove_special_characters(self, df: pd.DataFrame, columns: list, pattern: str='[^a-zA-Z0-9\\\\s]') -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes special characters from the specified string columns.\n        \n        Extended Explanation:\n            Using a regular expression pattern, this method cleans text data by removing characters that\n            do not match the allowed set defined by the pattern.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing textual data.\n            columns (list): List of column names to clean.\n            pattern (str): Regex pattern specifying allowed characters. Defaults to alphanumeric and whitespace.\n        \n        Returns:\n            pd.DataFrame: DataFrame with special characters removed from specified columns.\n        \n        Raises:\n            re.error: If the provided regex pattern is invalid.\n        \"\"\"\n        pass\n\n    def fill_with_custom_value(self, df: pd.DataFrame, columns: list, custom_value) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Fills missing or problematic entries in specified columns with a custom value.\n        \n        Extended Explanation:\n            This method replaces NaNs or erroneous entries within the given columns with a predetermined custom value.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame with missing or problematic entries.\n            columns (list): List of columns to fill.\n            custom_value: The value to use for filling missing entries.\n        \n        Returns:\n            pd.DataFrame: DataFrame with missing entries replaced by the custom value.\n        \n        Raises:\n            TypeError: If the custom_value is of the wrong type for the target column.\n        \"\"\"\n        pass\n\n    def fill_with_zero(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Fills missing or invalid values in designated columns with zero.\n        \n        Extended Explanation:\n            This method scans the specified columns for NaN or other placeholders for missing data and fills them with zero.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to process.\n            columns (list): List of column names where zeros will replace missing values.\n        \n        Returns:\n            pd.DataFrame: DataFrame with missing values filled with zero.\n        \n        Raises:\n            ValueError: If the operation fails due to type incompatibilities.\n        \"\"\"\n        pass\n\nclass ConsistencyCorrector:\n    \"\"\"\n    Provides methods for correcting inconsistencies and errors in data formatting.\n\n    This class includes functions that adjust data to fixed formats,\n    correct typographical errors, enforce consistency across entries, and resolve conflicts.\n    \"\"\"\n\n    def fix_inconsistent_data_formats(self, df: pd.DataFrame, columns: list, format_rules: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Standardizes data formats in specified columns based on provided rules.\n        \n        Extended Explanation:\n            This method applies a set of formatting rules to the designated columns to ensure the data\n            adheres to consistent standards.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to standardize.\n            columns (list): List of column names to adjust.\n            format_rules (dict): Mapping of column names to formatting rules.\n        \n        Returns:\n            pd.DataFrame: DataFrame with standardized formats.\n        \n        Raises:\n            KeyError: If a column is missing from the provided format_rules.\n        \"\"\"\n        pass\n\n    def correct_typos(self, df: pd.DataFrame, columns: list, typo_dict: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Corrects typographical errors in specified textual columns.\n        \n        Extended Explanation:\n            This method replaces common typos with their correct forms using a dictionary mapping.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            columns (list): List of columns to inspect for typos.\n            typo_dict (dict): Dictionary mapping incorrect spellings to correct ones.\n        \n        Returns:\n            pd.DataFrame: DataFrame with typos corrected.\n        \n        Raises:\n            ValueError: If typo_dict is empty or columns are missing.\n        \"\"\"\n        pass\n\n    def enforce_consistency_rules(self, df: pd.DataFrame, rules: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Enforces predefined consistency rules across the DataFrame.\n        \n        Extended Explanation:\n            This method applies a series of rules to ensure that data across columns adheres to\n            consistent formats and logical relationships.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to process.\n            rules (dict): Dictionary containing consistency rules to apply.\n        \n        Returns:\n            pd.DataFrame: DataFrame with consistency rules enforced.\n        \n        Raises:\n            ValueError: If rules are improperly defined.\n        \"\"\"\n        pass\n\n    def correct_data_entry_errors(self, df: pd.DataFrame, columns: list, error_corrections: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Corrects known data entry errors in specified columns.\n        \n        Extended Explanation:\n            This method uses an error correction mapping to replace incorrect entries due to data entry errors.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing errors.\n            columns (list): List of columns to correct.\n            error_corrections (dict): Dictionary mapping erroneous values to correct ones.\n        \n        Returns:\n            pd.DataFrame: DataFrame with data entry errors corrected.\n        \n        Raises:\n            KeyError: If an expected correction key is missing.\n        \"\"\"\n        pass\n\n    def resolve_conflicting_data(self, df: pd.DataFrame, conflict_resolution_rules: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Resolves conflicting data entries using specified rules.\n        \n        Extended Explanation:\n            This method applies conflict resolution strategies to deal with data contradictions,\n            ensuring that the resulting DataFrame has coherent and consistent data.\n        \n        Args:\n            df (pd.DataFrame): DataFrame with conflicting entries.\n            conflict_resolution_rules (dict): Dictionary of rules to resolve conflicts.\n        \n        Returns:\n            pd.DataFrame: DataFrame with resolved conflicting data.\n        \n        Raises:\n            ValueError: If conflict_resolution_rules do not cover all conflicts.\n        \"\"\"\n        pass\n\nclass ZeroColumnHandler:\n    \"\"\"\n    Provides methods for identifying and handling columns composed entirely or predominantly of zeros.\n\n    This class includes methods to detect columns with a high proportion of zeroes and to remove columns which contain \n    only zero values.\n    \"\"\"\n\n    def identify_zero_columns(self, df: pd.DataFrame, threshold: float=0.0) -> list:\n        \"\"\"\n        Summary:\n            Identifies columns in the DataFrame where the proportion of zero values meets or exceeds the threshold.\n        \n        Extended Explanation:\n            This method evaluates each column in the DataFrame and returns a list of column names that have a zero-value\n            proportion greater than or equal to the provided threshold.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to inspect.\n            threshold (float, optional): The minimum proportion of zeros required to classify a column as zero-dominated.\n                                         Defaults to 0.0 (all zeros).\n        \n        Returns:\n            list: A list of column names that meet the zero column criteria.\n        \n        Raises:\n            ValueError: If the threshold is not between 0 and 1.\n        \"\"\"\n        pass\n\n    def drop_columns_with_all_zeros(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Drops columns from the DataFrame that contain only zeros.\n        \n        Extended Explanation:\n            This method scans each column and removes those where every element is zero. This is useful for cleaning\n            datasets that have redundant or non-informative zero-only columns.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with columns containing only zeros removed.\n        \n        Raises:\n            ValueError: If no columns can be dropped due to incompatible data types.\n        \"\"\"\n        pass\n\ndef apply_active_learning_strategy(df: 'pd.DataFrame', strategy_params: dict) -> 'pd.DataFrame':\n    \"\"\"\n    Summary:\n        Applies an active learning strategy to modify the DataFrame for sample selection or re-weighting.\n    \n    Extended Explanation:\n        This function integrates active learning techniques into the data cleaning process. It processes the provided DataFrame \n        using parameters that define how samples should be selected or weighted. The resulting DataFrame is modified to better \n        suit active learning workflows.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame on which to apply the active learning strategy.\n        strategy_params (dict): A dictionary containing parameters for the active learning strategy, such as sampling \n                                criteria or re-weighting factors.\n    \n    Returns:\n        pd.DataFrame: A modified DataFrame adjusted according to the active learning strategy.\n    \n    Raises:\n        ValueError: If strategy_params is missing required keys or contains invalid values.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/active learning strategies",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/convert to integer",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/convert to ordinal",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/correct data entry errors",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/correct typos",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/decimal scaling",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/drop columns with all zeros",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/drop duplicate columns",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/enforce consistency rules",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/fill with custom value",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/fill with zero",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/filter based on percentile",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/filter by threshold",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/filter high outliers",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/fix inconsistent data formats",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/fuzzy deduplication",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/identify outliers",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/identify zero columns",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/keep first",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/remove based on iqr",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/remove exact duplicates",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/remove special characters",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/resolve conflicting data",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/standardize categorical data",
                                                        "Data Preparation/Data Cleaning/Supplementary Adjustments/standardize using robust scaling"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "__init__.py",
                                                    "path": "src/data_engineering/data_preparation/cleaning/__init__.py",
                                                    "code": "",
                                                    "feature_paths": [],
                                                    "units": []
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "imputation",
                                            "path": "src/data_engineering/data_preparation/imputation",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "imputation_methods.py",
                                                    "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                    "code": "import pandas as pd\n\ndef group_based_imputation(df: pd.DataFrame, group_col: str, target_col: str, strategy: str='mean') -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column by computing grouped statistics.\n    \n    This function imputes missing values in a specified target column within each group defined by 'group_col'.\n    The imputation strategy can be set to either 'mean' or 'median' to perform the corresponding imputation.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        group_col (str): Column name to group the data by.\n        target_col (str): The target column in which missing values are to be imputed.\n        strategy (str): The imputation strategy. Expected values are \"mean\" or \"median\". Defaults to \"mean\".\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the group-based strategy.\n        \n    Edge Cases:\n        - If the group or target columns do not exist, processing should be handled upstream.\n        - Assumes that the groups are non-empty and the target column is numeric.\n    \"\"\"\n    pass\n\ndef simple_imputation(df: pd.DataFrame, target_col: str, fill_value) -> pd.DataFrame:\n    \"\"\"\n    Perform a simple imputation on the target column using a constant fill value.\n    \n    This function replaces missing values in a specified column with a provided constant.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The name of the column in which to impute missing data.\n        fill_value: The constant value to use for imputing missing entries.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column replaced by the fill_value.\n        \n    Assumptions:\n        - The fill_value is appropriate for the data type of the target column.\n    \"\"\"\n    pass\n\ndef identify_missing(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Identify missing values in the DataFrame and return a DataFrame indicating the positions of missing data.\n    \n    This function scans the entire DataFrame to detect locations (rows and columns) with missing data.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame to be analyzed for missing values.\n        \n    Returns:\n        pd.DataFrame: A boolean DataFrame of the same shape as input, where True indicates a missing value.\n        \n    Edge Cases:\n        - An empty DataFrame is returned if no data is present.\n    \"\"\"\n    pass\n\ndef weighted_mean_imputation(df: pd.DataFrame, target_col: str, weight_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using a weighted mean computed from a specified weight column.\n    \n    The weighted mean is calculated considering the weights provided, and missing values in the target column\n    are replaced with the computed weighted mean.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The column in which to impute missing values.\n        weight_col (str): The column containing weights for calculating the weighted mean.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the weighted mean.\n        \n    Assumptions:\n        - Both target_col and weight_col exist in the DataFrame and contain numeric data.\n    \"\"\"\n    pass\n\ndef delete_columns(df: pd.DataFrame, missing_threshold: float=0.5) -> pd.DataFrame:\n    \"\"\"\n    Delete columns from the DataFrame based on a missing data threshold.\n    \n    This function drops columns where the proportion of missing values exceeds the specified threshold.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with potential missing data.\n        missing_threshold (float): The maximum allowed fraction of missing data for a column to be retained.\n                                  Columns with a higher fraction of missing data will be deleted. Defaults to 0.5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with columns removed if they exceeded the missing data threshold.\n        \n    Edge Cases:\n        - If missing_threshold is set to a value outside [0,1], behavior is undefined.\n    \"\"\"\n    pass\n\ndef local_median_imputation(df: pd.DataFrame, target_col: str, neighborhood: int=5) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column by computing the median over a local neighborhood.\n    \n    This function replaces missing values with the median computed from a specified number of neighboring entries.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        target_col (str): The column in which missing data is to be imputed.\n        neighborhood (int): The number of adjacent data points to consider for computing the local median. Defaults to 5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using a local median.\n        \n    Assumptions:\n        - The DataFrame is sorted appropriately such that a local neighborhood is meaningful.\n    \"\"\"\n    pass\n\ndef global_median_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using the global median.\n    \n    This function computes the median of the entire column (ignoring missing entries) and replaces missing values with it.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing data.\n        target_col (str): The name of the column where missing values will be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed with its global median.\n        \n    Constraints:\n        - Assumes that the target column contains numeric data.\n    \"\"\"\n    pass\n\ndef conditional_imputation(df: pd.DataFrame, target_col: str, condition: dict, strategy: str='mean') -> pd.DataFrame:\n    \"\"\"\n    Perform conditional imputation on the target column based on provided conditions.\n    \n    Depending on the chosen strategy, either the mean or median is computed on subsets of data that satisfy\n    the condition specified by a dictionary of column-value pairs. Missing values are imputed accordingly.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        target_col (str): The column in which missing values will be imputed.\n        condition (dict): A dictionary where keys are column names and values define the condition to filter the DataFrame.\n        strategy (str): The imputation strategy. Expected values are \"mean\" or \"median\". Defaults to \"mean\".\n        \n    Returns:\n        pd.DataFrame: A DataFrame with conditionally imputed values in the target column.\n        \n    Assumptions:\n        - The condition provided effectively segments the DataFrame into meaningful subsets.\n    \"\"\"\n    pass\n\ndef pairwise_deletion(df: pd.DataFrame, target_cols: list) -> pd.DataFrame:\n    \"\"\"\n    Apply pairwise deletion to handle missing data across specified columns.\n    \n    This function removes rows based on the presence of missing values in a pairwise manner from the specified columns.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing data.\n        target_cols (list): List of column names on which the pairwise deletion should be based.\n        \n    Returns:\n        pd.DataFrame: A DataFrame where rows with missing data in specified column pairs are removed.\n        \n    Edge Cases:\n        - If target_cols is empty, the function will return the original DataFrame.\n    \"\"\"\n    pass\n\ndef binary_mode_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing binary data in the target column using the mode.\n    \n    This function determines the most frequent binary value in the target column and uses it to fill missing entries.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing binary data.\n        target_col (str): The name of the binary column to be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the binary target column imputed with its mode.\n        \n    Constraints:\n        - Assumes that the target column only contains two distinct values aside from missing values.\n    \"\"\"\n    pass\n\ndef polynomial_interpolation_imputation(df: pd.DataFrame, target_col: str, degree: int=2) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using polynomial interpolation.\n    \n    This function fits a polynomial of a specified degree to the non-missing data and uses it to estimate missing values.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with gaps in the target column.\n        target_col (str): The column to interpolate.\n        degree (int): The degree of the polynomial to be used for interpolation. Defaults to 2.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed by polynomial interpolation.\n        \n    Remarks:\n        - Suitable for time series or sequential data where polynomial trends are expected.\n    \"\"\"\n    pass\n\ndef conditional_mode_imputation(df: pd.DataFrame, target_col: str, condition: dict) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column conditionally using the mode.\n    \n    The function segments the DataFrame based on given conditions and fills missing values with the most frequent value\n    within each segment.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing data.\n        target_col (str): The column to be imputed.\n        condition (dict): A dictionary specifying column names and their desired values to filter the DataFrame.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values conditionally imputed using the mode.\n        \n    Edge Cases:\n        - If the condition does not segregate the data, the global mode might be used.\n    \"\"\"\n    pass\n\ndef optimized_mode_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Perform mode imputation on large datasets with optimization considerations.\n    \n    This function imputes missing values in the target column by computing the mode in a manner optimized for large datasets,\n    potentially using chunk processing or memory-efficient algorithms.\n    \n    Args:\n        df (pd.DataFrame): The large input DataFrame with missing values.\n        target_col (str): The column on which to perform mode imputation.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values imputed using an optimized mode computation.\n        \n    Considerations:\n        - Designed for performance and memory efficiency on very large DataFrames.\n    \"\"\"\n    pass\n\ndef linear_interpolation_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using linear interpolation.\n    \n    The function linearly interpolates missing values based on surrounding data points in the target column.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The name of the column for linear interpolation.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed via linear interpolation.\n        \n    Constraints:\n        - Assumes the data is ordered so that linear interpolation is meaningful.\n    \"\"\"\n    pass\n\ndef mode_imputation(df: pd.DataFrame, target_col: str, subset: list=None) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using mode imputation.\n    \n    This function computes the mode either for the entire column or for a specified subset of data\n    (if 'subset' is provided) and fills missing values with the most frequent value.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing data.\n        target_col (str): The column in which to perform mode imputation.\n        subset (list, optional): A list of column names to consider when computing the mode. If None, the mode is\n                                 computed using the entire column. Defaults to None.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the mode.\n        \n    Assumptions:\n        - The target column contains categorical or numerical data for which mode is an appropriate statistic.\n    \"\"\"\n    pass\n\ndef global_mean_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using the global mean.\n    \n    This function computes the mean of the target column over the entire DataFrame (ignoring missing data)\n    and fills missing values with this mean.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The column to be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the global mean.\n        \n    Constraints:\n        - Assumes that the target column contains numeric data.\n    \"\"\"\n    pass\n\ndef flag_outliers(df: pd.DataFrame, target_col: str, method: str='IQR', factor: float=1.5) -> pd.DataFrame:\n    \"\"\"\n    Flag outlier values in the target column using a specified method.\n    \n    This function marks outliers in the DataFrame based on the specified statistical method (e.g., IQR).\n    The flagged outliers can be used for further inspection or processing.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        target_col (str): The column in which to flag outliers.\n        method (str): The method to detect outliers (e.g., \"IQR\"). Defaults to \"IQR\".\n        factor (float): The multiplier applied in the outlier detection method. Defaults to 1.5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with an additional boolean column (or modified target column) indicating outlier status.\n        \n    Remarks:\n        - The implementation should clearly document how outliers are flagged.\n    \"\"\"\n    pass\n\ndef flag_missing_rows(df: pd.DataFrame, threshold: float=0.2) -> pd.DataFrame:\n    \"\"\"\n    Flag rows with a high proportion of missing values.\n    \n    This function adds a boolean indicator to each row in the DataFrame informing whether the row has \n    missing data exceeding a specified threshold.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        threshold (float): The fraction of missing values in a row required to flag it as problematic. Defaults to 0.2.\n        \n    Returns:\n        pd.DataFrame: The original DataFrame augmented with an additional column indicating rows with excessive missing values.\n        \n    Constraints:\n        - Assumes the threshold is a value between 0 and 1.\n    \"\"\"\n    pass\n\ndef threshold_based_removal(df: pd.DataFrame, target_col: str, threshold: float) -> pd.DataFrame:\n    \"\"\"\n    Remove entries from the target column if their value exceeds a specified threshold.\n    \n    This function deletes rows or marks values for removal if they do not meet the threshold criterion.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        target_col (str): The column to check against the threshold.\n        threshold (float): The threshold value that determines if an entry should be removed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with rows removed or flagged where the target column exceeds the threshold.\n        \n    Edge Cases:\n        - Behavior when no rows meet the threshold should be clearly documented.\n    \"\"\"\n    pass\n\ndef categorical_imputation(df: pd.DataFrame, target_col: str, strategy: str='mode') -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in categorical columns using a specified strategy.\n    \n    This function imputes missing values for categorical data. By default, it uses the mode, but other strategies\n    can be specified if needed.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing categorical data.\n        target_col (str): The categorical column to impute.\n        strategy (str): The imputation strategy to use (e.g., \"mode\", \"constant\"). Defaults to \"mode\".\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the categorical column imputed.\n        \n    Assumptions:\n        - The target column contains categorical data.\n    \"\"\"\n    pass\n\ndef model_based_imputation(df: pd.DataFrame, target_col: str, model, features: list) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values using a predictive model.\n    \n    This function leverages a given model to predict missing values in the target column based on other related features.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The column where imputation is required.\n        model: A predictive model instance that implements fit/predict.\n        features (list): A list of column names to be used as predictors for the model.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with the target column imputed using model predictions.\n        \n    Assumptions:\n        - The provided model follows a standard interface (fit and predict methods).\n    \"\"\"\n    pass\n\ndef column_median_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using the median computed for that specific column.\n    \n    This function calculates the median value of the target column (ignoring missing values)\n    and fills missing entries with that median.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing data.\n        target_col (str): The name of the column to be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column filled using its median.\n        \n    Constraints:\n        - The target column is assumed to be numeric.\n    \"\"\"\n    pass\n\ndef delete_incomplete_records(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Delete records (rows) from the DataFrame that contain any missing values.\n    \n    This function removes all rows where any column has a missing entry.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with all rows containing missing values removed.\n        \n    Edge Cases:\n        - If the DataFrame is entirely missing values or empty, the result should be an empty DataFrame.\n    \"\"\"\n    pass\n\ndef drop_columns_based_on_percentage(df: pd.DataFrame, percentage: float) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from the DataFrame where the percentage of missing values exceeds a specified limit.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        percentage (float): The maximum allowed percentage (between 0 and 100) of missing values in a column.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with columns dropped if their missing data percentage exceeds the specified limit.\n        \n    Constraints:\n        - The value of percentage should be within the range 0 to 100.\n    \"\"\"\n    pass\n\ndef knn_imputation(df: pd.DataFrame, target_col: str, n_neighbors: int=5) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in a target column using the K-Nearest Neighbors algorithm.\n    \n    This function identifies the k-nearest rows with non-missing values in the target column and imputes\n    missing values by aggregating these neighbors (e.g., by taking their mean).\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        target_col (str): The column in which to impute missing values.\n        n_neighbors (int): The number of neighbors to consider for imputation. Defaults to 5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using KNN.\n        \n    Assumptions:\n        - The DataFrame is appropriately scaled so that the distance metric in KNN is meaningful.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/Weighted mean",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/conditional mean imputation",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/conditional median imputation",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/delete columns",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/delete incomplete records",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/drop based on percentage missing",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/flag outliers",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/flag rows with missing values",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/handle large datasets for mode imputation",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/identify missing",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/impute by group mean",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/impute by group median",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/impute categorical columns",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/impute missing binary data with mode",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/impute using local median",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/impute with column median",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/impute with conditional mode",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/impute with global mean",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/impute with global median",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/impute with mode",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/knn imputation",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/linear interpolation",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/mode imputation for numbers",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/mode of subset of dataset",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/model-based imputation",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/pairwise deletion",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/polynomial interpolation",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/simple imputation",
                                                        "Data Preparation/Imputation Techniques/Imputation Methods/threshold-based removal"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "__init__.py",
                                                    "path": "src/data_engineering/data_preparation/imputation/__init__.py",
                                                    "code": "",
                                                    "feature_paths": [],
                                                    "units": []
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "feature_engineering",
                                            "path": "src/data_engineering/data_preparation/feature_engineering",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "combine_features.py",
                                                    "path": "src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                                                    "code": "from typing import List, Dict, Callable, Optional\nimport pandas as pd\nfrom typing import List, Union\nfrom typing import List\n\ndef combine_numerical_features(numerical_dfs: List[pd.DataFrame]) -> pd.DataFrame:\n    \"\"\"\n    Combine multiple numerical feature DataFrames into a single DataFrame containing numerical features.\n    \n    This function takes a list of DataFrames that each contain numerical feature columns and merges them\n    by aligning their indices. It is designed to consolidate numerical features into one DataFrame for\n    subsequent modeling steps.\n    \n    Args:\n        numerical_dfs (List[pd.DataFrame]): A list of DataFrames, each containing numerical feature columns.\n    \n    Returns:\n        pd.DataFrame: A DataFrame resulting from the combination of the numerical feature DataFrames.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes all DataFrames have compatible indices for merging.\n    \"\"\"\n    pass\n\ndef concatenate_features(feature_dfs: List[pd.DataFrame], axis: int=1) -> pd.DataFrame:\n    \"\"\"\n    Concatenate multiple feature DataFrames along a specified axis.\n    \n    This function concatenates a list of DataFrames, which contain features that need to be combined into\n    a single DataFrame. It supports concatenation along either rows or columns based on the provided axis.\n    \n    Args:\n        feature_dfs (List[pd.DataFrame]): A list of DataFrames containing feature columns.\n        axis (int, optional): The axis along which to concatenate. Default is 1 (columns).\n    \n    Returns:\n        pd.DataFrame: A concatenated DataFrame containing all combined features.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes that the DataFrames are dimensionally compatible along the non-concatenation axis.\n    \"\"\"\n    pass\n\ndef combine_categorical_features(categorical_dfs: List[pd.DataFrame]) -> pd.DataFrame:\n    \"\"\"\n    Combine multiple categorical feature DataFrames into a single DataFrame.\n    \n    This function merges several DataFrames that contain categorical features. It ensures that the data\n    types remain consistent and that the categorical features are properly combined for further processing.\n    \n    Args:\n        categorical_dfs (List[pd.DataFrame]): A list of DataFrames, each with categorical feature columns.\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the combined categorical features.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes that the DataFrames have a common index or merging logic that allows them to be combined.\n    \"\"\"\n    pass\n\ndef aggregate_features(df: pd.DataFrame, group_by: Optional[List[str]], agg_functions: Dict[str, Callable]) -> pd.DataFrame:\n    \"\"\"\n    Aggregate features in a DataFrame based on specified grouping keys and aggregation functions.\n    \n    This function performs aggregation on a DataFrame by grouping the data using the provided key(s) and \n    then applying a set of aggregation functions. It is useful for summarizing feature values and deriving \n    aggregated representations of data for modeling.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing features to be aggregated.\n        group_by (Optional[List[str]]): Column names to group by; if None, aggregation is performed on the entire DataFrame.\n        agg_functions (Dict[str, Callable]): A dictionary mapping column names to aggregation functions (e.g., sum, mean).\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the aggregated features.\n    \n    Edge Cases:\n        - If group_by is None, aggregation is applied across the entire DataFrame.\n        - An empty DataFrame is returned if the input DataFrame is empty.\n    \"\"\"\n    pass\n\ndef merge_feature_sets(feature_dfs: List[pd.DataFrame], on: Union[str, List[str]]) -> pd.DataFrame:\n    \"\"\"\n    Merge multiple feature DataFrames based on a common key or set of keys.\n    \n    This function executes a merge (join) operation on a list of DataFrames containing features using the specified \n    key(s). It is aimed at integrating feature sets from different sources into a single cohesive DataFrame.\n    \n    Args:\n        feature_dfs (List[pd.DataFrame]): A list of DataFrames with features to be merged.\n        on (Union[str, List[str]]): A column name or list of column names to be used as join keys.\n    \n    Returns:\n        pd.DataFrame: A DataFrame resulting from the merge of the input feature sets.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes all DataFrames in the list contain the specified join key(s).\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Feature Engineering/Combine Features/aggregate features",
                                                        "Data Preparation/Feature Engineering/Combine Features/combine categorical features",
                                                        "Data Preparation/Feature Engineering/Combine Features/combine numerical features",
                                                        "Data Preparation/Feature Engineering/Combine Features/concatenate features",
                                                        "Data Preparation/Feature Engineering/Combine Features/merge feature sets"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "create_features.py",
                                                    "path": "src/data_engineering/data_preparation/feature_engineering/create_features.py",
                                                    "code": "import pandas as pd\nfrom typing import List\n\ndef derive_new_metrics(feature_data: pd.DataFrame, metric_params: dict) -> pd.DataFrame:\n    \"\"\"\n    Derive new metric features from the provided data based on specified parameters.\n\n    This function computes new metrics using the input feature_data and a dictionary of parameters\n    defining the metric calculations. This might include aggregations or transformations to derive\n    summary or performance measures not originally present in the data.\n\n    Args:\n        feature_data (pd.DataFrame): The input DataFrame containing features.\n        metric_params (dict): A dictionary specifying how to compute new metrics. Keys could be the\n                              new metric names and values could be functions or specifications of the\n                              operations to perform.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the new derived metric features added.\n\n    Edge Cases:\n        - If metric_params is empty, the function may return the original DataFrame.\n        - Assumes feature_data is a valid DataFrame.\n    \"\"\"\n    pass\n\ndef create_time_based_features(data: pd.DataFrame, time_column: str, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Create time-based features from a specified time column in the DataFrame.\n\n    This function extracts various temporal features (such as hour, day, month, weekday, etc.)\n    from the provided time_column and appends them as new features in the output DataFrame.\n    Additional keyword arguments may define specific extraction rules or configurations.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the time column.\n        time_column (str): The name of the column in the DataFrame that holds datetime information.\n        **kwargs: Additional parameters such as timezone adjustments or custom extraction options.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the new time-based features appended.\n\n    Edge Cases:\n        - If time_column does not exist in data, the function should handle it appropriately.\n        - Assumes the time_column is in a parseable datetime format.\n    \"\"\"\n    pass\n\ndef generate_ratios(feature_data: pd.DataFrame, ratio_definitions: dict) -> pd.DataFrame:\n    \"\"\"\n    Generate ratio features based on specified numerator-denominator pairs.\n\n    This function calculates new features by computing ratios for selected columns in the\n    feature_data. The ratio_definitions dictionary maps new feature names to a tuple of two strings,\n    each representing the column names for the numerator and denominator.\n\n    Args:\n        feature_data (pd.DataFrame): The input DataFrame containing the features.\n        ratio_definitions (dict): A dictionary where each key is the name of the new ratio feature and\n                                  the value is a tuple (numerator_column, denominator_column).\n\n    Returns:\n        pd.DataFrame: A DataFrame with the newly generated ratio features added.\n\n    Edge Cases:\n        - If any denominator is zero or missing, the function should handle division errors.\n        - If ratio_definitions is empty, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef create_location_based_features(data: pd.DataFrame, location_column: str, additional_params: dict=None) -> pd.DataFrame:\n    \"\"\"\n    Create location-based features from a specified location column in the DataFrame.\n\n    This function extracts and/or transforms the location data (e.g., geographical coordinates, \n    addresses, regions) found in the location_column and generates new features such as distance measures, \n    region flags, or spatial clusters. Additional parameters can guide specific feature extraction methods.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing location data.\n        location_column (str): The name of the column that holds location information.\n        additional_params (dict, optional): A dictionary with extra options to guide the feature \n                                            extraction process. Defaults to None.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the newly created location-based features appended.\n\n    Edge Cases:\n        - If location_column is missing or contains invalid entries, the function\n          should handle such cases gracefully.\n    \"\"\"\n    pass\n\ndef create_date_features(data: pd.DataFrame, date_column: str, date_format: str='%Y-%m-%d') -> pd.DataFrame:\n    \"\"\"\n    Create new features by extracting elements from a date field in the DataFrame.\n\n    This function converts the given date_column into various derived features such as year, month, day,\n    weekday, and other date-related components based on the specified date_format. These new features can \n    be utilized in downstream analyses.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing the date information.\n        date_column (str): The name of the column containing date strings or datetime objects.\n        date_format (str, optional): The format in which dates are structured if they are in string form.\n                                     Defaults to '%Y-%m-%d'.\n\n    Returns:\n        pd.DataFrame: A DataFrame with newly created features derived from the date column.\n\n    Edge Cases:\n        - If date_column does not exist or dates are not parsable using the given format, the function\n          should report an error or skip feature creation.\n    \"\"\"\n    pass\n\ndef create_external_data_features(main_data: pd.DataFrame, external_data: pd.DataFrame, join_keys: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Create new features by integrating external data with the main DataFrame.\n\n    This function merges the main_data with external_data based on the specified join_keys to augment the \n    feature set with external information. This can be used to incorporate additional context or metadata \n    into the main dataset.\n\n    Args:\n        main_data (pd.DataFrame): The primary DataFrame containing the original features.\n        external_data (pd.DataFrame): The external DataFrame that contains supplementary features.\n        join_keys (List[str]): A list of column names used to join the two DataFrames.\n\n    Returns:\n        pd.DataFrame: A DataFrame resulting from the merge process, enriched with external features.\n\n    Edge Cases:\n        - If the join_keys are not present in both DataFrames, the function should handle the error.\n    \"\"\"\n    pass\n\ndef create_existing_data_features(data: pd.DataFrame, feature_instructions: dict) -> pd.DataFrame:\n    \"\"\"\n    Create new features by transforming or combining existing features within the DataFrame.\n\n    This function applies a set of specified transformations (outlined in feature_instructions) to the \n    DataFrame in order to generate new, derived features from the existing ones. These instructions \n    could include arithmetic operations, concatenations, or other custom transformations.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the original features.\n        feature_instructions (dict): A dictionary defining how to transform or combine existing features.\n                                     The keys might refer to new feature names, and the values indicate the\n                                     operations or source columns involved.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the newly created features from existing data added.\n\n    Edge Cases:\n        - If feature_instructions is empty, the function could return the original DataFrame.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Feature Engineering/Create New Features/create features from date",
                                                        "Data Preparation/Feature Engineering/Create New Features/create features from existing data",
                                                        "Data Preparation/Feature Engineering/Create New Features/create features from external data",
                                                        "Data Preparation/Feature Engineering/Create New Features/derive new metrics",
                                                        "Data Preparation/Feature Engineering/Create New Features/generate ratios",
                                                        "Data Preparation/Feature Engineering/Create New Features/location-based features",
                                                        "Data Preparation/Feature Engineering/Create New Features/time-based features"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "feature_extraction.py",
                                                    "path": "src/data_engineering/data_preparation/feature_engineering/feature_extraction.py",
                                                    "code": "from typing import Optional\nimport numpy as np\nfrom typing import List, Optional\nimport pandas as pd\n\ndef extract_categorical_features(data: pd.DataFrame, categorical_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Extract categorical features from the input DataFrame.\n\n    This function isolates categorical features from the provided DataFrame for use in downstream tasks such as encoding\n    or modeling. It either extracts user-specified columns or automatically identifies categorical types if none are provided.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the data from which to extract categorical features.\n        categorical_columns (Optional[List[str]]): A list of column names to extract as categorical features. \n            If None, the function will attempt to infer categorical columns automatically.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the extracted categorical features.\n\n    Raises:\n        ValueError: If the input DataFrame is empty or if the specified 'categorical_columns' are not found in the DataFrame.\n    \"\"\"\n    pass\n\ndef extract_numerical_features(data: pd.DataFrame, numerical_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Extract numerical features from the input DataFrame.\n\n    This function identifies and extracts numerical features from a DataFrame, which may then be utilized for scaling,\n    statistical analysis, or machine learning model training. It supports both automatic detection and explicit column specification.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing a mix of feature types.\n        numerical_columns (Optional[List[str]]): A list of column names representing numerical features to extract.\n            If None, the function will automatically detect numerical columns.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the extracted numerical features.\n\n    Raises:\n        ValueError: If the input DataFrame is empty or if no numerical columns are found based on the given criteria.\n    \"\"\"\n    pass\n\ndef extract_text_features(data: pd.DataFrame, text_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Extract features from text data in the input DataFrame.\n\n    This function processes the DataFrame to extract textual features, applying common text processing techniques such as \n    tokenization or embedding extraction. Users may supply a list of text_columns, or the function can attempt to infer text data.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing textual data.\n        text_columns (Optional[List[str]]): A list of column names that contain text data. \n            If None, the function will attempt to automatically identify text-containing columns.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing features extracted from the text data.\n\n    Raises:\n        ValueError: If the input DataFrame is empty, or if specified text columns are not present in the DataFrame.\n    \"\"\"\n    pass\n\ndef extract_image_features(image_array: np.ndarray, config: Optional[dict]=None) -> np.ndarray:\n    \"\"\"\n    Extract features from image data represented as a numpy array.\n\n    This function processes an image (represented as a numpy array) to extract meaningful features such as descriptors or embeddings.\n    An optional configuration dictionary allows for customization of the feature extraction process.\n\n    Args:\n        image_array (np.ndarray): The input image data in numpy array format.\n        config (Optional[dict]): A dictionary containing configuration parameters to customize the feature extraction.\n            If None, default extraction parameters are used.\n\n    Returns:\n        np.ndarray: A numpy array containing the extracted image features.\n\n    Raises:\n        ValueError: If the input image array is empty or does not conform to expected dimensions, or if the configuration \n                    is missing required keys for processing.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Feature Engineering/Feature Extraction/extract categorical features",
                                                        "Data Preparation/Feature Engineering/Feature Extraction/extract features from text",
                                                        "Data Preparation/Feature Engineering/Feature Extraction/extract numerical features",
                                                        "Data Preparation/Feature Engineering/Feature Extraction/image feature extraction"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "feature_scaling.py",
                                                    "path": "src/data_engineering/data_preparation/feature_engineering/feature_scaling.py",
                                                    "code": "from typing import Optional, List\nfrom typing import Optional, List, Union\nimport pandas as pd\n\ndef scale_features_to_range(df: pd.DataFrame, feature_columns: Optional[List[str]]=None, min_value: Union[int, float]=0, max_value: Union[int, float]=1) -> pd.DataFrame:\n    \"\"\"\n    Scale features in a DataFrame to a specified range using min-max scaling.\n\n    This function implements the min-max scaling algorithm which rescales the data\n    in the specified feature columns to fit within the provided [min_value, max_value] range.\n    If 'feature_columns' is not provided, all numeric columns in the DataFrame will be scaled.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the features to scale.\n        feature_columns (Optional[List[str]]): A list of column names to be scaled.\n            If None, all numeric columns are considered.\n        min_value (Union[int, float]): The minimum value of the desired scale range. Default is 0.\n        max_value (Union[int, float]): The maximum value of the desired scale range. Default is 1.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the specified features scaled to the given range.\n\n    Edge Cases and Assumptions:\n        - The function assumes that 'df' contains numeric data for the selected columns.\n        - If feature_columns is provided but some columns are non-numeric, the behavior is undefined.\n        - The function does not modify the input DataFrame in-place; it returns a new DataFrame.\n    \"\"\"\n    pass\n\ndef standard_scale_features(df: pd.DataFrame, feature_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Standardize features in a DataFrame using standard scaling (z-score normalization).\n\n    This function applies standard scaling to the selected feature columns by subtracting\n    the mean and dividing by the standard deviation for each feature. If 'feature_columns' is\n    not provided, all numeric columns will be standardized.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the features to standardize.\n        feature_columns (Optional[List[str]]): A list of column names to be standardized.\n            If None, standardization will be applied to all numeric columns.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the specified features standardized.\n\n    Edge Cases and Assumptions:\n        - The function assumes that 'df' contains numeric data for the selected columns.\n        - Columns with zero variance may result in division by zero; these cases are not explicitly handled.\n        - The input DataFrame remains unchanged; a new DataFrame with standardized values is returned.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Feature Engineering/Feature Scaling/min-max scaling",
                                                        "Data Preparation/Feature Engineering/Feature Scaling/scale features to range",
                                                        "Data Preparation/Feature Engineering/Feature Scaling/standard scaling"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "interaction_terms.py",
                                                    "path": "src/data_engineering/data_preparation/feature_engineering/interaction_terms.py",
                                                    "code": "from typing import List, Optional, Union\nimport pandas as pd\n\nclass InteractionTermsGenerator:\n    \"\"\"\n    InteractionTermsGenerator is a utility class that provides methods to generate interaction features\n    from a given DataFrame. It supports creating categorical interaction terms, polynomial terms,\n    multiplicative terms, and ratio terms. These transformations are commonly used in feature engineering \n    to capture non-linear relationships and interactions between features.\n\n    Methods:\n        create_categorical_interaction_terms(df, columns) -> pd.DataFrame:\n            Generates interaction terms between specified categorical columns.\n        \n        create_polynomial_terms(df, degree, include_bias) -> pd.DataFrame:\n            Generates polynomial features of a given degree based on numerical columns.\n            \n        create_multiplicative_terms(df, columns) -> pd.DataFrame:\n            Generates new features by computing the product of values across the given columns.\n            \n        create_ratio_terms(df, numerator_columns, denominator_columns, epsilon) -> pd.DataFrame:\n            Generates new features by computing the ratio between specified numerator and denominator columns.\n    \"\"\"\n\n    def create_categorical_interaction_terms(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n        \"\"\"\n        Generate categorical interaction terms by combining specified categorical columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing the categorical features.\n            columns (List[str]): List of column names to combine for creating interaction terms.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with new columns representing the categorical interaction terms.\n        \n        Edge Cases:\n            - Returns the original DataFrame if the list of columns is empty.\n            - Assumes that the provided columns exist in the DataFrame.\n        \"\"\"\n        pass\n\n    def create_polynomial_terms(self, df: pd.DataFrame, degree: int=2, include_bias: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Generate polynomial terms for numerical features up to the specified degree.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame with numerical features.\n            degree (int, optional): Degree of the polynomial features to be generated. Defaults to 2.\n            include_bias (bool, optional): If True, include a bias (intercept) column. Defaults to False.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with additional polynomial features.\n        \n        Edge Cases:\n            - Assumes that the DataFrame contains only numerical columns for transformation.\n            - If degree is less than 2, no additional interaction is created.\n        \"\"\"\n        pass\n\n    def create_multiplicative_terms(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n        \"\"\"\n        Create multiplicative interaction terms by multiplying the values of specified columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing features.\n            columns (List[str]): List of columns whose element-wise products are to be computed.\n            \n        Returns:\n            pd.DataFrame: A DataFrame enriched with multiplicative interaction features.\n        \n        Edge Cases:\n            - If fewer than two columns are provided, the function returns the DataFrame unmodified.\n            - Assumes numerical data for valid multiplication.\n        \"\"\"\n        pass\n\n    def create_ratio_terms(self, df: pd.DataFrame, numerator_columns: List[str], denominator_columns: List[str], epsilon: float=1e-08) -> pd.DataFrame:\n        \"\"\"\n        Create ratio terms by computing the element-wise ratios of the specified numerator and denominator columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical data.\n            numerator_columns (List[str]): List of column names to be used as numerators.\n            denominator_columns (List[str]): List of column names to be used as denominators.\n            epsilon (float, optional): A small number added to denominators to avoid division by zero. Defaults to 1e-8.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with new columns representing the ratio terms.\n        \n        Edge Cases:\n            - Assumes that for each ratio, corresponding numerator and denominator columns exist.\n            - If denominator values are zero, the epsilon value prevents division errors.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Feature Engineering/Interaction Terms/categorical interaction terms",
                                                        "Data Preparation/Feature Engineering/Interaction Terms/create multiplicative terms",
                                                        "Data Preparation/Feature Engineering/Interaction Terms/create polynomial terms",
                                                        "Data Preparation/Feature Engineering/Interaction Terms/create ratio terms"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "polynomial_features.py",
                                                    "path": "src/data_engineering/data_preparation/feature_engineering/polynomial_features.py",
                                                    "code": "from typing import Optional\nimport pandas as pd\n\nclass PolynomialFeaturesGenerator:\n    \"\"\"\n    A generator for creating polynomial features from numerical data.\n\n    This class provides methods to generate higher-order polynomial terms\n    from a given DataFrame. It includes a generic method to create polynomial terms \n    up to any specified degree, a method specifically to create third-order terms,\n    and a method to generate second-order (degree 2) polynomial features.\n\n    Methods:\n        generate_higher_order_terms(df: pd.DataFrame, degree: int) -> pd.DataFrame:\n            Generates polynomial features up to the specified degree.\n        create_third_order_terms(df: pd.DataFrame) -> pd.DataFrame:\n            Generates polynomial features of exactly third order.\n        create_degree2_polynomial_features(df: pd.DataFrame) -> pd.DataFrame:\n            Generates polynomial features of degree 2.\n    \n    Note:\n        - It is assumed that the input DataFrame contains only numerical data.\n        - Preprocessing to handle non-numerical data should be done beforehand.\n    \"\"\"\n\n    def generate_higher_order_terms(self, df: pd.DataFrame, degree: int) -> pd.DataFrame:\n        \"\"\"\n        Generate polynomial features up to the specified degree.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n            degree (int): Maximum power for the polynomial expansion (>= 2).\n\n        Returns:\n            pd.DataFrame: DataFrame with the generated polynomial features appended.\n        \"\"\"\n        pass\n\n    def create_third_order_terms(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Create polynomial features of exactly third order.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n\n        Returns:\n            pd.DataFrame: DataFrame with third-order polynomial features added.\n        \"\"\"\n        pass\n\n    def create_degree2_polynomial_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate polynomial features of degree 2.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n\n        Returns:\n            pd.DataFrame: DataFrame with second-order (degree 2) polynomial features appended.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Feature Engineering/Polynomial Features/create polynomial features of degree 2",
                                                        "Data Preparation/Feature Engineering/Polynomial Features/create third-order terms",
                                                        "Data Preparation/Feature Engineering/Polynomial Features/generate higher-order terms"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "feature_selection.py",
                                                    "path": "src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                    "code": "from typing import List, Any\nfrom typing import List, Tuple, Any\nimport pandas as pd\n\nclass FilterFeatureSelector:\n    \"\"\"\n    A collection of filter-based feature selection methods.\n\n    This class implements multiple filtering techniques for feature selection such as:\n      - Select with chi-squared tests.\n      - Select with mutual information scores.\n      - Select features with high correlation to the target.\n      - Filter features based on low variance.\n      - Rank features by their computed importance.\n\n    Each method is designed to work with input pandas DataFrames and aligns with the\n    data flow expectations of the system.\n    \"\"\"\n\n    def select_with_chi_squared(self, X: pd.DataFrame, y: Any, k: int) -> List[str]:\n        \"\"\"\n        Select the top 'k' features based on chi-squared test scores.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix with non-negative features.\n            y (Any): The target variable.\n            k (int): The number of top features to select.\n\n        Returns:\n            List[str]: A list of selected feature names based on chi-squared results.\n\n        Raises:\n            ValueError: If X contains negative values or if k is out of valid range.\n        \"\"\"\n        pass\n\n    def select_with_mutual_information(self, X: pd.DataFrame, y: Any, k: int) -> List[str]:\n        \"\"\"\n        Select the top 'k' features based on mutual information scores.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            y (Any): The target variable.\n            k (int): The number of top features to select.\n\n        Returns:\n            List[str]: A list of feature names selected based on mutual information.\n\n        Raises:\n            ValueError: If the mutual information calculation fails.\n        \"\"\"\n        pass\n\n    def select_high_correlation(self, X: pd.DataFrame, y: Any, threshold: float) -> List[str]:\n        \"\"\"\n        Select features that have a correlation with the target variable above a given threshold.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix with named columns.\n            y (Any): The target variable used for correlation computation.\n            threshold (float): The minimum correlation coefficient to consider. \n                               Should be between 0 and 1.\n\n        Returns:\n            List[str]: A list of feature names that exhibit high correlation with the target.\n\n        Raises:\n            ValueError: If threshold is not between 0 and 1.\n        \"\"\"\n        pass\n\n    def select_low_variance(self, X: pd.DataFrame, threshold: float) -> List[str]:\n        \"\"\"\n        Filter out features with variance lower than the specified threshold.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            threshold (float): The minimum variance required for a feature to be retained.\n\n        Returns:\n            List[str]: A list of feature names with variance exceeding the threshold.\n\n        Raises:\n            ValueError: If the threshold is negative.\n        \"\"\"\n        pass\n\n    def rank_features_by_importance(self, X: pd.DataFrame, y: Any) -> List[Tuple[str, float]]:\n        \"\"\"\n        Rank features based on their importance scores derived from a model-based approach.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            y (Any): The target variable.\n            \n        Returns:\n            List[Tuple[str, float]]: A list of tuples where each tuple contains a feature name and \n                                     its corresponding importance score, sorted in descending order.\n\n        Raises:\n            Exception: If feature importance cannot be computed.\n        \"\"\"\n        pass\n\ndef recursive_feature_elimination(X: pd.DataFrame, y: Any, estimator: Any, min_features_to_select: int=1) -> List[str]:\n    \"\"\"\n    Perform recursive feature elimination to select the most important features.\n\n    This function iteratively removes features from the input dataset based on the performance\n    of the provided estimator until the specified minimum number of features is reached.\n    It helps in reducing model complexity and enhancing model generalization.\n\n    Args:\n        X (pd.DataFrame): The input feature matrix with named columns.\n        y (Any): The target variable.\n        estimator (Any): A machine learning estimator that provides feature importance or coefficient attributes.\n        min_features_to_select (int, optional): The minimum number of features to retain. Defaults to 1.\n\n    Returns:\n        List[str]: A list of selected feature names.\n\n    Edge Cases:\n        - The estimator must support feature importance or provide coefficients.\n        - X is expected to have unique column names.\n    \"\"\"\n    pass\n\ndef wrapper_feature_selection(X: pd.DataFrame, y: Any, estimator: Any, cv: int=5) -> List[str]:\n    \"\"\"\n    Perform feature selection using a wrapper method based on model performance.\n\n    This function evaluates different subsets of features using cross-validation with the provided\n    estimator and selects the combination that optimizes the performance metric.\n    \n    Args:\n        X (pd.DataFrame): The input feature matrix.\n        y (Any): The target variable.\n        estimator (Any): A machine learning estimator used to evaluate feature subsets.\n        cv (int, optional): The number of cross-validation folds to use. Defaults to 5.\n\n    Returns:\n        List[str]: A list of feature names selected by the wrapper method.\n\n    Edge Cases:\n        - The estimator must be compatible with cross-validation.\n        - Computational cost may be high for large feature sets.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/feature importance ranking",
                                                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/filter methods",
                                                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/recursive feature elimination",
                                                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/select features with high correlation",
                                                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/select features with low variance",
                                                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/select with chi-squared",
                                                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/select with mutual information",
                                                        "Data Preparation/Feature Engineering/Feature Selection & Reduction/wrapper methods"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "__init__.py",
                                                    "path": "src/data_engineering/data_preparation/feature_engineering/__init__.py",
                                                    "code": "",
                                                    "feature_paths": [],
                                                    "units": []
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "aggregation",
                                            "path": "src/data_engineering/data_preparation/aggregation",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "bucketization.py",
                                                    "path": "src/data_engineering/data_preparation/aggregation/bucketization.py",
                                                    "code": "from typing import List\nimport pandas as pd\n\ndef bucketize_by_quantiles(df: pd.DataFrame, quantiles: List[float]) -> pd.DataFrame:\n    \"\"\"\n    Bucketize the DataFrame by quantiles.\n    \n    This function divides the data in a specified DataFrame column into buckets based on the provided quantile values.\n    It calculates the boundaries corresponding to the quantiles and assigns each row to a bucket derived from these boundaries.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n        quantiles (List[float]): A list of quantile values (ranging from 0 to 1) that determine the bucket boundaries.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column indicating the quantile bucket for each row.\n    \n    Edge Cases:\n        - An empty DataFrame will return an empty DataFrame.\n        - An empty quantiles list will result in no bucketing.\n        - The function assumes that the appropriate data column will be managed externally.\n    \"\"\"\n    pass\n\ndef dynamic_bucketization(df: pd.DataFrame, strategy: str, parameters: dict) -> pd.DataFrame:\n    \"\"\"\n    Dynamically bucketize the DataFrame based on a specified strategy.\n    \n    This function automatically determines bucket boundaries according to the distribution of the data.\n    The 'strategy' argument specifies the dynamic bucketing method to use (e.g., equal frequency or clustering-based),\n    while the 'parameters' dictionary provides any additional configuration needed for that strategy.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n        strategy (str): A string representing the dynamic bucketing strategy to apply.\n        parameters (dict): A dictionary of strategy-specific parameters for bucketization.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column that denotes the dynamic bucket assignment for each row.\n    \n    Edge Cases:\n        - An empty DataFrame will result in an empty DataFrame.\n        - If an unrecognized strategy is provided, the function may default to a no-op bucketing process.\n    \"\"\"\n    pass\n\ndef bucketize_by_range(df: pd.DataFrame, range_start: float, range_end: float, bucket_size: float) -> pd.DataFrame:\n    \"\"\"\n    Bucketize the DataFrame by a defined numerical range.\n    \n    This function segments the data into buckets based on a continuous range defined by a start value, an end value,\n    and a fixed bucket size. It creates equal intervals within the specified range and assigns each data row to its corresponding bucket.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the values to be bucketized.\n        range_start (float): The beginning value of the range.\n        range_end (float): The ending value of the range.\n        bucket_size (float): The fixed width of each bucket within the range.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column indicating the bucket assignment for each row.\n    \n    Edge Cases:\n        - If the range length is not an exact multiple of the bucket_size, the final bucket may be smaller.\n        - An empty DataFrame will return an empty DataFrame.\n        - Values outside the specified range may require special handling or may be excluded.\n    \"\"\"\n    pass\n\ndef bucketize_by_custom_bins(df: pd.DataFrame, bins: List[float]) -> pd.DataFrame:\n    \"\"\"\n    Bucketize the DataFrame using custom bin edges.\n    \n    This function applies user-defined bin edges to classify the data into discrete buckets.\n    It uses the provided list of bin boundaries to determine the bucket for each row in the DataFrame.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n        bins (List[float]): A list of floating-point numbers defining the custom bin edges.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column that indicates the bucket assignment based on the custom bins.\n    \n    Edge Cases:\n        - An empty list of bins will result in no bucketization.\n        - Non-monotonic or overlapping bin values may lead to undefined bucket assignments.\n        - An empty DataFrame will simply return an empty DataFrame.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Data Aggregation & Profiling/Bucketization/bucketize by custom bins",
                                                        "Data Preparation/Data Aggregation & Profiling/Bucketization/bucketize by quantiles",
                                                        "Data Preparation/Data Aggregation & Profiling/Bucketization/bucketize by range",
                                                        "Data Preparation/Data Aggregation & Profiling/Bucketization/dynamic bucketization"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "aggregation.py",
                                                    "path": "src/data_engineering/data_preparation/aggregation/aggregation.py",
                                                    "code": "from typing import List\nfrom typing import List, Optional\nimport pandas as pd\n\ndef aggregate_mean(df: pd.DataFrame, columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Compute the mean value for specified columns in the given DataFrame.\n\n    This function calculates the arithmetic mean of the specified columns.\n    If no columns are provided, it computes the mean for all numerical columns.\n    It is designed to assist in data aggregation profiling by summarizing data trends.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing data to aggregate.\n        columns (List[str], optional): List of column names for which the mean should be computed.\n            If None, mean is computed on all numerical columns.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the mean values for the specified columns.\n\n    Edge Cases:\n        - If the DataFrame is empty, the function will return an empty DataFrame.\n        - If columns are specified but some are not present in the DataFrame, the behavior is undefined.\n    \"\"\"\n    pass\n\ndef aggregate_median(df: pd.DataFrame, columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Compute the median value for specified columns in the given DataFrame.\n\n    This function calculates the median of the provided columns.\n    If columns are not explicitly provided, it calculates the median for every numerical column in the DataFrame.\n    This aggregated insight is useful for understanding the central tendency of data distributions.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing data for aggregation.\n        columns (List[str], optional): List of column names for which the median should be computed.\n            Defaults to None, meaning all numerical columns will be considered.\n\n    Returns:\n        pd.DataFrame: A DataFrame with median values for each specified column.\n\n    Edge Cases:\n        - An empty DataFrame will return an empty DataFrame.\n        - Missing columns specified in 'columns' may lead to unexpected behavior.\n    \"\"\"\n    pass\n\ndef group_and_count(df: pd.DataFrame, group_by: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Group the DataFrame by the specified columns and count the occurrences in each group.\n\n    The function performs a group by operation and returns the count of rows for each group.\n    This can be useful for data profiling purposes to identify the distribution of categorical\n    or discrete feature occurrences within the dataset.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame to be grouped.\n        group_by (List[str]): A list of columns to group the DataFrame by.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the count of rows for each group defined by the group_by columns.\n\n    Edge Cases:\n        - If no rows exist in the DataFrame, an empty DataFrame is returned.\n        - If group_by columns contain null values, the count might include these as a separate group.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Data Aggregation & Profiling/Aggregation/group and count",
                                                        "Data Preparation/Data Aggregation & Profiling/Aggregation/mean",
                                                        "Data Preparation/Data Aggregation & Profiling/Aggregation/median"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "zero_column.py",
                                                    "path": "src/data_engineering/data_preparation/aggregation/zero_column.py",
                                                    "code": "import pandas as pd\nfrom typing import Optional\n\ndef drop_columns_with_mostly_zeros(df: pd.DataFrame, threshold: float=0.8) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from the DataFrame that contain mostly zeros.\n    \n    This function examines each column in the input DataFrame and drops those columns where\n    the proportion of zero values exceeds the specified threshold, indicating that the column\n    is mostly populated with zeros. This is useful as a preprocessing step for cleaning data,\n    particularly in scenarios where such columns may not contribute meaningful information.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be processed.\n        threshold (float, optional): The proportion of zero values in a column above which\n            the column will be dropped. The value should be in the range (0, 1]. Default is 0.8,\n            meaning columns with more than 80% zeros will be removed.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with columns that have mostly zeros removed.\n\n    Edge Cases:\n        - If the DataFrame is empty, the function returns an empty DataFrame.\n        - If no column meets the dropping criteria, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef drop_columns_with_zero_variance(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from the DataFrame that have zero variance.\n    \n    This function identifies columns in the input DataFrame where all values are identical\n    (i.e., the variance is zero) and removes them. Such columns typically do not provide\n    useful discriminatory information in further data analysis or modeling, and hence,\n    their removal can help in reducing dimensionality.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be processed.\n    \n    Returns:\n        pd.DataFrame: A modified DataFrame with columns of zero variance removed.\n\n    Edge Cases:\n        - If the DataFrame is empty, the function returns an empty DataFrame.\n        - If no columns have zero variance, the original DataFrame is returned unchanged.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Data Aggregation & Profiling/Zero Column Handling/drop columns with mostly zeros",
                                                        "Data Preparation/Data Aggregation & Profiling/Zero Column Handling/drop columns with zero variance"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "profiling.py",
                                                    "path": "src/data_engineering/data_preparation/aggregation/profiling.py",
                                                    "code": "from typing import Dict, Any\nimport pandas as pd\n\ndef analyze_correlation(df: pd.DataFrame, method: str='pearson') -> Dict[str, Any]:\n    \"\"\"\n    Perform a correlation analysis on the provided DataFrame.\n\n    This function computes the correlation matrix for the numerical features in the DataFrame using the specified\n    correlation method (e.g., 'pearson', 'spearman', or 'kendall'). It returns a dictionary containing the correlation\n    matrix and additional insights such as pairs of features with high correlations. This analysis helps in data profiling\n    by identifying relationships between features.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing numerical features to analyze.\n        method (str, optional): The correlation method to use; valid values include 'pearson', 'spearman', and 'kendall'.\n                                Defaults to 'pearson'.\n\n    Returns:\n        Dict[str, Any]: A dictionary with the following keys:\n            - 'correlation_matrix': A DataFrame representing the computed correlation matrix.\n            - 'highly_correlated_pairs': A list of tuples indicating feature pairs with high correlation.\n            - 'summary': A summary of the correlation analysis providing further insights.\n\n    Edge Cases:\n        - If the DataFrame is empty or lacks sufficient numerical data, the function should return an empty dictionary.\n        - The function assumes that inputs have appropriate numeric data types for correlation calculation.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [
                                                        "Data Preparation/Data Aggregation & Profiling/Profiling/correlation analysis"
                                                    ],
                                                    "units": []
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "__init__.py",
                                                    "path": "src/data_engineering/data_preparation/aggregation/__init__.py",
                                                    "code": "",
                                                    "feature_paths": [],
                                                    "units": []
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/data_engineering/data_preparation/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "data_encoding",
                                    "path": "src/data_engineering/data_encoding",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "label_encoding.py",
                                            "path": "src/data_engineering/data_encoding/label_encoding.py",
                                            "code": "import pandas as pd\n\ndef frequency_based_label_encoding(df: pd.DataFrame, column: str) -> pd.DataFrame:\n    \"\"\"\n    Perform frequency-based label encoding on a specified column of a DataFrame.\n    \n    This function converts categorical labels in the provided column into numerical labels based\n    on the frequency of each unique category. Categories that appear more frequently are assigned lower\n    numerical values. This type of encoding is useful when the frequency order carries predictive significance.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the target column.\n        column (str): The name of the column containing categorical labels to encode.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with the specified column transformed into frequency-based numerical labels.\n    \n    Notes:\n        - The function assumes that the specified column contains categorical data.\n        - Handling of missing values or unseen categories must be considered in the implementation.\n    \"\"\"\n    pass\n\ndef ordinal_label_encoding(df: pd.DataFrame, column: str, categories: list) -> pd.DataFrame:\n    \"\"\"\n    Apply ordinal label encoding to a specified column of a DataFrame.\n    \n    This function maps categorical labels in the given column to integer values that reflect a user-specified\n    order. The order is provided by the 'categories' list, where the position of a category in the list\n    determines its ordinal value. This encoding is appropriate when there is an inherent ranking among the categories.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the target column.\n        column (str): The name of the column containing categorical labels to encode.\n        categories (list): A list of categories defining the desired order for ordinal encoding.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with the specified column transformed using ordinal label encoding.\n    \n    Notes:\n        - The function assumes that the provided 'categories' list covers all possible labels in the column.\n        - Users must ensure that the order specified in the 'categories' list accurately reflects the intended ranking.\n        - Behavior for handling missing or unexpected labels should be defined as needed.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [
                                                "Data Encoding/Label Encoding/Details/frequency-based label encoding",
                                                "Data Encoding/Label Encoding/Details/ordinal label encoding"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "onehot_encoding.py",
                                            "path": "src/data_engineering/data_encoding/onehot_encoding.py",
                                            "code": "import pandas as pd\nfrom scipy import sparse\nfrom typing import List, Optional, Union\n\nclass CombinedOneHotEncoder:\n    \"\"\"\n    A combined one-hot encoder that supports both dense one-hot encoding and sparse matrix optimization.\n\n    This class provides an interface for encoding categorical features into a one-hot representation.\n    Users can choose to produce a dense output (a pandas DataFrame) or an optimized sparse matrix,\n    depending on the requirements for memory and computational efficiency.\n\n    Attributes:\n        sparse (bool): Determines the output format. If True, the transform method returns a sparse \n                       matrix optimized for performance. If False, it returns a dense pandas DataFrame.\n    \n    Methods:\n        fit(X: pd.DataFrame, columns: Optional[List[str]] = None) -> CombinedOneHotEncoder:\n            Fit the encoder on input data to learn the unique categories from specified columns.\n        \n        transform(X: pd.DataFrame) -> Union[pd.DataFrame, sparse.csr_matrix]:\n            Transform the input DataFrame into one-hot encoded format, producing either a dense DataFrame\n            or a sparse matrix based on the encoder configuration.\n        \n        fit_transform(X: pd.DataFrame, columns: Optional[List[str]] = None) -> Union[pd.DataFrame, sparse.csr_matrix]:\n            Fit the encoder and transform the data in one step.\n    \n    Args:\n        sparse (bool): A flag to determine whether to use sparse matrix optimization (True) or dense \n                       representation (False). Default is False.\n    \n    Returns:\n        An instance of CombinedOneHotEncoder.\n\n    Edge Cases and Constraints:\n        - The input DataFrame should contain the categorical columns to be encoded.\n        - If columns is None, the encoder may attempt to infer the categorical columns from the data.\n        - The transform method must handle cases where unknown categories are encountered gracefully.\n        - This interface does not implement the actual encoding logic; it provides the necessary structure.\n    \"\"\"\n\n    def __init__(self, sparse: bool=False):\n        self.sparse = sparse\n        pass\n\n    def fit(self, X: pd.DataFrame, columns: Optional[List[str]]=None) -> 'CombinedOneHotEncoder':\n        \"\"\"\n        Fit the encoder to the input DataFrame by learning the unique categories from specified columns.\n\n        Args:\n            X (pd.DataFrame): The input data containing categorical features.\n            columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, the encoder\n                                           may attempt to infer categorical columns automatically.\n\n        Returns:\n            CombinedOneHotEncoder: The fitted encoder instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> Union[pd.DataFrame, sparse.csr_matrix]:\n        \"\"\"\n        Transform the input DataFrame into a one-hot encoded format.\n\n        Based on the encoder's configuration, the output will either be a dense pandas DataFrame or \n        an optimized sparse matrix.\n\n        Args:\n            X (pd.DataFrame): The data to transform.\n\n        Returns:\n            Union[pd.DataFrame, sparse.csr_matrix]: The one-hot encoded representation of the input data.\n        \"\"\"\n        pass\n\n    def fit_transform(self, X: pd.DataFrame, columns: Optional[List[str]]=None) -> Union[pd.DataFrame, sparse.csr_matrix]:\n        \"\"\"\n        Fit the encoder to the data and then transform it into one-hot encoded format.\n\n        Args:\n            X (pd.DataFrame): The input data to fit and transform.\n            columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, it may infer columns.\n\n        Returns:\n            Union[pd.DataFrame, sparse.csr_matrix]: The transformed one-hot encoded data.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Data Encoding/One-hot Encoding/Combined One-hot/dense one-hot encoding",
                                                "Data Encoding/One-hot Encoding/Combined One-hot/sparse matrix optimization"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "binary_encoding.py",
                                            "path": "src/data_engineering/data_encoding/binary_encoding.py",
                                            "code": "from typing import Optional\nimport pandas as pd\n\ndef compress_binary_encoding(df: pd.DataFrame, column: str) -> pd.DataFrame:\n    \"\"\"\n    Apply compressed binary encoding to the specified column of a DataFrame.\n\n    This function transforms the binary encoding of a given column in the DataFrame\n    by applying compression techniques. It is intended to reduce storage overhead while\n    preserving the binary nature of the column's data. The function assumes the input column \n    contains data that can be represented in a binary format and performs a compression\n    operation that is tailored to binary encoded data.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the binary-encoded column.\n        column (str): The name of the column in the DataFrame to be compressed.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the specified column processed through compressed\n                      binary encoding. All other columns remain unchanged.\n\n    Edge Cases:\n        - If the specified column does not exist, the function should handle the situation appropriately.\n        - The function assumes that the column's data is already in a binary encodable format.\n        - No in-place modification is performed; a new DataFrame is returned.\n\n    Note:\n        This is a stub interface. Implementation logic is to be provided later.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [
                                                "Data Encoding/Binary Encoding/Details/compressed binary encoding"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/data_engineering/data_encoding/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "data_splitting",
                                    "path": "src/data_engineering/data_splitting",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "time_based.py",
                                            "path": "src/data_engineering/data_splitting/time_based.py",
                                            "code": "import pandas as pd\n\nclass TimeBasedSplitter:\n    \"\"\"\n    A splitter for performing time-based data splits according to different temporal granularities.\n\n    This class encapsulates multiple strategies to split a pandas DataFrame based on time attributes.\n    It provides methods to split the data by time zones, month, hour, day, and year.\n    \n    Each method expects a time-indexed pandas DataFrame and a specific column name holding the relevant time information.\n    The returned structure is a dictionary where the keys represent the extracted time units and the values are the corresponding\n    segments of the DataFrame.\n\n    Methods:\n        split_by_timezones(df: pd.DataFrame, timezone_column: str) -> dict:\n            Splits data based on time zone values.\n        split_by_month(df: pd.DataFrame, month_column: str) -> dict:\n            Splits data based on the month extracted from a datetime column.\n        split_by_hour(df: pd.DataFrame, hour_column: str) -> dict:\n            Splits data based on the hour extracted from a datetime column.\n        split_by_day(df: pd.DataFrame, day_column: str) -> dict:\n            Splits data based on the day extracted from a datetime column.\n        split_by_year(df: pd.DataFrame, year_column: str) -> dict:\n            Splits data based on the year extracted from a datetime column.\n    \n    Note:\n        - No splitting logic is implemented in this interface definition.\n        - The client is responsible for verifying that the DataFrame contains the required columns with appropriate data formats.\n    \"\"\"\n\n    def split_by_timezones(self, df: pd.DataFrame, timezone_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame based on time zone information provided in the specified column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            timezone_column (str): The name of the column with time zone data.\n        \n        Returns:\n            dict: A dictionary where keys are time zone identifiers and values are the corresponding DataFrame segments.\n        \"\"\"\n        pass\n\n    def split_by_month(self, df: pd.DataFrame, month_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the month extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            month_column (str): The name of the column from which the month information will be derived.\n        \n        Returns:\n            dict: A dictionary with months as keys and DataFrame segments as values.\n        \"\"\"\n        pass\n\n    def split_by_hour(self, df: pd.DataFrame, hour_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the hour extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            hour_column (str): The name of the column from which the hour information will be derived.\n        \n        Returns:\n            dict: A dictionary with hours as keys and corresponding DataFrame segments as values.\n        \"\"\"\n        pass\n\n    def split_by_day(self, df: pd.DataFrame, day_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the day extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            day_column (str): The name of the column from which day information will be derived.\n        \n        Returns:\n            dict: A dictionary with days as keys and corresponding DataFrame segments as values.\n        \"\"\"\n        pass\n\n    def split_by_year(self, df: pd.DataFrame, year_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the year extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            year_column (str): The name of the column from which year information will be derived.\n        \n        Returns:\n            dict: A dictionary with years as keys and corresponding DataFrame segments as values.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Data Splitting/Temporal Split/Time-based Split/split by day",
                                                "Data Splitting/Temporal Split/Time-based Split/split by hour",
                                                "Data Splitting/Temporal Split/Time-based Split/split by month",
                                                "Data Splitting/Temporal Split/Time-based Split/split by time zones",
                                                "Data Splitting/Temporal Split/Time-based Split/split by year"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "event_based.py",
                                            "path": "src/data_engineering/data_splitting/event_based.py",
                                            "code": "import pandas as pd\nfrom typing import Dict\n\nclass EventBasedSplitter:\n    \"\"\"\n    A class that provides various methods for splitting a DataFrame based on event-related criteria.\n    \n    This class encapsulates three event-based splitting strategies:\n    1. Splitting the dataset based on event count.\n    2. Splitting the dataset based on event frequency.\n    3. Splitting the dataset based on event types.\n    \n    Each method takes a DataFrame and relevant parameters to determine how the data is segmented.\n    \n    Methods:\n        split_by_event_count: Splits the DataFrame based on the count of events.\n        split_by_event_frequency: Splits the DataFrame based on the frequency of events.\n        split_by_event_types: Splits the DataFrame based on event type categorization.\n    \"\"\"\n\n    def split_by_event_count(self, df: pd.DataFrame, event_count_column: str, count_threshold: int) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Splits the DataFrame based on the count of events recorded in a specific column.\n        \n        This method divides the input DataFrame into multiple segments based on a threshold value applied \n        to the event count column. Different segments may reflect varying event participation quantities.\n        \n        Args:\n            df (pd.DataFrame): The input data containing events.\n            event_count_column (str): The column name that holds the event count.\n            count_threshold (int): The threshold value used to determine splitting logic.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary where keys represent split segment identifiers and values \n            are the corresponding DataFrame segments.\n        \"\"\"\n        pass\n\n    def split_by_event_frequency(self, df: pd.DataFrame, frequency_column: str, frequency_threshold: float) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Splits the DataFrame based on the frequency of events observed in a given column.\n        \n        The method segments the DataFrame into multiple groups based on whether the event frequency \n        exceeds or falls below a given threshold. This is useful for categorizing events based on how \n        often they occur.\n        \n        Args:\n            df (pd.DataFrame): The input DataFrame containing event frequency information.\n            frequency_column (str): The column name that contains the event frequency data.\n            frequency_threshold (float): The threshold frequency to determine how the dataset is split.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary mapping segment names to their corresponding DataFrame splits.\n        \"\"\"\n        pass\n\n    def split_by_event_types(self, df: pd.DataFrame, event_type_column: str) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Splits the DataFrame based on distinct event types as specified in a given column.\n        \n        This method categorizes the input data into multiple segments, each corresponding to a unique\n        event type found in the specified column. It facilitates analysis per event category.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing event data.\n            event_type_column (str): The column name that classifies the types of events.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary where each key is an event type and the value is the \n            subset of the DataFrame corresponding to that event type.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Data Splitting/Temporal Split/Event-based Split/split by event count",
                                                "Data Splitting/Temporal Split/Event-based Split/split by event frequency",
                                                "Data Splitting/Temporal Split/Event-based Split/split by event types"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "cv_temporal.py",
                                            "path": "src/data_engineering/data_splitting/cv_temporal.py",
                                            "code": "from typing import List, Tuple\nimport pandas as pd\n\nclass CVTemporalSplitter:\n    \"\"\"\n    Provides cross-validation splitting strategies for temporal data with various approaches.\n    \n    This class implements three methods to generate cross-validation splits that respect the temporal \n    structure of the data. It supports:\n      - Time-series cross-validation: Splitting based on chronological order.\n      - Cross-validation with event windows: Creating splits based on event-based windows.\n      - Cross-validation with time windows: Dividing data into splits based on predefined time intervals.\n    \n    Each method returns a list of tuples, where each tuple contains two lists:\n      - The first list contains indices for the training set.\n      - The second list contains indices for the testing set.\n      \n    Methods:\n        time_series_cv: Generates splits for time-series cross-validation.\n        event_windows_cv: Generates splits based on event windows.\n        time_windows_cv: Generates splits based on time windows.\n    \"\"\"\n\n    def time_series_cv(self, X: pd.DataFrame, n_splits: int=5) -> List[Tuple[List[int], List[int]]]:\n        \"\"\"\n        Perform time-series cross-validation by splitting data sequentially.\n        \n        This method assumes that the input DataFrame X is ordered by time. The function generates\n        n_splits pairs of training and testing indices, where each successive split uses later portions \n        of the data for testing.\n        \n        Args:\n            X (pd.DataFrame): The input data ordered chronologically.\n            n_splits (int): The number of splits to generate. Default is 5.\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices.\n        \"\"\"\n        pass\n\n    def event_windows_cv(self, X: pd.DataFrame, event_column: str, window_size: int) -> List[Tuple[List[int], List[int]]]:\n        \"\"\"\n        Perform cross-validation using event windows.\n        \n        This method splits the DataFrame based on event occurrences specified in the event_column.\n        Each window is determined by a fixed number of events (window_size), allowing the cross-validation\n        process to account for clustered or bursty events and their implications on data distribution.\n        \n        Args:\n            X (pd.DataFrame): The input data that includes an event indicator column.\n            event_column (str): The name of the column that specifies events.\n            window_size (int): The number of events to include in each window.\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing indices for training and testing sets.\n        \"\"\"\n        pass\n\n    def time_windows_cv(self, X: pd.DataFrame, time_column: str, window_duration) -> List[Tuple[List[int], List[int]]]:\n        \"\"\"\n        Perform cross-validation using predefined time windows.\n        \n        This method splits the DataFrame based on a time column. Each split is created by segmenting the data \n        into intervals determined by the window_duration parameter. This allows for evaluation across \n        consistent time intervals in the data.\n        \n        Args:\n            X (pd.DataFrame): The input data containing a time-based column.\n            time_column (str): The name of the column representing time.\n            window_duration: The duration of each time window (e.g., an integer representing days or a pandas offset).\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Data Splitting/Temporal Split/Cross-validation Temporal Split/cross-validation with event windows",
                                                "Data Splitting/Temporal Split/Cross-validation Temporal Split/cross-validation with time windows",
                                                "Data Splitting/Temporal Split/Cross-validation Temporal Split/time-series cross-validation"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "train_test_temporal.py",
                                            "path": "src/data_engineering/data_splitting/train_test_temporal.py",
                                            "code": "from typing import Dict, Any\nimport pandas as pd\n\nclass TrainTestTemporalSplitter:\n    \"\"\"\n    Provides train-test splitting strategies based on temporal criteria.\n\n    This interface allows splitting a dataset into training and testing portions based on three distinct\n    temporal features:\n      1. split by test period: Splitting based on a designated test period (e.g., a specific date or time span).\n      2. split by time intervals: Splitting by uniform or specified time intervals.\n      3. split by event intervals: Splitting based on event-triggered intervals in the data.\n\n    Methods in this class should accept the input data (typically in a pandas DataFrame) and splitting parameters,\n    and return a dictionary that contains at least two keys (commonly 'train' and 'test') pointing to the respective\n    DataFrame splits.\n\n    Assumptions:\n      - The input DataFrame contains a temporal index or column that can be leveraged for splitting.\n      - The split parameters are pre-validated and provided in a format understandable by each method.\n      - This interface is stateless; any stateful configuration should be managed externally.\n\n    Usage:\n      splitter = TrainTestTemporalSplitter()\n      splits = splitter.split_by_test_period(df, test_period=\"2021-01-01\")\n    \"\"\"\n\n    def split_by_test_period(self, df: pd.DataFrame, test_period: Any) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Split the input DataFrame into training and testing sets based on a specified test period.\n        \n        This method uses a user-defined test period (which could be a specific date or a time range) to\n        determine the boundary between training and testing data.\n\n        Args:\n            df (pd.DataFrame): The input dataset containing temporal data.\n            test_period (Any): The criterion defining the test period. This might be a date string, a tuple of dates,\n                               or any object representing the test period boundary.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n            the split DataFrames.\n\n        Edge Cases:\n            - If the test period does not match any records, the function should return an empty test set.\n            - It is assumed that the DataFrame's temporal column is pre-processed and valid.\n        \"\"\"\n        pass\n\n    def split_by_time_intervals(self, df: pd.DataFrame, interval_params: Dict[str, Any]) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Split the input DataFrame into training and testing sets based on specified time intervals.\n        \n        This method creates splits by dividing the dataset into segments defined by uniform or custom time intervals.\n        \n        Args:\n            df (pd.DataFrame): The input dataset containing time-related data.\n            interval_params (Dict[str, Any]): Parameters defining the time interval splits. For example, it may include\n                                              the interval duration, the start time, and the end time.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n            the resulting DataFrame splits based on time intervals.\n        \n        Constraints:\n            - It is assumed that the interval parameters are provided correctly.\n            - The DataFrame must contain a time-index or a dedicated time column.\n        \"\"\"\n        pass\n\n    def split_by_event_intervals(self, df: pd.DataFrame, event_intervals: Dict[str, Any]) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Split the input DataFrame into training and testing sets based on event-driven intervals.\n        \n        This method leverages events annotated in the DataFrame (or computed externally) to decide the split points,\n        where the events signify transitions between training and testing periods.\n        \n        Args:\n            df (pd.DataFrame): The input dataset that includes event-related data markers.\n            event_intervals (Dict[str, Any]): A dictionary containing parameters that define the event intervals. These\n                                              parameters might dictate how many events mark the split or conditions based\n                                              on event types.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' containing the respective splits determined \n            by event intervals.\n        \n        Edge Cases:\n            - The method should correctly handle cases where events are sparse or overly frequent.\n            - It assumes that the DataFrame includes valid and pre-processed event markers.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Data Splitting/Temporal Split/Train-Test Temporal Split/split by event intervals",
                                                "Data Splitting/Temporal Split/Train-Test Temporal Split/split by test period",
                                                "Data Splitting/Temporal Split/Train-Test Temporal Split/split by time intervals"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/data_engineering/data_splitting/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "data_validation",
                                    "path": "src/data_engineering/data_validation",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "cross_validation.py",
                                            "path": "src/data_engineering/data_validation/cross_validation.py",
                                            "code": "import pandas as pd\nimport numpy as np\nfrom typing import List, Tuple, Optional\n\ndef stratified_cross_validation(X: pd.DataFrame, y: pd.Series, n_splits: int=5, random_state: Optional[int]=None) -> List[Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Perform stratified cross-validation splitting on the given dataset.\n    \n    This function splits the dataset into training and testing folds ensuring that each fold maintains approximately \n    the same percentage of samples from each target class. It is particularly useful in scenarios involving imbalanced\n    classification where preserving class distribution during evaluation is crucial.\n    \n    Args:\n        X (pd.DataFrame): The input features dataset.\n        y (pd.Series): The target labels corresponding to the data in X.\n        n_splits (int): The number of folds to split the data into. Must be at least 2. Default is 5.\n        random_state (Optional[int]): A seed for the random number generator to ensure reproducibility. Default is None.\n    \n    Returns:\n        List[Tuple[np.ndarray, np.ndarray]]: A list where each element is a tuple containing two numpy arrays:\n            - The first array represents the indices for the training set.\n            - The second array represents the indices for the testing set.\n    \n    Edge Cases and Constraints:\n        - n_splits must be at least 2, otherwise valid splits cannot be produced.\n        - If the distribution of classes in y is such that any class does not have enough samples to support the specified n_splits, \n          the function's behavior is undefined and should either raise an error or be handled by pre-validation.\n        - It is assumed that the order of samples in X and y is aligned.\n    \n    Assumptions:\n        - Input X and y are non-empty and correctly aligned.\n        - The function does not perform the actual splitting logic; it defines the interface to be implemented.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [
                                                "Data Validation/Cross-validation/Details/stratified cross-validation"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/data_engineering/data_validation/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "file",
                                    "name": "__init__.py",
                                    "path": "src/data_engineering/__init__.py",
                                    "code": "",
                                    "feature_paths": [],
                                    "units": []
                                }
                            ]
                        },
                        {
                            "type": "directory",
                            "name": "advanced_modeling",
                            "path": "src/advanced_modeling",
                            "children": [
                                {
                                    "type": "directory",
                                    "name": "model_evaluation",
                                    "path": "src/advanced_modeling/model_evaluation",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "roc_analysis.py",
                                            "path": "src/advanced_modeling/model_evaluation/roc_analysis.py",
                                            "code": "import numpy as np\n\nclass ROCCurveVisualizer:\n    \"\"\"\n    Provides functionalities for visualizing ROC curves along with threshold selection and AUC computation.\n\n    This class encapsulates methods to plot ROC curves, select optimal thresholds based on metrics,\n    and compute the Area Under the Curve (AUC) from the ROC data. This combined interface supports\n    interactive and analytical evaluation of classifier performance.\n\n    Methods:\n        plot_roc_curve:\n            Generate a plot of the ROC curve given arrays of false positive and true positive rates.\n        select_threshold:\n            Determine the optimal threshold value using a metric (e.g., Youden's index).\n        compute_auc:\n            Calculate the Area Under the ROC Curve (AUC) from input ROC data.\n\n    Usage:\n        Instantiate the class and invoke its methods with appropriate numpy arrays to perform visualization\n        and analysis of ROC performance.\n    \"\"\"\n\n    def plot_roc_curve(self, fpr: np.ndarray, tpr: np.ndarray) -> None:\n        \"\"\"\n        Plot the ROC curve using false positive rates and true positive rates.\n\n        Args:\n            fpr (np.ndarray): Array of false positive rates.\n            tpr (np.ndarray): Array of true positive rates.\n\n        Returns:\n            None\n\n        Edge Cases:\n            - It is assumed that fpr and tpr are of equal length. Inconsistent array lengths must be handled externally.\n        \"\"\"\n        pass\n\n    def select_threshold(self, thresholds: np.ndarray, metric_values: np.ndarray) -> float:\n        \"\"\"\n        Select the optimal threshold based on a given metric (e.g., maximizing Youden's index).\n\n        Args:\n            thresholds (np.ndarray): Array of threshold values corresponding to the ROC points.\n            metric_values (np.ndarray): Array of computed metric values at each threshold.\n\n        Returns:\n            float: The selected optimal threshold value.\n\n        Notes:\n            - In cases where multiple thresholds achieve the same optimal metric value, the first occurrence is returned.\n        \"\"\"\n        pass\n\n    def compute_auc(self, fpr: np.ndarray, tpr: np.ndarray) -> float:\n        \"\"\"\n        Compute the Area Under the Curve (AUC) for the provided ROC data.\n\n        Args:\n            fpr (np.ndarray): Array of false positive rates.\n            tpr (np.ndarray): Array of true positive rates.\n\n        Returns:\n            float: The computed AUC value.\n\n        Edge Cases:\n            - The method assumes that the input arrays are non-empty and of equal length.\n        \"\"\"\n        pass\n\ndef compute_multiclass_roc(y_true: np.ndarray, y_score: np.ndarray, classes: list) -> dict:\n    \"\"\"\n    Compute ROC metrics for multi-class classification.\n\n    This function calculates the ROC curve for each class in a multi-class classification setting.\n    It returns false positive rates, true positive rates, and corresponding thresholds for each class.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix indicating true classes.\n        y_score (np.ndarray): Array of probability estimates or decision scores from the classifier.\n        classes (list): List of class identifiers corresponding to columns in y_score.\n\n    Returns:\n        dict: A dictionary where each key is a class identifier and each value is another dictionary\n              containing 'fpr' (np.ndarray), 'tpr' (np.ndarray), and 'thresholds' (np.ndarray).\n\n    Notes:\n        - It is assumed that y_true and y_score have matching dimensions and ordering.\n        - The function does not handle any plotting or visualization.\n    \"\"\"\n    pass\n\ndef compute_micro_average_roc(y_true: np.ndarray, y_score: np.ndarray) -> dict:\n    \"\"\"\n    Compute the micro-average ROC curve for multi-label or multi-class classification.\n\n    This function aggregates the performance across all classes to generate a single ROC curve,\n    which is particularly useful when dealing with imbalanced or multi-label datasets.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix.\n        y_score (np.ndarray): Array of probability estimates or decision scores for each class.\n\n    Returns:\n        dict: A dictionary containing 'fpr' (np.ndarray), 'tpr' (np.ndarray), and 'thresholds' (np.ndarray)\n              representing the micro-average ROC curve metrics.\n\n    Edge Cases:\n        - If the arrays are mismatched in dimensions, it is assumed that validation occurs externally.\n    \"\"\"\n    pass\n\ndef compute_precision_recall_curve(y_true: np.ndarray, y_score: np.ndarray, average: str='binary') -> dict:\n    \"\"\"\n    Compute the precision-recall curve for model evaluation.\n\n    This function calculates precision and recall at various threshold settings, aiding in the analysis\n    of the trade-off between precision and recall in classification tasks.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix for multi-class problems.\n        y_score (np.ndarray): Array of predicted probabilities or decision scores.\n        average (str, optional): Specifies the averaging method ('binary', 'macro', 'micro'). Defaults to 'binary'.\n\n    Returns:\n        dict: A dictionary containing arrays for 'precision', 'recall', and 'thresholds' (each as np.ndarray).\n\n    Edge Cases:\n        - The function assumes that y_true and y_score are properly formatted and that the 'average'\n          parameter is appropriately set for the task.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [
                                                "Model Evaluation/Comparative Evaluation/ROC Analysis/micro-average roc",
                                                "Model Evaluation/Comparative Evaluation/ROC Analysis/multi-class roc",
                                                "Model Evaluation/Comparative Evaluation/ROC Analysis/precision-recall curve",
                                                "Model Evaluation/Comparative Evaluation/ROC Analysis/roc curve plotting",
                                                "Model Evaluation/Comparative Evaluation/ROC Analysis/roc curve with auc",
                                                "Model Evaluation/Comparative Evaluation/ROC Analysis/roc curve with threshold selection"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "precision_recall.py",
                                            "path": "src/advanced_modeling/model_evaluation/precision_recall.py",
                                            "code": "from typing import Optional\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_precision_recall_curve(precisions: np.ndarray, recalls: np.ndarray, average_precision: Optional[float]=None, title: str='Precision-Recall Curve') -> None:\n    \"\"\"\n    Plot a precision-recall curve given the precision and recall values, and optionally display the average precision score.\n\n    This function takes as input the computed precision and recall values, and generates a plot of the\n    precision-recall curve. The plot can include an annotation for average precision if provided. It is used\n    as part of the model evaluation process to visually assess the trade-off between precision and recall\n    across different threshold levels.\n\n    Args:\n        precisions (np.ndarray): An array of precision values calculated for different threshold values.\n        recalls (np.ndarray): An array of recall values corresponding to the precision values.\n        average_precision (Optional[float]): An optional scalar representing the average precision score.\n        title (str): Title for the plot. Defaults to \"Precision-Recall Curve\".\n\n    Returns:\n        None: This function does not return any value; it simply generates a plot.\n\n    Edge Cases:\n        - The lengths of 'precisions' and 'recalls' must be equal; otherwise, the plot cannot be generated.\n        - If the arrays are empty, the function will not produce a meaningful plot.\n        - It is assumed that 'precisions' and 'recalls' are properly precomputed and correspond to sorted threshold values.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [
                                                "Model Evaluation/Comparative Evaluation/Precision-Recall Analysis/plot precision-recall curves"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "statistical_testing.py",
                                            "path": "src/advanced_modeling/model_evaluation/statistical_testing.py",
                                            "code": "from typing import Any\nfrom typing import Union\nimport matplotlib.pyplot as plt\nfrom typing import Dict, Tuple, Union\nfrom typing import Dict, Any\nimport numpy as np\nfrom typing import Tuple, Union\n\ndef two_tailed_t_test(sample1: Union[np.ndarray, list], sample2: Union[np.ndarray, list], alpha: float=0.05) -> float:\n    \"\"\"\n    Perform a two-tailed t-test to compare the means of two independent samples.\n\n    This function calculates the t-test statistic and corresponding p-value for two samples under a two-tailed hypothesis.\n    It is used to assess whether the means of two independent groups are statistically different.\n\n    Args:\n        sample1 (np.ndarray or list): The first set of observations.\n        sample2 (np.ndarray or list): The second set of observations.\n        alpha (float, optional): The significance level to determine statistical threshold; default is 0.05.\n\n    Returns:\n        float: The p-value indicating the probability of observing the data under the null hypothesis.\n    \n    Edge Cases:\n        - Samples with very small sizes may result in low statistical power.\n        - Non-normal data distributions may affect test validity.\n    \"\"\"\n    pass\n\ndef one_tailed_t_test(sample1: Union[np.ndarray, list], sample2: Union[np.ndarray, list], tail: str='right', alpha: float=0.05) -> float:\n    \"\"\"\n    Perform a one-tailed t-test to compare the means of two independent samples.\n\n    This function calculates the t-test statistic and p-value for a one-tailed hypothesis, where the alternative hypothesis \n    specifies a direction (either 'right' indicating sample1 > sample2 or 'left' indicating sample1 < sample2).\n\n    Args:\n        sample1 (np.ndarray or list): The first set of observations.\n        sample2 (np.ndarray or list): The second set of observations.\n        tail (str, optional): Indicates the test direction; valid options are \"right\" or \"left\". Default is \"right\".\n        alpha (float, optional): The significance level; default is 0.05.\n\n    Returns:\n        float: The p-value for the one-tailed test.\n\n    Raises:\n        ValueError: If the 'tail' argument is not \"right\" or \"left\".\n\n    Edge Cases:\n        - Small sample sizes or non-normal data can impact the test's reliability.\n        - The function does not handle equal variances issues explicitly.\n    \"\"\"\n    pass\n\ndef paired_t_test_with_confidence_interval(before: Union[np.ndarray, list], after: Union[np.ndarray, list], confidence: float=0.95) -> Tuple[float, Tuple[float, float]]:\n    \"\"\"\n    Perform a paired t-test and calculate the confidence interval for the mean difference between paired observations.\n\n    This function is used when comparing paired samples (e.g., pre-test and post-test measurements) and needs to\n    report both the p-value and the confidence interval of the difference.\n\n    Args:\n        before (np.ndarray or list): Measurements from the first condition (e.g., before treatment).\n        after (np.ndarray or list): Measurements from the second condition (e.g., after treatment).\n        confidence (float, optional): The confidence level for the confidence interval; default is 0.95.\n\n    Returns:\n        Tuple[float, Tuple[float, float]]:\n            - float: The p-value from the paired t-test.\n            - Tuple[float, float]: The lower and upper bounds of the confidence interval for the mean difference.\n\n    Edge Cases:\n        - Requires that both inputs have the same length.\n        - Results may be influenced by outliers.\n    \"\"\"\n    pass\n\ndef paired_t_test_multiple_metrics(metrics_data: Dict[str, Tuple[Union[np.ndarray, list], Union[np.ndarray, list]]], alpha: float=0.05) -> Dict[str, float]:\n    \"\"\"\n    Perform paired t-tests on multiple metrics simultaneously, returning p-values for each metric.\n\n    This function processes a dictionary of paired data for various metrics and computes the p-value \n    for the paired t-test for each metric. It is useful in multi-metric evaluation scenarios.\n\n    Args:\n        metrics_data (Dict[str, Tuple[np.ndarray or list, np.ndarray or list]]):\n            A dictionary where each key is a metric name and its value is a tuple containing two lists or arrays,\n            representing paired observations.\n        alpha (float, optional): The significance level for the tests; default is 0.05.\n\n    Returns:\n        Dict[str, float]: A mapping from each metric name to its corresponding p-value from the paired t-test.\n\n    Edge Cases:\n        - Each metric's paired data must have matching lengths.\n        - The function does not correct for multiple comparisons by default.\n    \"\"\"\n    pass\n\ndef interpret_test_results(test_statistic: float, p_value: float, alpha: float=0.05) -> str:\n    \"\"\"\n    Interpret the results of a statistical test by providing a textual explanation based on the test statistic and p-value.\n\n    This function helps in understanding the outcome of statistical tests by comparing the p-value to the significance level\n    and summarizing the result in plain language.\n\n    Args:\n        test_statistic (float): The computed test statistic.\n        p_value (float): The p-value obtained from the statistical test.\n        alpha (float, optional): The significance level used in the test; default is 0.05.\n\n    Returns:\n        str: A textual interpretation of the statistical test result detailing whether the null hypothesis is rejected.\n\n    Edge Cases:\n        - If the p-value is exactly equal to alpha, the interpretation will note the borderline significance.\n    \"\"\"\n    pass\n\ndef visualize_paired_t_test_results(test_results: Any, title: str='Paired T-Test Results') -> None:\n    \"\"\"\n    Visualize the results of a paired t-test, optionally including the confidence interval and significant differences.\n\n    This function generates a plot to help illustrate the outcome of a paired t-test including elements such as \n    mean differences, confidence intervals, and individual sample points if applicable.\n\n    Args:\n        test_results (Any): A data structure containing the paired t-test results. This can include p-values, \n                            confidence intervals, and mean differences.\n        title (str, optional): The title for the plot; default is \"Paired T-Test Results\".\n\n    Returns:\n        None: The function creates a plot but does not return any data.\n\n    Edge Cases:\n        - Expects the input test_results to be in a format compatible with the plotting logic.\n    \"\"\"\n    pass\n\ndef test_statistical_assumptions(data: np.ndarray, assumptions: Dict[str, Any]=None) -> Dict[str, Any]:\n    \"\"\"\n    Test the statistical assumptions underlying the use of t-tests (e.g., normality, variance homogeneity).\n\n    This function evaluates whether the input data meets key assumptions required for valid t-testing.\n    It returns a dictionary with the results of these assumption tests.\n\n    Args:\n        data (np.ndarray): The dataset to be tested for normality, homogeneity of variances, and other relevant assumptions.\n        assumptions (Dict[str, Any], optional): A configuration dictionary specifying which assumption tests to perform \n                                                  and their parameters; if None, default tests are applied.\n\n    Returns:\n        Dict[str, Any]: A mapping from assumption names to their test outcomes (e.g., p-values or pass/fail indicators).\n\n    Edge Cases:\n        - The function assumes that the data is one-dimensional or structured in a way suitable for assumption testing.\n        - When assumptions dictionary is provided, keys must match recognized assumption tests.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [
                                                "Model Evaluation/Comparative Evaluation/Statistical Testing/interpret results",
                                                "Model Evaluation/Comparative Evaluation/Statistical Testing/one-tailed test",
                                                "Model Evaluation/Comparative Evaluation/Statistical Testing/paired t-test with confidence interval",
                                                "Model Evaluation/Comparative Evaluation/Statistical Testing/paired t-test with multiple metrics",
                                                "Model Evaluation/Comparative Evaluation/Statistical Testing/test assumptions",
                                                "Model Evaluation/Comparative Evaluation/Statistical Testing/two-tailed t-test",
                                                "Model Evaluation/Comparative Evaluation/Statistical Testing/visualize paired t-test results"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/advanced_modeling/model_evaluation/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "optimization",
                                    "path": "src/advanced_modeling/optimization",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "genetic_algorithms.py",
                                            "path": "src/advanced_modeling/optimization/genetic_algorithms.py",
                                            "code": "from typing import List, Callable, Any, Optional\n\nclass GeneticAlgorithm:\n    \"\"\"\n    Implements a genetic algorithm incorporating elitism strategy,\n    customizable fitness function, and a crossover operation for generating new individuals.\n    \n    This class encapsulates the core operations of a genetic algorithm including:\n      - Preserving a fraction of the best individuals (elitism).\n      - Computing fitness using a custom fitness function.\n      - Generating offspring via a crossover operation.\n    \n    Attributes:\n        population (List[Any]): The current population of individuals.\n        fitness_function (Callable[[Any], float]): A callable that computes the fitness score of an individual.\n        elitism_rate (float): The fraction of top individuals to retain for the next generation.\n        mutation_rate (float): The probability of mutation for offspring.\n    \n    Methods:\n        run(generations: int) -> List[Any]:\n            Executes the genetic algorithm for a specified number of generations.\n            \n        crossover(parent1: Any, parent2: Any) -> Any:\n            Combines two parent individuals to create a new offspring.\n    \"\"\"\n\n    def __init__(self, population: List[Any], fitness_function: Callable[[Any], float], elitism_rate: float=0.1, mutation_rate: float=0.01) -> None:\n        \"\"\"\n        Initializes the genetic algorithm with the given population, fitness function, and parameters for elitism and mutation.\n        \n        Args:\n            population (List[Any]): Initial list of individuals representing the starting population.\n            fitness_function (Callable[[Any], float]): A custom function to evaluate the fitness of an individual.\n            elitism_rate (float, optional): Proportion of top individuals to be carried over unaltered to the next generation. Defaults to 0.1.\n            mutation_rate (float, optional): Probability of mutation during offspring generation. Defaults to 0.01.\n        \n        Assumptions:\n            - The population is non-empty and diverse enough for evolutionary processes.\n            - The fitness_function properly returns a numeric score indicating individual performance.\n        \"\"\"\n        self.population = population\n        self.fitness_function = fitness_function\n        self.elitism_rate = elitism_rate\n        self.mutation_rate = mutation_rate\n        pass\n\n    def run(self, generations: int) -> List[Any]:\n        \"\"\"\n        Executes the genetic algorithm over a specified number of generations.\n        \n        Incorporates elitism by retaining the top-performing individuals,\n        applies the custom fitness function to evaluate each generation,\n        and uses the crossover operation to create new offspring.\n        \n        Args:\n            generations (int): The number of generations (iterations) for which the algorithm should run.\n        \n        Returns:\n            List[Any]: The final population after completion of all generations.\n        \n        Edge Cases:\n            - If generations is less than or equal to zero, the function should either return the initial population or handle it gracefully.\n        \"\"\"\n        pass\n\n    def crossover(self, parent1: Any, parent2: Any) -> Any:\n        \"\"\"\n        Performs a crossover operation between two parent individuals to produce a new offspring.\n        \n        This function is responsible for combining genetic information from both parents,\n        potentially incorporating techniques such as one-point or two-point crossover.\n        \n        Args:\n            parent1 (Any): The first parent individual.\n            parent2 (Any): The second parent individual.\n        \n        Returns:\n            Any: A new offspring individual created by merging the attributes of the two parents.\n        \n        Assumptions:\n            - Both parents are compatible in structure such that their genetic material can be sensibly combined.\n            - The method does not enforce mutation; mutation is handled separately in the algorithm.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Optimization Methods/Evolutionary Optimization/Genetic Algorithms/crossover operation",
                                                "Optimization Methods/Evolutionary Optimization/Genetic Algorithms/genetic algorithm with custom fitness function",
                                                "Optimization Methods/Evolutionary Optimization/Genetic Algorithms/genetic algorithm with elitism"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "hyperparameter.py",
                                            "path": "src/advanced_modeling/optimization/hyperparameter.py",
                                            "code": "from typing import Any, Callable, Dict, List\n\nclass HyperbandOptimizer:\n    \"\"\"\n    Interface for hyperparameter optimization using the Hyperband tuning strategy.\n\n    HyperbandOptimizer implements the Hyperband algorithm to efficiently search a\n    large hyperparameter space. It dynamically allocates resources to promising\n    configurations while discarding poor performers early using successive halving.\n\n    Usage:\n        1. Instantiate the optimizer with the maximum iteration count, a downsampling factor,\n           and a user-defined evaluation function.\n        2. Call the 'tune' method with a list of hyperparameter configurations.\n        3. Retrieve the best found configuration according to the given evaluation function.\n\n    Args:\n        max_iter (int): The maximum resource allocation (e.g., maximum iterations or epochs)\n            for the tuning process.\n        eta (int): The reduction factor; determines the proportion of configurations discarded\n            in each round (typically, eta > 1).\n        evaluation_fn (Callable[[Dict[str, Any], int], float]): A callable function that evaluates\n            each hyperparameter configuration with a given allocated resource. It accepts a configuration\n            dictionary and an integer resource allocation, returning a performance metric (e.g., loss).\n\n    Methods:\n        tune(configurations: List[Dict[str, Any]]) -> Dict[str, Any]:\n            Executes the Hyperband algorithm on a list of hyperparameter configurations and returns\n            the configuration that achieved the best performance metric.\n    \"\"\"\n\n    def __init__(self, max_iter: int, eta: int, evaluation_fn: Callable[[Dict[str, Any], int], float]) -> None:\n        \"\"\"\n        Initialize the HyperbandOptimizer with tuning parameters.\n\n        Args:\n            max_iter (int): Maximum resource (iterations/epochs) available for hyperparameter tuning.\n            eta (int): Downsampling factor for successive halving; number of configurations to retain in each round.\n            evaluation_fn (Callable[[Dict[str, Any], int], float]): Function to evaluate a hyperparameter\n                configuration with a given resource allocation.\n        \"\"\"\n        pass\n\n    def tune(self, configurations: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Run the Hyperband algorithm to select the best hyperparameter configuration.\n\n        This method evaluates the provided hyperparameter configurations using the Hyperband\n        strategy. It allocates resources dynamically and prunes configurations based on their\n        performance, ultimately returning the best configuration.\n\n        Args:\n            configurations (List[Dict[str, Any]]): A list of hyperparameter configurations where\n                each configuration is represented as a dictionary mapping parameter names to values.\n\n        Returns:\n            Dict[str, Any]: The best hyperparameter configuration determined by the evaluation function.\n\n        Raises:\n            ValueError: If the provided configurations list is empty.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Optimization Methods/Hyperparameter Optimization/Tuning Strategies/hyperband"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "meta_learning.py",
                                            "path": "src/advanced_modeling/optimization/meta_learning.py",
                                            "code": "from typing import Any, List\n\nclass MetaLearningModel:\n    \"\"\"\n    A meta-learning model that integrates multiple base learners using a meta-learner.\n    \n    This class implements a meta-learning approach, in which predictions from multiple base \n    models are combined by a meta-learner to improve overall performance. It encapsulates the \n    functionality for training and prediction in ensemble strategies, specifically within the \n    meta-learning paradigm.\n    \n    Attributes:\n        base_learners (List[Any]): A list of trained base learning models.\n        meta_learner (Any): A model that aggregates the predictions from the base learners.\n    \"\"\"\n\n    def __init__(self, base_learners: List[Any], meta_learner: Any) -> None:\n        \"\"\"\n        Initialize the meta-learning model with the given base learners and meta-learner.\n        \n        Args:\n            base_learners (List[Any]): A list of pre-configured base model instances.\n            meta_learner (Any): A meta-model responsible for combining base learners' outputs.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def fit(self, X: Any, y: Any, **kwargs) -> 'MetaLearningModel':\n        \"\"\"\n        Train the meta-learning model on the provided dataset.\n        \n        This method fits both the base learners and the meta-learner, based on the input \n        data and targets, so that the meta-learner can learn to integrate predictions effectively.\n        \n        Args:\n            X (Any): The feature set used for training (e.g., pd.DataFrame or np.ndarray).\n            y (Any): The target variable used for training (e.g., pd.Series or np.ndarray).\n            **kwargs: Additional keyword arguments for configuring the training process.\n        \n        Returns:\n            MetaLearningModel: The fitted meta-learning model instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: Any) -> Any:\n        \"\"\"\n        Generate predictions using the trained meta-learning model.\n        \n        This method aggregates predictions from base learners and applies the meta-learner to produce\n        a final output.\n        \n        Args:\n            X (Any): Input data for which predictions are to be generated (e.g., pd.DataFrame or np.ndarray).\n        \n        Returns:\n            Any: The aggregated predictions from the meta-learning model.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [
                                                "Optimization Methods/Ensemble Strategies/Meta-Learning/meta-learning"
                                            ],
                                            "units": []
                                        },
                                        {
                                            "type": "file",
                                            "name": "__init__.py",
                                            "path": "src/advanced_modeling/optimization/__init__.py",
                                            "code": "",
                                            "feature_paths": [],
                                            "units": []
                                        }
                                    ]
                                },
                                {
                                    "type": "file",
                                    "name": "__init__.py",
                                    "path": "src/advanced_modeling/__init__.py",
                                    "code": "",
                                    "feature_paths": [],
                                    "units": []
                                }
                            ]
                        },
                        {
                            "type": "file",
                            "name": "__init__.py",
                            "path": "src/__init__.py",
                            "code": "",
                            "feature_paths": [],
                            "units": []
                        }
                    ]
                },
                {
                    "type": "directory",
                    "name": "docs",
                    "path": "docs",
                    "children": []
                },
                {
                    "type": "directory",
                    "name": "tests",
                    "path": "tests",
                    "children": [
                        {
                            "type": "file",
                            "name": "__init__.py",
                            "path": "tests/__init__.py",
                            "code": "",
                            "feature_paths": [],
                            "units": []
                        }
                    ]
                },
                {
                    "type": "directory",
                    "name": "utils",
                    "path": "utils",
                    "children": [
                        {
                            "type": "file",
                            "name": "__init__.py",
                            "path": "utils/__init__.py",
                            "code": "",
                            "feature_paths": [],
                            "units": []
                        }
                    ]
                },
                {
                    "type": "directory",
                    "name": "configs",
                    "path": "configs",
                    "children": []
                },
                {
                    "type": "directory",
                    "name": "scripts",
                    "path": "scripts",
                    "children": []
                },
                {
                    "type": "file",
                    "name": "README.md",
                    "path": "README.md",
                    "code": "",
                    "feature_paths": [],
                    "units": []
                },
                {
                    "type": "file",
                    "name": "requirements.txt",
                    "path": "requirements.txt",
                    "code": "",
                    "feature_paths": [],
                    "units": []
                },
                {
                    "type": "file",
                    "name": "setup.py",
                    "path": "setup.py",
                    "code": "",
                    "feature_paths": [],
                    "units": []
                },
                {
                    "type": "file",
                    "name": "main.py",
                    "path": "main.py",
                    "code": "",
                    "feature_paths": [],
                    "units": []
                },
                {
                    "type": "directory",
                    "name": "src",
                    "path": "./src",
                    "children": [
                        {
                            "type": "directory",
                            "name": "general",
                            "path": "./src/general",
                            "children": [
                                {
                                    "type": "file",
                                    "name": "base.py",
                                    "path": "./src/general/base.py",
                                    "code": "from abc import ABC, abstractmethod\n\nclass BaseEstimator(ABC):\n    \"\"\"\n    BaseEstimator is the abstract base class for all machine learning algorithms.\n    It defines the core interface for estimators used in supervised, unsupervised,\n    and other learning methods. Subclasses are expected to implement training and\n    prediction logic.\n    \"\"\"\n    \n    @abstractmethod\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the model using the training data.\n\n        Args:\n            X (DataFrame or array-like): Training data.\n            y (Series or array-like, optional): Target values for supervised learning.\n\n        Returns:\n            self: The fitted estimator instance.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def predict(self, X):\n        \"\"\"\n        Make predictions using the fitted model.\n\n        Args:\n            X (DataFrame or array-like): Data for which predictions are made.\n\n        Returns:\n            Array-like: Predicted values.\n        \"\"\"\n        pass\n\n    def score(self, X, y):\n        \"\"\"\n        Evaluate the performance of the model.\n\n        Args:\n            X (DataFrame or array-like): Data on which to evaluate the model.\n            y (Series or array-like): True values.\n\n        Returns:\n            float: A quantitative score representing model performance.\n        \"\"\"\n        pass\n\n\nclass BaseTransformer(ABC):\n    \"\"\"\n    BaseTransformer is the common interface for components that perform data transformations.\n    This includes tasks such as scaling, dimensionality reduction, feature extraction, etc.\n    Subclasses should implement the fitting and transformation logic.\n    \"\"\"\n    \n    @abstractmethod\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the transformer to the data.\n\n        Args:\n            X (DataFrame or array-like): The data to fit.\n            y (Series or array-like, optional): Target values if applicable.\n\n        Returns:\n            self: The fitted transformer instance.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def transform(self, X):\n        \"\"\"\n        Transform the input data according to the component's logic.\n\n        Args:\n            X (DataFrame or array-like): Data to transform.\n\n        Returns:\n            Transformed data in a format similar to the input.\n        \"\"\"\n        pass\n\n    def fit_transform(self, X, y=None):\n        \"\"\"\n        Fit to the data, then apply the transformation.\n\n        Args:\n            X (DataFrame or array-like): Data to fit and transform.\n            y (Series or array-like, optional): Optional target values.\n\n        Returns:\n            The transformed data.\n        \"\"\"\n        self.fit(X, y)\n        return self.transform(X)\n\n\nclass BaseEvaluator(ABC):\n    \"\"\"\n    BaseEvaluator provides a standard interface for evaluating machine learning models.\n    It encapsulates the logic for calculating performance metrics and generating reports.\n    Subclasses should implement specific evaluation methods.\n    \"\"\"\n    \n    @abstractmethod\n    def evaluate(self, predictions, ground_truth):\n        \"\"\"\n        Evaluate predictions against the ground truth.\n\n        Args:\n            predictions (array-like): Model predictions.\n            ground_truth (array-like): True labels or values.\n\n        Returns:\n            dict: A dictionary mapping metric names to computed values.\n        \"\"\"\n        pass\n\n    def report(self, metrics):\n        \"\"\"\n        Generate a formatted report based on computed evaluation metrics.\n\n        Args:\n            metrics (dict): Dictionary where keys are metric names and values are metric scores.\n\n        Returns:\n            str: A formatted report summarizing the evaluation.\n        \"\"\"\n        pass",
                                    "feature_paths": [],
                                    "units": [
                                        {
                                            "name": "from abc import ABC, abstractmethod",
                                            "unit_type": "import",
                                            "file_path": "./src/general/base.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "from abc import ABC, abstractmethod",
                                            "lineno": 1
                                        },
                                        {
                                            "name": "BaseEstimator",
                                            "unit_type": "class",
                                            "file_path": "./src/general/base.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "class BaseEstimator(ABC):\n    \"\"\"\n    BaseEstimator is the abstract base class for all machine learning algorithms.\n    It defines the core interface for estimators used in supervised, unsupervised,\n    and other learning methods. Subclasses are expected to implement training and\n    prediction logic.\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the model using the training data.\n\n        Args:\n            X (DataFrame or array-like): Training data.\n            y (Series or array-like, optional): Target values for supervised learning.\n\n        Returns:\n            self: The fitted estimator instance.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def predict(self, X):\n        \"\"\"\n        Make predictions using the fitted model.\n\n        Args:\n            X (DataFrame or array-like): Data for which predictions are made.\n\n        Returns:\n            Array-like: Predicted values.\n        \"\"\"\n        pass\n\n    def score(self, X, y):\n        \"\"\"\n        Evaluate the performance of the model.\n\n        Args:\n            X (DataFrame or array-like): Data on which to evaluate the model.\n            y (Series or array-like): True values.\n\n        Returns:\n            float: A quantitative score representing model performance.\n        \"\"\"\n        pass",
                                            "lineno": 3
                                        },
                                        {
                                            "name": "fit",
                                            "unit_type": "method",
                                            "file_path": "./src/general/base.py",
                                            "parent": "BaseEstimator",
                                            "extra": {},
                                            "code": "@abstractmethod\ndef fit(self, X, y=None):\n    \"\"\"\n        Fit the model using the training data.\n\n        Args:\n            X (DataFrame or array-like): Training data.\n            y (Series or array-like, optional): Target values for supervised learning.\n\n        Returns:\n            self: The fitted estimator instance.\n        \"\"\"\n    pass",
                                            "lineno": 12
                                        },
                                        {
                                            "name": "predict",
                                            "unit_type": "method",
                                            "file_path": "./src/general/base.py",
                                            "parent": "BaseEstimator",
                                            "extra": {},
                                            "code": "@abstractmethod\ndef predict(self, X):\n    \"\"\"\n        Make predictions using the fitted model.\n\n        Args:\n            X (DataFrame or array-like): Data for which predictions are made.\n\n        Returns:\n            Array-like: Predicted values.\n        \"\"\"\n    pass",
                                            "lineno": 26
                                        },
                                        {
                                            "name": "score",
                                            "unit_type": "method",
                                            "file_path": "./src/general/base.py",
                                            "parent": "BaseEstimator",
                                            "extra": {},
                                            "code": "def score(self, X, y):\n    \"\"\"\n        Evaluate the performance of the model.\n\n        Args:\n            X (DataFrame or array-like): Data on which to evaluate the model.\n            y (Series or array-like): True values.\n\n        Returns:\n            float: A quantitative score representing model performance.\n        \"\"\"\n    pass",
                                            "lineno": 38
                                        },
                                        {
                                            "name": "BaseTransformer",
                                            "unit_type": "class",
                                            "file_path": "./src/general/base.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "class BaseTransformer(ABC):\n    \"\"\"\n    BaseTransformer is the common interface for components that perform data transformations.\n    This includes tasks such as scaling, dimensionality reduction, feature extraction, etc.\n    Subclasses should implement the fitting and transformation logic.\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the transformer to the data.\n\n        Args:\n            X (DataFrame or array-like): The data to fit.\n            y (Series or array-like, optional): Target values if applicable.\n\n        Returns:\n            self: The fitted transformer instance.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def transform(self, X):\n        \"\"\"\n        Transform the input data according to the component's logic.\n\n        Args:\n            X (DataFrame or array-like): Data to transform.\n\n        Returns:\n            Transformed data in a format similar to the input.\n        \"\"\"\n        pass\n\n    def fit_transform(self, X, y=None):\n        \"\"\"\n        Fit to the data, then apply the transformation.\n\n        Args:\n            X (DataFrame or array-like): Data to fit and transform.\n            y (Series or array-like, optional): Optional target values.\n\n        Returns:\n            The transformed data.\n        \"\"\"\n        self.fit(X, y)\n        return self.transform(X)",
                                            "lineno": 52
                                        },
                                        {
                                            "name": "fit",
                                            "unit_type": "method",
                                            "file_path": "./src/general/base.py",
                                            "parent": "BaseTransformer",
                                            "extra": {},
                                            "code": "@abstractmethod\ndef fit(self, X, y=None):\n    \"\"\"\n        Fit the transformer to the data.\n\n        Args:\n            X (DataFrame or array-like): The data to fit.\n            y (Series or array-like, optional): Target values if applicable.\n\n        Returns:\n            self: The fitted transformer instance.\n        \"\"\"\n    pass",
                                            "lineno": 60
                                        },
                                        {
                                            "name": "transform",
                                            "unit_type": "method",
                                            "file_path": "./src/general/base.py",
                                            "parent": "BaseTransformer",
                                            "extra": {},
                                            "code": "@abstractmethod\ndef transform(self, X):\n    \"\"\"\n        Transform the input data according to the component's logic.\n\n        Args:\n            X (DataFrame or array-like): Data to transform.\n\n        Returns:\n            Transformed data in a format similar to the input.\n        \"\"\"\n    pass",
                                            "lineno": 74
                                        },
                                        {
                                            "name": "fit_transform",
                                            "unit_type": "method",
                                            "file_path": "./src/general/base.py",
                                            "parent": "BaseTransformer",
                                            "extra": {},
                                            "code": "def fit_transform(self, X, y=None):\n    \"\"\"\n        Fit to the data, then apply the transformation.\n\n        Args:\n            X (DataFrame or array-like): Data to fit and transform.\n            y (Series or array-like, optional): Optional target values.\n\n        Returns:\n            The transformed data.\n        \"\"\"\n    self.fit(X, y)\n    return self.transform(X)",
                                            "lineno": 86
                                        },
                                        {
                                            "name": "BaseEvaluator",
                                            "unit_type": "class",
                                            "file_path": "./src/general/base.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "class BaseEvaluator(ABC):\n    \"\"\"\n    BaseEvaluator provides a standard interface for evaluating machine learning models.\n    It encapsulates the logic for calculating performance metrics and generating reports.\n    Subclasses should implement specific evaluation methods.\n    \"\"\"\n\n    @abstractmethod\n    def evaluate(self, predictions, ground_truth):\n        \"\"\"\n        Evaluate predictions against the ground truth.\n\n        Args:\n            predictions (array-like): Model predictions.\n            ground_truth (array-like): True labels or values.\n\n        Returns:\n            dict: A dictionary mapping metric names to computed values.\n        \"\"\"\n        pass\n\n    def report(self, metrics):\n        \"\"\"\n        Generate a formatted report based on computed evaluation metrics.\n\n        Args:\n            metrics (dict): Dictionary where keys are metric names and values are metric scores.\n\n        Returns:\n            str: A formatted report summarizing the evaluation.\n        \"\"\"\n        pass",
                                            "lineno": 101
                                        },
                                        {
                                            "name": "evaluate",
                                            "unit_type": "method",
                                            "file_path": "./src/general/base.py",
                                            "parent": "BaseEvaluator",
                                            "extra": {},
                                            "code": "@abstractmethod\ndef evaluate(self, predictions, ground_truth):\n    \"\"\"\n        Evaluate predictions against the ground truth.\n\n        Args:\n            predictions (array-like): Model predictions.\n            ground_truth (array-like): True labels or values.\n\n        Returns:\n            dict: A dictionary mapping metric names to computed values.\n        \"\"\"\n    pass",
                                            "lineno": 109
                                        },
                                        {
                                            "name": "report",
                                            "unit_type": "method",
                                            "file_path": "./src/general/base.py",
                                            "parent": "BaseEvaluator",
                                            "extra": {},
                                            "code": "def report(self, metrics):\n    \"\"\"\n        Generate a formatted report based on computed evaluation metrics.\n\n        Args:\n            metrics (dict): Dictionary where keys are metric names and values are metric scores.\n\n        Returns:\n            str: A formatted report summarizing the evaluation.\n        \"\"\"\n    pass",
                                            "lineno": 122
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "type": "directory",
                            "name": "workflow",
                            "path": "./src/workflow",
                            "children": [
                                {
                                    "type": "file",
                                    "name": "base_component.py",
                                    "path": "./src/workflow/base_component.py",
                                    "code": "from abc import ABC, abstractmethod\n\nclass BaseWorkflowComponent(ABC):\n    \"\"\"\n    BaseWorkflowComponent defines the interface for workflow operations.\n    Components such as preprocessing steps and evaluation stages should inherit from\n    this class and implement the process method.\n    \"\"\"\n    \n    @abstractmethod\n    def process(self, data):\n        \"\"\"\n        Process the given data as part of the workflow.\n\n        Args:\n            data (any): Input data (e.g., DataFrame, dict) to be processed.\n\n        Returns:\n            any: The processed output data.\n        \"\"\"\n        pass",
                                    "feature_paths": [],
                                    "units": [
                                        {
                                            "name": "from abc import ABC, abstractmethod",
                                            "unit_type": "import",
                                            "file_path": "./src/workflow/base_component.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "from abc import ABC, abstractmethod",
                                            "lineno": 1
                                        },
                                        {
                                            "name": "BaseWorkflowComponent",
                                            "unit_type": "class",
                                            "file_path": "./src/workflow/base_component.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "class BaseWorkflowComponent(ABC):\n    \"\"\"\n    BaseWorkflowComponent defines the interface for workflow operations.\n    Components such as preprocessing steps and evaluation stages should inherit from\n    this class and implement the process method.\n    \"\"\"\n\n    @abstractmethod\n    def process(self, data):\n        \"\"\"\n        Process the given data as part of the workflow.\n\n        Args:\n            data (any): Input data (e.g., DataFrame, dict) to be processed.\n\n        Returns:\n            any: The processed output data.\n        \"\"\"\n        pass",
                                            "lineno": 3
                                        },
                                        {
                                            "name": "process",
                                            "unit_type": "method",
                                            "file_path": "./src/workflow/base_component.py",
                                            "parent": "BaseWorkflowComponent",
                                            "extra": {},
                                            "code": "@abstractmethod\ndef process(self, data):\n    \"\"\"\n        Process the given data as part of the workflow.\n\n        Args:\n            data (any): Input data (e.g., DataFrame, dict) to be processed.\n\n        Returns:\n            any: The processed output data.\n        \"\"\"\n    pass",
                                            "lineno": 11
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "preprocessing",
                                    "path": "./src/workflow/preprocessing",
                                    "children": [
                                        {
                                            "type": "directory",
                                            "name": "feature_engineering",
                                            "path": "./src/workflow/preprocessing/feature_engineering",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "data_transformation.py",
                                                    "path": "./src/workflow/preprocessing/feature_engineering/data_transformation.py",
                                                    "code": "from scipy import sparse\nfrom typing import Optional, List, Union\nimport pandas as pd\n\ndef apply_onehot_encoding(df: pd.DataFrame, columns: Optional[List[str]]=None, sparse_output: bool=False) -> Union[pd.DataFrame, sparse.csr_matrix]:\n    \"\"\"\n    Apply one-hot encoding to the specified categorical columns of the input DataFrame.\n    \n    This function transforms categorical columns into one-hot encoded representations.\n    If 'columns' are not specified, all columns with object or category dtypes will be encoded.\n    The output can be returned as a dense DataFrame or as a sparse CSR matrix based on the\n    'sparse_output' flag.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing data to encode.\n        columns (Optional[List[str]]): List of column names to one-hot encode. If None, infer categorical columns.\n        sparse_output (bool): Flag indicating whether the output should be a sparse CSR matrix. Defaults to False.\n\n    Returns:\n        Union[pd.DataFrame, sparse.csr_matrix]: One-hot encoded data in dense or sparse format.\n\n    Edge Cases:\n        - If no categorical columns exist or the specified columns are not present, the function returns the original DataFrame.\n        - If an empty DataFrame is passed, an empty DataFrame (or corresponding sparse matrix) is returned.\n\n    Assumptions:\n        - The input DataFrame is properly formatted.\n    \"\"\"\n    pass\n\ndef scale_and_normalize_features(df: pd.DataFrame, feature_columns: Optional[List[str]]=None, scaling_range: Optional[tuple]=(0, 1), normalization_method: str='zscore') -> pd.DataFrame:\n    \"\"\"\n    Scale and normalize the specified features in the input DataFrame.\n    \n    This function performs two transformations:\n      1. Scaling: Rescales numerical features to a specified range (default is [0, 1]).\n      2. Normalization: Applies normalization to standardize features. The default normalization\n         method is 'zscore' (transforming data to have zero mean and unit variance).\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing features to be transformed.\n        feature_columns (Optional[List[str]]): List of columns to be scaled and normalized. \n                                               If None, all numeric columns are processed.\n        scaling_range (Optional[tuple]): Desired range for scaling as (min, max). Defaults to (0, 1).\n        normalization_method (str): The normalization method to apply. Supported:\n                                    'zscore' for standard normalization.\n                                    Additional methods can be incorporated if needed.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with scaled and normalized feature values.\n\n    Edge Cases:\n        - If the specified feature_columns list is empty or none of the columns are numeric,\n          the original DataFrame is returned.\n        - If an invalid normalization method is specified, behavior is undefined (implementation should handle errors).\n\n    Assumptions:\n        - The input DataFrame is valid and contains numeric types for the scaling and normalization.\n        - The normalization_method adheres to supported methods which are validated elsewhere.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from scipy import sparse",
                                                            "unit_type": "import",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/data_transformation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from scipy import sparse",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import Optional, List, Union",
                                                            "unit_type": "import",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/data_transformation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional, List, Union",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/data_transformation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "apply_onehot_encoding",
                                                            "unit_type": "function",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/data_transformation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def apply_onehot_encoding(df: pd.DataFrame, columns: Optional[List[str]]=None, sparse_output: bool=False) -> Union[pd.DataFrame, sparse.csr_matrix]:\n    \"\"\"\n    Apply one-hot encoding to the specified categorical columns of the input DataFrame.\n    \n    This function transforms categorical columns into one-hot encoded representations.\n    If 'columns' are not specified, all columns with object or category dtypes will be encoded.\n    The output can be returned as a dense DataFrame or as a sparse CSR matrix based on the\n    'sparse_output' flag.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing data to encode.\n        columns (Optional[List[str]]): List of column names to one-hot encode. If None, infer categorical columns.\n        sparse_output (bool): Flag indicating whether the output should be a sparse CSR matrix. Defaults to False.\n\n    Returns:\n        Union[pd.DataFrame, sparse.csr_matrix]: One-hot encoded data in dense or sparse format.\n\n    Edge Cases:\n        - If no categorical columns exist or the specified columns are not present, the function returns the original DataFrame.\n        - If an empty DataFrame is passed, an empty DataFrame (or corresponding sparse matrix) is returned.\n\n    Assumptions:\n        - The input DataFrame is properly formatted.\n    \"\"\"\n    pass",
                                                            "lineno": 5
                                                        },
                                                        {
                                                            "name": "scale_and_normalize_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/data_transformation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def scale_and_normalize_features(df: pd.DataFrame, feature_columns: Optional[List[str]]=None, scaling_range: Optional[tuple]=(0, 1), normalization_method: str='zscore') -> pd.DataFrame:\n    \"\"\"\n    Scale and normalize the specified features in the input DataFrame.\n    \n    This function performs two transformations:\n      1. Scaling: Rescales numerical features to a specified range (default is [0, 1]).\n      2. Normalization: Applies normalization to standardize features. The default normalization\n         method is 'zscore' (transforming data to have zero mean and unit variance).\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing features to be transformed.\n        feature_columns (Optional[List[str]]): List of columns to be scaled and normalized. \n                                               If None, all numeric columns are processed.\n        scaling_range (Optional[tuple]): Desired range for scaling as (min, max). Defaults to (0, 1).\n        normalization_method (str): The normalization method to apply. Supported:\n                                    'zscore' for standard normalization.\n                                    Additional methods can be incorporated if needed.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with scaled and normalized feature values.\n\n    Edge Cases:\n        - If the specified feature_columns list is empty or none of the columns are numeric,\n          the original DataFrame is returned.\n        - If an invalid normalization method is specified, behavior is undefined (implementation should handle errors).\n\n    Assumptions:\n        - The input DataFrame is valid and contains numeric types for the scaling and normalization.\n        - The normalization_method adheres to supported methods which are validated elsewhere.\n    \"\"\"\n    pass",
                                                            "lineno": 31
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "feature_extraction.py",
                                                    "path": "./src/workflow/preprocessing/feature_engineering/feature_extraction.py",
                                                    "code": "import pandas as pd\nfrom typing import Optional\n\ndef extract_pca_features(data: pd.DataFrame, n_components: int=2, method: str='svd') -> pd.DataFrame:\n    \"\"\"\n    Apply Principal Component Analysis (PCA) to extract principal components as features from the input data.\n    \n    This function is designed for the preprocessing pipeline's feature extraction stage, enabling dimensionality\n    reduction via PCA. It provides an interface for transforming the input DataFrame into a reduced set of features\n    while preserving as much variance as possible from the original dataset.\n    \n    Args:\n        data (pd.DataFrame): The input data containing the features to be transformed.\n        n_components (int, optional): The number of principal components to extract. Defaults to 2.\n        method (str, optional): The PCA computation method to use. Accepted values may include 'svd' for singular value decomposition,\n                                or other method identifiers as defined in the underlying PCA implementation. Defaults to 'svd'.\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the extracted principal components, with at most n_components columns.\n    \n    Edge Cases & Assumptions:\n        - It is assumed that the input DataFrame 'data' is preprocessed (e.g., missing values handled) prior to PCA.\n        - The function does not handle scaling; any required scaling should occur in an earlier pipeline stage.\n        - The 'method' parameter allows for future extension to different PCA computation algorithms.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/feature_extraction.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/feature_extraction.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "extract_pca_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/feature_extraction.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def extract_pca_features(data: pd.DataFrame, n_components: int=2, method: str='svd') -> pd.DataFrame:\n    \"\"\"\n    Apply Principal Component Analysis (PCA) to extract principal components as features from the input data.\n    \n    This function is designed for the preprocessing pipeline's feature extraction stage, enabling dimensionality\n    reduction via PCA. It provides an interface for transforming the input DataFrame into a reduced set of features\n    while preserving as much variance as possible from the original dataset.\n    \n    Args:\n        data (pd.DataFrame): The input data containing the features to be transformed.\n        n_components (int, optional): The number of principal components to extract. Defaults to 2.\n        method (str, optional): The PCA computation method to use. Accepted values may include 'svd' for singular value decomposition,\n                                or other method identifiers as defined in the underlying PCA implementation. Defaults to 'svd'.\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the extracted principal components, with at most n_components columns.\n    \n    Edge Cases & Assumptions:\n        - It is assumed that the input DataFrame 'data' is preprocessed (e.g., missing values handled) prior to PCA.\n        - The function does not handle scaling; any required scaling should occur in an earlier pipeline stage.\n        - The 'method' parameter allows for future extension to different PCA computation algorithms.\n    \"\"\"\n    pass",
                                                            "lineno": 4
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "data_augmentation.py",
                                                    "path": "./src/workflow/preprocessing/feature_engineering/data_augmentation.py",
                                                    "code": "from typing import List, Optional\nimport pandas as pd\nimport numpy as np\nfrom typing import Optional, Dict\n\ndef augment_text_data(text_data: pd.DataFrame, augmentation_methods: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Augment textual data using specified augmentation methods.\n    \n    This function applies one or more text augmentation techniques to the input\n    DataFrame containing textual data. Typical augmentation methods may include\n    synonym replacement, random insertion, random swap, and deletion. This interface\n    enables users to enrich the dataset for downstream machine learning tasks,\n    especially in scenarios with limited text samples.\n    \n    Args:\n        text_data (pd.DataFrame): A DataFrame containing text data to augment.\n        augmentation_methods (Optional[List[str]]): A list of augmentation method names \n            to apply (e.g., ['synonym_replacement', 'random_insertion']). If None, default \n            augmentation methods may be used.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with the augmented text data.\n    \n    Edge Cases:\n        - If the input DataFrame is empty, the function returns an empty DataFrame.\n        - If augmentation_methods is None or empty, the function should handle it gracefully,\n          potentially applying a default method or returning the original DataFrame.\n    \n    Assumptions:\n        - The DataFrame has a column (or columns) that can be interpreted as textual data.\n    \"\"\"\n    pass\n\ndef augment_image_data(image_data: np.ndarray, augmentation_params: Optional[Dict[str, any]]=None) -> np.ndarray:\n    \"\"\"\n    Apply augmentation techniques to image data.\n    \n    This function performs image augmentation operations such as rotation, scaling,\n    flipping, and cropping on the input numpy array representing image(s). The optional \n    augmentation parameters allow for configuring specific details of the augmentation \n    process. This is particularly useful for expanding image datasets and improving\n    model robustness in computer vision tasks.\n    \n    Args:\n        image_data (np.ndarray): A numpy array representing one or more images.\n        augmentation_params (Optional[Dict[str, any]]): A dictionary containing augmentation \n            parameter settings (e.g., rotation angle, scaling factors, crop dimensions). If \n            None, default augmentation parameters should be applied.\n    \n    Returns:\n        np.ndarray: A numpy array of the augmented image data.\n    \n    Edge Cases:\n        - If the image_data array is empty, the function returns an empty array.\n        - If augmentation_params is None, default augmentation transformations are applied.\n    \n    Assumptions:\n        - The input numpy array conforms to the expected shape and data type for image data.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import List, Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/data_augmentation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List, Optional",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/data_augmentation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "import numpy as np",
                                                            "unit_type": "import",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/data_augmentation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import numpy as np",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "from typing import Optional, Dict",
                                                            "unit_type": "import",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/data_augmentation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional, Dict",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "augment_text_data",
                                                            "unit_type": "function",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/data_augmentation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def augment_text_data(text_data: pd.DataFrame, augmentation_methods: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Augment textual data using specified augmentation methods.\n    \n    This function applies one or more text augmentation techniques to the input\n    DataFrame containing textual data. Typical augmentation methods may include\n    synonym replacement, random insertion, random swap, and deletion. This interface\n    enables users to enrich the dataset for downstream machine learning tasks,\n    especially in scenarios with limited text samples.\n    \n    Args:\n        text_data (pd.DataFrame): A DataFrame containing text data to augment.\n        augmentation_methods (Optional[List[str]]): A list of augmentation method names \n            to apply (e.g., ['synonym_replacement', 'random_insertion']). If None, default \n            augmentation methods may be used.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with the augmented text data.\n    \n    Edge Cases:\n        - If the input DataFrame is empty, the function returns an empty DataFrame.\n        - If augmentation_methods is None or empty, the function should handle it gracefully,\n          potentially applying a default method or returning the original DataFrame.\n    \n    Assumptions:\n        - The DataFrame has a column (or columns) that can be interpreted as textual data.\n    \"\"\"\n    pass",
                                                            "lineno": 6
                                                        },
                                                        {
                                                            "name": "augment_image_data",
                                                            "unit_type": "function",
                                                            "file_path": "./src/workflow/preprocessing/feature_engineering/data_augmentation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def augment_image_data(image_data: np.ndarray, augmentation_params: Optional[Dict[str, any]]=None) -> np.ndarray:\n    \"\"\"\n    Apply augmentation techniques to image data.\n    \n    This function performs image augmentation operations such as rotation, scaling,\n    flipping, and cropping on the input numpy array representing image(s). The optional \n    augmentation parameters allow for configuring specific details of the augmentation \n    process. This is particularly useful for expanding image datasets and improving\n    model robustness in computer vision tasks.\n    \n    Args:\n        image_data (np.ndarray): A numpy array representing one or more images.\n        augmentation_params (Optional[Dict[str, any]]): A dictionary containing augmentation \n            parameter settings (e.g., rotation angle, scaling factors, crop dimensions). If \n            None, default augmentation parameters should be applied.\n    \n    Returns:\n        np.ndarray: A numpy array of the augmented image data.\n    \n    Edge Cases:\n        - If the image_data array is empty, the function returns an empty array.\n        - If augmentation_params is None, default augmentation transformations are applied.\n    \n    Assumptions:\n        - The input numpy array conforms to the expected shape and data type for image data.\n    \"\"\"\n    pass",
                                                            "lineno": 35
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "normalization",
                                            "path": "./src/workflow/preprocessing/normalization",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "z_score.py",
                                                    "path": "./src/workflow/preprocessing/normalization/z_score.py",
                                                    "code": "from typing import Optional, List\nimport pandas as pd\n\ndef normalize_z_score(df: pd.DataFrame, columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Normalize the input dataframe using Z-score normalization.\n\n    This function transforms specified numerical columns of the input dataframe so that they have a mean of 0 \n    and a standard deviation of 1. If the 'columns' parameter is None, the function will apply normalization \n    to all numeric columns present in the dataframe.\n\n    Args:\n        df (pd.DataFrame): The input dataframe containing the data to be normalized.\n        columns (Optional[List[str]]): A list of column names to normalize. If None, all numeric columns will be processed.\n\n    Returns:\n        pd.DataFrame: A new dataframe with the selected columns normalized using Z-score scaling.\n\n    Notes:\n        - The function assumes that the specified columns contain numeric data.\n        - It does not modify the original dataframe.\n        - Columns with zero standard deviation may lead to division by zero errors; handling such cases is left to the implementer.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import Optional, List",
                                                            "unit_type": "import",
                                                            "file_path": "./src/workflow/preprocessing/normalization/z_score.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional, List",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/workflow/preprocessing/normalization/z_score.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "normalize_z_score",
                                                            "unit_type": "function",
                                                            "file_path": "./src/workflow/preprocessing/normalization/z_score.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def normalize_z_score(df: pd.DataFrame, columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Normalize the input dataframe using Z-score normalization.\n\n    This function transforms specified numerical columns of the input dataframe so that they have a mean of 0 \n    and a standard deviation of 1. If the 'columns' parameter is None, the function will apply normalization \n    to all numeric columns present in the dataframe.\n\n    Args:\n        df (pd.DataFrame): The input dataframe containing the data to be normalized.\n        columns (Optional[List[str]]): A list of column names to normalize. If None, all numeric columns will be processed.\n\n    Returns:\n        pd.DataFrame: A new dataframe with the selected columns normalized using Z-score scaling.\n\n    Notes:\n        - The function assumes that the specified columns contain numeric data.\n        - It does not modify the original dataframe.\n        - Columns with zero standard deviation may lead to division by zero errors; handling such cases is left to the implementer.\n    \"\"\"\n    pass",
                                                            "lineno": 4
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "evaluation",
                                    "path": "./src/workflow/evaluation",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "accuracy.py",
                                            "path": "./src/workflow/evaluation/accuracy.py",
                                            "code": "from typing import Optional\nimport numpy as np\n\ndef compute_f1_score(y_true: np.ndarray, y_pred: np.ndarray, average: Optional[str]='binary') -> float:\n    \"\"\"\n    Compute the F1 score, which is the harmonic mean of precision and recall.\n    \n    The F1 score is a widely used accuracy metric for classification tasks,\n    combining the precision and recall into a single score by taking their harmonic mean.\n    This function serves as a public-facing interface for computing the F1 score\n    for a given set of true labels and predicted labels.\n    \n    Args:\n        y_true (np.ndarray): Array of true class labels.\n        y_pred (np.ndarray): Array of predicted class labels.\n        average (str, optional): Defines the type of averaging performed on the data.\n             - 'binary': Only report results for the class specified by pos_label.\n             - 'micro', 'macro', 'weighted': Different averaging methods applicable to multi-class problems.\n             Defaults to 'binary'.\n    \n    Returns:\n        float: The computed F1 score as a floating-point value.\n    \n    Edge Cases:\n        - The function assumes that y_true and y_pred are valid numpy arrays of the same shape.\n        - For binary classification, it is assumed that the labels are encoded in a consistent manner.\n        - If the input arrays are empty or the metric is ill-defined, the behavior should be handled gracefully.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "from typing import Optional",
                                                    "unit_type": "import",
                                                    "file_path": "./src/workflow/evaluation/accuracy.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Optional",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "import numpy as np",
                                                    "unit_type": "import",
                                                    "file_path": "./src/workflow/evaluation/accuracy.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import numpy as np",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "compute_f1_score",
                                                    "unit_type": "function",
                                                    "file_path": "./src/workflow/evaluation/accuracy.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def compute_f1_score(y_true: np.ndarray, y_pred: np.ndarray, average: Optional[str]='binary') -> float:\n    \"\"\"\n    Compute the F1 score, which is the harmonic mean of precision and recall.\n    \n    The F1 score is a widely used accuracy metric for classification tasks,\n    combining the precision and recall into a single score by taking their harmonic mean.\n    This function serves as a public-facing interface for computing the F1 score\n    for a given set of true labels and predicted labels.\n    \n    Args:\n        y_true (np.ndarray): Array of true class labels.\n        y_pred (np.ndarray): Array of predicted class labels.\n        average (str, optional): Defines the type of averaging performed on the data.\n             - 'binary': Only report results for the class specified by pos_label.\n             - 'micro', 'macro', 'weighted': Different averaging methods applicable to multi-class problems.\n             Defaults to 'binary'.\n    \n    Returns:\n        float: The computed F1 score as a floating-point value.\n    \n    Edge Cases:\n        - The function assumes that y_true and y_pred are valid numpy arrays of the same shape.\n        - For binary classification, it is assumed that the labels are encoded in a consistent manner.\n        - If the input arrays are empty or the metric is ill-defined, the behavior should be handled gracefully.\n    \"\"\"\n    pass",
                                                    "lineno": 4
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "performance.py",
                                            "path": "./src/workflow/evaluation/performance.py",
                                            "code": "import numpy as np\nfrom typing import Optional, List, Any\n\ndef compute_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, labels: Optional[List[Any]]=None) -> np.ndarray:\n    \"\"\"\n    Compute the confusion matrix to evaluate the accuracy of a classification.\n\n    The confusion matrix is a summary table used to assess the performance of a classification\n    algorithm, by displaying the count of true positive, false positive, true negative, and false negative predictions.\n    This implementation compares the actual labels (y_true) to the predicted labels (y_pred).\n\n    Args:\n        y_true (np.ndarray): Array of true labels.\n        y_pred (np.ndarray): Array of predicted labels.\n        labels (Optional[List[Any]]): List of unique labels used in the classification to define the mapping\n            of matrix rows and columns. If None, this will be inferred from the input data.\n\n    Returns:\n        np.ndarray: A 2D numpy array where the element at index [i, j] represents the count of samples \n        with true label corresponding to the i-th class and predicted label corresponding to the j-th class.\n\n    Edge Cases:\n        - If the input arrays are empty or their lengths do not match, the behavior is undefined.\n        - If labels are not provided, they must be inferred correctly from y_true and y_pred to maintain consistency.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import numpy as np",
                                                    "unit_type": "import",
                                                    "file_path": "./src/workflow/evaluation/performance.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import numpy as np",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "from typing import Optional, List, Any",
                                                    "unit_type": "import",
                                                    "file_path": "./src/workflow/evaluation/performance.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Optional, List, Any",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "compute_confusion_matrix",
                                                    "unit_type": "function",
                                                    "file_path": "./src/workflow/evaluation/performance.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def compute_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, labels: Optional[List[Any]]=None) -> np.ndarray:\n    \"\"\"\n    Compute the confusion matrix to evaluate the accuracy of a classification.\n\n    The confusion matrix is a summary table used to assess the performance of a classification\n    algorithm, by displaying the count of true positive, false positive, true negative, and false negative predictions.\n    This implementation compares the actual labels (y_true) to the predicted labels (y_pred).\n\n    Args:\n        y_true (np.ndarray): Array of true labels.\n        y_pred (np.ndarray): Array of predicted labels.\n        labels (Optional[List[Any]]): List of unique labels used in the classification to define the mapping\n            of matrix rows and columns. If None, this will be inferred from the input data.\n\n    Returns:\n        np.ndarray: A 2D numpy array where the element at index [i, j] represents the count of samples \n        with true label corresponding to the i-th class and predicted label corresponding to the j-th class.\n\n    Edge Cases:\n        - If the input arrays are empty or their lengths do not match, the behavior is undefined.\n        - If labels are not provided, they must be inferred correctly from y_true and y_pred to maintain consistency.\n    \"\"\"\n    pass",
                                                    "lineno": 4
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "type": "directory",
                            "name": "data_engineering",
                            "path": "./src/data_engineering",
                            "children": [
                                {
                                    "type": "file",
                                    "name": "base_processor.py",
                                    "path": "./src/data_engineering/base_processor.py",
                                    "code": "from abc import ABC, abstractmethod\n\nclass BaseDataProcessor(ABC):\n    \"\"\"\n    BaseDataProcessor provides a common interface for data preparation tasks.\n    This includes cleaning, feature engineering, aggregation, and validation steps.\n    Subclasses are expected to implement specific processing logic.\n    \"\"\"\n    \n    @abstractmethod\n    def process(self, data):\n        \"\"\"\n        Process and transform raw input data.\n\n        Args:\n            data (DataFrame): Raw data to process.\n\n        Returns:\n            DataFrame: Cleaned and transformed data.\n        \"\"\"\n        pass\n\n    def validate(self, data):\n        \"\"\"\n        Validate the processed data to ensure quality and consistency.\n\n        Args:\n            data (DataFrame): Data to be validated.\n\n        Returns:\n            bool: True if data passes the validation criteria, False otherwise.\n        \"\"\"\n        pass",
                                    "feature_paths": [],
                                    "units": [
                                        {
                                            "name": "from abc import ABC, abstractmethod",
                                            "unit_type": "import",
                                            "file_path": "./src/data_engineering/base_processor.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "from abc import ABC, abstractmethod",
                                            "lineno": 1
                                        },
                                        {
                                            "name": "BaseDataProcessor",
                                            "unit_type": "class",
                                            "file_path": "./src/data_engineering/base_processor.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "class BaseDataProcessor(ABC):\n    \"\"\"\n    BaseDataProcessor provides a common interface for data preparation tasks.\n    This includes cleaning, feature engineering, aggregation, and validation steps.\n    Subclasses are expected to implement specific processing logic.\n    \"\"\"\n\n    @abstractmethod\n    def process(self, data):\n        \"\"\"\n        Process and transform raw input data.\n\n        Args:\n            data (DataFrame): Raw data to process.\n\n        Returns:\n            DataFrame: Cleaned and transformed data.\n        \"\"\"\n        pass\n\n    def validate(self, data):\n        \"\"\"\n        Validate the processed data to ensure quality and consistency.\n\n        Args:\n            data (DataFrame): Data to be validated.\n\n        Returns:\n            bool: True if data passes the validation criteria, False otherwise.\n        \"\"\"\n        pass",
                                            "lineno": 3
                                        },
                                        {
                                            "name": "process",
                                            "unit_type": "method",
                                            "file_path": "./src/data_engineering/base_processor.py",
                                            "parent": "BaseDataProcessor",
                                            "extra": {},
                                            "code": "@abstractmethod\ndef process(self, data):\n    \"\"\"\n        Process and transform raw input data.\n\n        Args:\n            data (DataFrame): Raw data to process.\n\n        Returns:\n            DataFrame: Cleaned and transformed data.\n        \"\"\"\n    pass",
                                            "lineno": 11
                                        },
                                        {
                                            "name": "validate",
                                            "unit_type": "method",
                                            "file_path": "./src/data_engineering/base_processor.py",
                                            "parent": "BaseDataProcessor",
                                            "extra": {},
                                            "code": "def validate(self, data):\n    \"\"\"\n        Validate the processed data to ensure quality and consistency.\n\n        Args:\n            data (DataFrame): Data to be validated.\n\n        Returns:\n            bool: True if data passes the validation criteria, False otherwise.\n        \"\"\"\n    pass",
                                            "lineno": 23
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "data_preparation",
                                    "path": "./src/data_engineering/data_preparation",
                                    "children": [
                                        {
                                            "type": "directory",
                                            "name": "cleaning",
                                            "path": "./src/data_engineering/data_preparation/cleaning",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "initial_cleaning.py",
                                                    "path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                    "code": "import pandas as pd\n\ndef dropna_values(df: pd.DataFrame, axis: int=0, how: str='any') -> pd.DataFrame:\n    \"\"\"\n    Drop NA/null values from a DataFrame.\n\n    This function removes missing values from the DataFrame along the specified axis.\n    It uses pandas' dropna functionality.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        axis (int, optional): The axis along which to drop NA values (0 for rows, 1 for columns). Defaults to 0.\n        how (str, optional): Determine if row or column is removed based on 'any' or 'all' NA values. Defaults to 'any'.\n\n    Returns:\n        pd.DataFrame: DataFrame with NA values dropped.\n\n    Edge Cases:\n        Returns the original DataFrame if there are no NA values.\n    \"\"\"\n    pass\n\ndef convert_to_appropriate_types(df: pd.DataFrame, type_mapping: dict) -> pd.DataFrame:\n    \"\"\"\n    Convert DataFrame columns to appropriate types.\n\n    This function casts DataFrame columns based on a provided mapping from column names\n    to desired data types.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        type_mapping (dict): Dictionary mapping column names (str) to target data types.\n\n    Returns:\n        pd.DataFrame: DataFrame with columns converted to the specified types.\n\n    Edge Cases:\n        Columns not present in the mapping remain unchanged.\n    \"\"\"\n    pass\n\ndef clip_dataframe_values(df: pd.DataFrame, lower: float=None, upper: float=None) -> pd.DataFrame:\n    \"\"\"\n    Clip the values of a DataFrame to specified lower and/or upper bounds.\n\n    This function limits all numerical values in the DataFrame to be within\n    the specified range.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        lower (float, optional): Lower bound for clipping. Defaults to None.\n        upper (float, optional): Upper bound for clipping. Defaults to None.\n\n    Returns:\n        pd.DataFrame: DataFrame with values clipped within the specified bounds.\n\n    Edge Cases:\n        If both bounds are None, the original DataFrame is returned unchanged.\n    \"\"\"\n    pass\n\ndef detect_outliers_using_clustering(df: pd.DataFrame, clustering_params: dict) -> pd.DataFrame:\n    \"\"\"\n    Detect outliers in a DataFrame using clustering techniques.\n\n    This function applies a clustering algorithm to the DataFrame to identify\n    data points that are considered outliers based on cluster assignments.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        clustering_params (dict): Parameters for the clustering algorithm (e.g., number of clusters, distance metrics).\n\n    Returns:\n        pd.DataFrame: DataFrame with an additional indicator column for detected outliers.\n\n    Edge Cases:\n        If the DataFrame is empty or clustering fails, an unmodified DataFrame may be returned.\n    \"\"\"\n    pass\n\ndef drop_columns_with_missing(df: pd.DataFrame, threshold: float=0.5) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from a DataFrame that have a proportion of missing values above a threshold.\n\n    This function evaluates each column for the proportion of missing values\n    and removes those columns that exceed the defined threshold.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Maximum allowed fraction of missing values per column. Defaults to 0.5.\n\n    Returns:\n        pd.DataFrame: DataFrame after dropping columns with excessive missing data.\n\n    Edge Cases:\n        If no columns meet the dropping criteria, the original DataFrame is returned intact.\n    \"\"\"\n    pass\n\ndef tag_outliers(df: pd.DataFrame, method: str='IQR', factor: float=1.5) -> pd.DataFrame:\n    \"\"\"\n    Tag outliers in a DataFrame based on a specified statistical method.\n\n    This function identifies and tags outlier data points using either the IQR method\n    or another specified method and adds a flag column indicating outlier status.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        method (str, optional): Statistical method to use ('IQR' or other). Defaults to 'IQR'.\n        factor (float, optional): Multiplicative factor for determining thresholds. Defaults to 1.5.\n\n    Returns:\n        pd.DataFrame: DataFrame with an added column that flags outlier rows.\n\n    Edge Cases:\n        If the method is unsupported or no outliers are found, a flag column with default values is added.\n    \"\"\"\n    pass\n\ndef remove_by_standard_deviation(df: pd.DataFrame, threshold: float=3.0) -> pd.DataFrame:\n    \"\"\"\n    Remove rows from a DataFrame that deviate from the mean by more than a specified number of standard deviations.\n\n    This function calculates the standard deviation for numerical columns and removes rows\n    that fall outside the defined threshold range.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Number of standard deviations from the mean to consider as the limit. Defaults to 3.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with extreme outlier rows removed.\n\n    Edge Cases:\n        Returns the original DataFrame if no rows exceed the threshold.\n    \"\"\"\n    pass\n\ndef remove_outlier_rows(df: pd.DataFrame, outlier_indicator_column: str='is_outlier') -> pd.DataFrame:\n    \"\"\"\n    Remove rows identified as outliers from a DataFrame.\n\n    This function drops rows that have been flagged as outliers based on a specified indicator column.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        outlier_indicator_column (str, optional): Column name that indicates outlier status. Defaults to 'is_outlier'.\n\n    Returns:\n        pd.DataFrame: DataFrame with outlier rows removed.\n\n    Edge Cases:\n        If the indicator column does not exist, the original DataFrame is returned unchanged.\n    \"\"\"\n    pass\n\ndef remove_top_percent(df: pd.DataFrame, percent: float=1.0) -> pd.DataFrame:\n    \"\"\"\n    Remove the top percentage of extreme values from a DataFrame.\n\n    This function removes the rows corresponding to the top specified percentage \n    (by value) of a target numerical column, often used to exclude extreme outliers.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        percent (float, optional): The percentage of top extreme values to remove. Defaults to 1.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with the top percent of extreme values removed.\n\n    Edge Cases:\n        If the computed cutoff excludes no rows, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef standardize_string_data(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n    Standardize string data in specified DataFrame columns.\n\n    This function ensures that string data in the selected columns is standardized,\n    which may include trimming, lowercasing, or other normalization.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list): List of column names (str) to standardize.\n\n    Returns:\n        pd.DataFrame: DataFrame with standardized string data in specified columns.\n\n    Edge Cases:\n        If a specified column is not of string type, it will be skipped.\n    \"\"\"\n    pass\n\ndef standardize_date_formats(df: pd.DataFrame, date_columns: list, date_format: str='%Y-%m-%d') -> pd.DataFrame:\n    \"\"\"\n    Standardize the date formats in specified DataFrame columns.\n\n    This function converts dates in given columns to a uniform date format.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        date_columns (list): List of column names (str) containing date values.\n        date_format (str, optional): The target format for dates. Defaults to \"%Y-%m-%d\".\n\n    Returns:\n        pd.DataFrame: DataFrame with dates in standardized format.\n\n    Edge Cases:\n        If a date conversion fails, the original value is retained.\n    \"\"\"\n    pass\n\ndef convert_units(df: pd.DataFrame, conversion_mapping: dict) -> pd.DataFrame:\n    \"\"\"\n    Convert units of measurement for specified columns in a DataFrame.\n\n    This function applies unit conversions to DataFrame columns based on a provided mapping,\n    where each key is a column and the value is a conversion function or factor.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        conversion_mapping (dict): A dictionary mapping column names to conversion parameters.\n\n    Returns:\n        pd.DataFrame: DataFrame with converted unit values.\n\n    Edge Cases:\n        If a column is not present or conversion fails, the column remains unchanged.\n    \"\"\"\n    pass\n\ndef use_forward_fill(df: pd.DataFrame, columns: list=None) -> pd.DataFrame:\n    \"\"\"\n    Fill missing values in a DataFrame using forward fill method.\n\n    This function applies a forward-fill strategy to impute missing values,\n    propagating the last valid observation forward.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list, optional): List of column names (str) to forward fill. \n                                  If None, applies to all columns.\n\n    Returns:\n        pd.DataFrame: DataFrame with missing values filled using forward fill.\n\n    Edge Cases:\n        If there are no missing values, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef use_z_score_method(df: pd.DataFrame, threshold: float=3.0) -> pd.DataFrame:\n    \"\"\"\n    Identify outliers in a DataFrame using the Z-score method.\n\n    This function calculates the Z-score for numerical columns and flags rows\n    where the absolute Z-score exceeds the defined threshold.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Z-score threshold to identify outliers. Defaults to 3.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with an additional indicator for outliers based on the Z-score.\n\n    Edge Cases:\n        If no outliers are present according to the threshold, the DataFrame is returned unmodified.\n    \"\"\"\n    pass\n\ndef use_iqr_method(df: pd.DataFrame, factor: float=1.5) -> pd.DataFrame:\n    \"\"\"\n    Identify outliers in a DataFrame using the IQR method.\n\n    This function computes the interquartile range for numerical columns and flags rows\n    that lie outside the acceptable range defined by a multiplier factor.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        factor (float, optional): Multiplier for the IQR to set thresholds for outlier detection. Defaults to 1.5.\n\n    Returns:\n        pd.DataFrame: DataFrame with an indicator marking rows considered outliers based on the IQR method.\n\n    Edge Cases:\n        If the IQR cannot be computed (e.g., insufficient data), the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef standardize_numeric_formats(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n    Standardize the numeric formats in specified DataFrame columns.\n\n    This function ensures that numerical columns are formatted uniformly,\n    which may include rounding, setting precision, or applying consistent scaling.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list): List of column names (str) to standardize numerically.\n\n    Returns:\n        pd.DataFrame: DataFrame with standardized numeric columns.\n\n    Edge Cases:\n        If a non-numeric column is included in the columns list, it is skipped.\n    \"\"\"\n    pass\n\ndef fill_with_constant(df: pd.DataFrame, value, columns: list=None) -> pd.DataFrame:\n    \"\"\"\n    Fill missing or undesirable values in a DataFrame with a constant value.\n\n    This function replaces missing values or outlier values with a specified constant across\n    all or selected columns.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        value: The constant value to fill in.\n        columns (list, optional): List of column names (str) to apply the fill.\n                                  If None, applies to entire DataFrame.\n\n    Returns:\n        pd.DataFrame: DataFrame with specified columns filled with the constant value.\n\n    Edge Cases:\n        If no columns are specified and the DataFrame has no missing values, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef reset_index_to_default(df: pd.DataFrame, drop: bool=True) -> pd.DataFrame:\n    \"\"\"\n    Reset the index of a DataFrame to the default integer index.\n\n    This function reinitializes the DataFrame index, optionally dropping the old index.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        drop (bool, optional): Whether to drop the existing index. Defaults to True.\n\n    Returns:\n        pd.DataFrame: DataFrame with the index reset to the default integer index.\n\n    Edge Cases:\n        If the index is already the default range index, the DataFrame is returned unchanged.\n    \"\"\"\n    pass\n\ndef convert_text_to_number(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n    Convert textual representations of numbers in specified DataFrame columns to numeric types.\n\n    This function attempts to cast string representations of numbers to numeric values.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list): List of column names (str) that contain textual numbers.\n\n    Returns:\n        pd.DataFrame: DataFrame with the specified columns converted to numeric types.\n\n    Edge Cases:\n        Improperly formatted strings that cannot be converted will remain unchanged or be set to NaN.\n    \"\"\"\n    pass\n\ndef correct_data_inconsistencies(df: pd.DataFrame, correction_rules: dict) -> pd.DataFrame:\n    \"\"\"\n    Correct data inconsistencies in a DataFrame based on predefined rules.\n\n    This function applies a set of rules to standardize and correct inconsistent data entries,\n    ensuring data quality and uniformity.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        correction_rules (dict): A mapping from column names to correction functions or rules.\n\n    Returns:\n        pd.DataFrame: DataFrame with corrected data inconsistencies.\n\n    Edge Cases:\n        If a column does not have a corresponding rule, it is left unchanged.\n    \"\"\"\n    pass\n\ndef drop_null_columns(df: pd.DataFrame, threshold: float=1.0) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from a DataFrame that consist entirely of null values.\n\n    This function examines each column and drops those that have a fraction of non-null entries\n    lower than the specified threshold (typically 1.0 to indicate all values are null).\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Fraction of non-null values required to retain the column.\n                                     Defaults to 1.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with null-only columns removed.\n\n    Edge Cases:\n        If no columns meet the criteria for dropping, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef reset_and_drop_old_index(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Reset the index of a DataFrame to default and drop the old index column if it exists.\n\n    This function reinitializes the DataFrame index and ensures any previous index column is discarded.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n\n    Returns:\n        pd.DataFrame: DataFrame with a new default index and without the old index column.\n\n    Edge Cases:\n        If there is no old index column to drop, the function behaves like a simple index reset.\n    \"\"\"\n    pass\n\ndef replace_with_default_value(df: pd.DataFrame, default_mapping: dict) -> pd.DataFrame:\n    \"\"\"\n    Replace specified values in a DataFrame with default values.\n\n    This function uses a mapping of column names to default replacement values to\n    substitute certain data entries in a DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        default_mapping (dict): Dictionary where keys are column names and values are the default value to apply.\n\n    Returns:\n        pd.DataFrame: DataFrame with specified values replaced by defaults.\n\n    Edge Cases:\n        If a column in the mapping does not exist in the DataFrame, it is ignored.\n    \"\"\"\n    pass\n\ndef drop_rows(df: pd.DataFrame, condition) -> pd.DataFrame:\n    \"\"\"\n    Drop rows from a DataFrame that meet a specified condition.\n\n    This function removes rows based on a provided condition, which can be a boolean mask or a callable\n    that takes a row and returns a boolean value.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        condition: A boolean array, boolean Series, or callable that defines the drop condition.\n\n    Returns:\n        pd.DataFrame: DataFrame with rows dropped based on the condition.\n\n    Edge Cases:\n        If the condition does not match any rows, the original DataFrame is returned.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "dropna_values",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def dropna_values(df: pd.DataFrame, axis: int=0, how: str='any') -> pd.DataFrame:\n    \"\"\"\n    Drop NA/null values from a DataFrame.\n\n    This function removes missing values from the DataFrame along the specified axis.\n    It uses pandas' dropna functionality.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        axis (int, optional): The axis along which to drop NA values (0 for rows, 1 for columns). Defaults to 0.\n        how (str, optional): Determine if row or column is removed based on 'any' or 'all' NA values. Defaults to 'any'.\n\n    Returns:\n        pd.DataFrame: DataFrame with NA values dropped.\n\n    Edge Cases:\n        Returns the original DataFrame if there are no NA values.\n    \"\"\"\n    pass",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "convert_to_appropriate_types",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def convert_to_appropriate_types(df: pd.DataFrame, type_mapping: dict) -> pd.DataFrame:\n    \"\"\"\n    Convert DataFrame columns to appropriate types.\n\n    This function casts DataFrame columns based on a provided mapping from column names\n    to desired data types.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        type_mapping (dict): Dictionary mapping column names (str) to target data types.\n\n    Returns:\n        pd.DataFrame: DataFrame with columns converted to the specified types.\n\n    Edge Cases:\n        Columns not present in the mapping remain unchanged.\n    \"\"\"\n    pass",
                                                            "lineno": 23
                                                        },
                                                        {
                                                            "name": "clip_dataframe_values",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def clip_dataframe_values(df: pd.DataFrame, lower: float=None, upper: float=None) -> pd.DataFrame:\n    \"\"\"\n    Clip the values of a DataFrame to specified lower and/or upper bounds.\n\n    This function limits all numerical values in the DataFrame to be within\n    the specified range.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        lower (float, optional): Lower bound for clipping. Defaults to None.\n        upper (float, optional): Upper bound for clipping. Defaults to None.\n\n    Returns:\n        pd.DataFrame: DataFrame with values clipped within the specified bounds.\n\n    Edge Cases:\n        If both bounds are None, the original DataFrame is returned unchanged.\n    \"\"\"\n    pass",
                                                            "lineno": 42
                                                        },
                                                        {
                                                            "name": "detect_outliers_using_clustering",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def detect_outliers_using_clustering(df: pd.DataFrame, clustering_params: dict) -> pd.DataFrame:\n    \"\"\"\n    Detect outliers in a DataFrame using clustering techniques.\n\n    This function applies a clustering algorithm to the DataFrame to identify\n    data points that are considered outliers based on cluster assignments.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        clustering_params (dict): Parameters for the clustering algorithm (e.g., number of clusters, distance metrics).\n\n    Returns:\n        pd.DataFrame: DataFrame with an additional indicator column for detected outliers.\n\n    Edge Cases:\n        If the DataFrame is empty or clustering fails, an unmodified DataFrame may be returned.\n    \"\"\"\n    pass",
                                                            "lineno": 62
                                                        },
                                                        {
                                                            "name": "drop_columns_with_missing",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def drop_columns_with_missing(df: pd.DataFrame, threshold: float=0.5) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from a DataFrame that have a proportion of missing values above a threshold.\n\n    This function evaluates each column for the proportion of missing values\n    and removes those columns that exceed the defined threshold.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Maximum allowed fraction of missing values per column. Defaults to 0.5.\n\n    Returns:\n        pd.DataFrame: DataFrame after dropping columns with excessive missing data.\n\n    Edge Cases:\n        If no columns meet the dropping criteria, the original DataFrame is returned intact.\n    \"\"\"\n    pass",
                                                            "lineno": 81
                                                        },
                                                        {
                                                            "name": "tag_outliers",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def tag_outliers(df: pd.DataFrame, method: str='IQR', factor: float=1.5) -> pd.DataFrame:\n    \"\"\"\n    Tag outliers in a DataFrame based on a specified statistical method.\n\n    This function identifies and tags outlier data points using either the IQR method\n    or another specified method and adds a flag column indicating outlier status.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        method (str, optional): Statistical method to use ('IQR' or other). Defaults to 'IQR'.\n        factor (float, optional): Multiplicative factor for determining thresholds. Defaults to 1.5.\n\n    Returns:\n        pd.DataFrame: DataFrame with an added column that flags outlier rows.\n\n    Edge Cases:\n        If the method is unsupported or no outliers are found, a flag column with default values is added.\n    \"\"\"\n    pass",
                                                            "lineno": 100
                                                        },
                                                        {
                                                            "name": "remove_by_standard_deviation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def remove_by_standard_deviation(df: pd.DataFrame, threshold: float=3.0) -> pd.DataFrame:\n    \"\"\"\n    Remove rows from a DataFrame that deviate from the mean by more than a specified number of standard deviations.\n\n    This function calculates the standard deviation for numerical columns and removes rows\n    that fall outside the defined threshold range.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Number of standard deviations from the mean to consider as the limit. Defaults to 3.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with extreme outlier rows removed.\n\n    Edge Cases:\n        Returns the original DataFrame if no rows exceed the threshold.\n    \"\"\"\n    pass",
                                                            "lineno": 120
                                                        },
                                                        {
                                                            "name": "remove_outlier_rows",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def remove_outlier_rows(df: pd.DataFrame, outlier_indicator_column: str='is_outlier') -> pd.DataFrame:\n    \"\"\"\n    Remove rows identified as outliers from a DataFrame.\n\n    This function drops rows that have been flagged as outliers based on a specified indicator column.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        outlier_indicator_column (str, optional): Column name that indicates outlier status. Defaults to 'is_outlier'.\n\n    Returns:\n        pd.DataFrame: DataFrame with outlier rows removed.\n\n    Edge Cases:\n        If the indicator column does not exist, the original DataFrame is returned unchanged.\n    \"\"\"\n    pass",
                                                            "lineno": 139
                                                        },
                                                        {
                                                            "name": "remove_top_percent",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def remove_top_percent(df: pd.DataFrame, percent: float=1.0) -> pd.DataFrame:\n    \"\"\"\n    Remove the top percentage of extreme values from a DataFrame.\n\n    This function removes the rows corresponding to the top specified percentage \n    (by value) of a target numerical column, often used to exclude extreme outliers.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        percent (float, optional): The percentage of top extreme values to remove. Defaults to 1.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with the top percent of extreme values removed.\n\n    Edge Cases:\n        If the computed cutoff excludes no rows, the original DataFrame is returned.\n    \"\"\"\n    pass",
                                                            "lineno": 157
                                                        },
                                                        {
                                                            "name": "standardize_string_data",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def standardize_string_data(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n    Standardize string data in specified DataFrame columns.\n\n    This function ensures that string data in the selected columns is standardized,\n    which may include trimming, lowercasing, or other normalization.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list): List of column names (str) to standardize.\n\n    Returns:\n        pd.DataFrame: DataFrame with standardized string data in specified columns.\n\n    Edge Cases:\n        If a specified column is not of string type, it will be skipped.\n    \"\"\"\n    pass",
                                                            "lineno": 176
                                                        },
                                                        {
                                                            "name": "standardize_date_formats",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def standardize_date_formats(df: pd.DataFrame, date_columns: list, date_format: str='%Y-%m-%d') -> pd.DataFrame:\n    \"\"\"\n    Standardize the date formats in specified DataFrame columns.\n\n    This function converts dates in given columns to a uniform date format.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        date_columns (list): List of column names (str) containing date values.\n        date_format (str, optional): The target format for dates. Defaults to \"%Y-%m-%d\".\n\n    Returns:\n        pd.DataFrame: DataFrame with dates in standardized format.\n\n    Edge Cases:\n        If a date conversion fails, the original value is retained.\n    \"\"\"\n    pass",
                                                            "lineno": 195
                                                        },
                                                        {
                                                            "name": "convert_units",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def convert_units(df: pd.DataFrame, conversion_mapping: dict) -> pd.DataFrame:\n    \"\"\"\n    Convert units of measurement for specified columns in a DataFrame.\n\n    This function applies unit conversions to DataFrame columns based on a provided mapping,\n    where each key is a column and the value is a conversion function or factor.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        conversion_mapping (dict): A dictionary mapping column names to conversion parameters.\n\n    Returns:\n        pd.DataFrame: DataFrame with converted unit values.\n\n    Edge Cases:\n        If a column is not present or conversion fails, the column remains unchanged.\n    \"\"\"\n    pass",
                                                            "lineno": 214
                                                        },
                                                        {
                                                            "name": "use_forward_fill",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def use_forward_fill(df: pd.DataFrame, columns: list=None) -> pd.DataFrame:\n    \"\"\"\n    Fill missing values in a DataFrame using forward fill method.\n\n    This function applies a forward-fill strategy to impute missing values,\n    propagating the last valid observation forward.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list, optional): List of column names (str) to forward fill. \n                                  If None, applies to all columns.\n\n    Returns:\n        pd.DataFrame: DataFrame with missing values filled using forward fill.\n\n    Edge Cases:\n        If there are no missing values, the original DataFrame is returned.\n    \"\"\"\n    pass",
                                                            "lineno": 233
                                                        },
                                                        {
                                                            "name": "use_z_score_method",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def use_z_score_method(df: pd.DataFrame, threshold: float=3.0) -> pd.DataFrame:\n    \"\"\"\n    Identify outliers in a DataFrame using the Z-score method.\n\n    This function calculates the Z-score for numerical columns and flags rows\n    where the absolute Z-score exceeds the defined threshold.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Z-score threshold to identify outliers. Defaults to 3.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with an additional indicator for outliers based on the Z-score.\n\n    Edge Cases:\n        If no outliers are present according to the threshold, the DataFrame is returned unmodified.\n    \"\"\"\n    pass",
                                                            "lineno": 253
                                                        },
                                                        {
                                                            "name": "use_iqr_method",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def use_iqr_method(df: pd.DataFrame, factor: float=1.5) -> pd.DataFrame:\n    \"\"\"\n    Identify outliers in a DataFrame using the IQR method.\n\n    This function computes the interquartile range for numerical columns and flags rows\n    that lie outside the acceptable range defined by a multiplier factor.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        factor (float, optional): Multiplier for the IQR to set thresholds for outlier detection. Defaults to 1.5.\n\n    Returns:\n        pd.DataFrame: DataFrame with an indicator marking rows considered outliers based on the IQR method.\n\n    Edge Cases:\n        If the IQR cannot be computed (e.g., insufficient data), the original DataFrame is returned.\n    \"\"\"\n    pass",
                                                            "lineno": 272
                                                        },
                                                        {
                                                            "name": "standardize_numeric_formats",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def standardize_numeric_formats(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n    Standardize the numeric formats in specified DataFrame columns.\n\n    This function ensures that numerical columns are formatted uniformly,\n    which may include rounding, setting precision, or applying consistent scaling.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list): List of column names (str) to standardize numerically.\n\n    Returns:\n        pd.DataFrame: DataFrame with standardized numeric columns.\n\n    Edge Cases:\n        If a non-numeric column is included in the columns list, it is skipped.\n    \"\"\"\n    pass",
                                                            "lineno": 291
                                                        },
                                                        {
                                                            "name": "fill_with_constant",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def fill_with_constant(df: pd.DataFrame, value, columns: list=None) -> pd.DataFrame:\n    \"\"\"\n    Fill missing or undesirable values in a DataFrame with a constant value.\n\n    This function replaces missing values or outlier values with a specified constant across\n    all or selected columns.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        value: The constant value to fill in.\n        columns (list, optional): List of column names (str) to apply the fill.\n                                  If None, applies to entire DataFrame.\n\n    Returns:\n        pd.DataFrame: DataFrame with specified columns filled with the constant value.\n\n    Edge Cases:\n        If no columns are specified and the DataFrame has no missing values, the original DataFrame is returned.\n    \"\"\"\n    pass",
                                                            "lineno": 310
                                                        },
                                                        {
                                                            "name": "reset_index_to_default",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def reset_index_to_default(df: pd.DataFrame, drop: bool=True) -> pd.DataFrame:\n    \"\"\"\n    Reset the index of a DataFrame to the default integer index.\n\n    This function reinitializes the DataFrame index, optionally dropping the old index.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        drop (bool, optional): Whether to drop the existing index. Defaults to True.\n\n    Returns:\n        pd.DataFrame: DataFrame with the index reset to the default integer index.\n\n    Edge Cases:\n        If the index is already the default range index, the DataFrame is returned unchanged.\n    \"\"\"\n    pass",
                                                            "lineno": 331
                                                        },
                                                        {
                                                            "name": "convert_text_to_number",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def convert_text_to_number(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n    Convert textual representations of numbers in specified DataFrame columns to numeric types.\n\n    This function attempts to cast string representations of numbers to numeric values.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        columns (list): List of column names (str) that contain textual numbers.\n\n    Returns:\n        pd.DataFrame: DataFrame with the specified columns converted to numeric types.\n\n    Edge Cases:\n        Improperly formatted strings that cannot be converted will remain unchanged or be set to NaN.\n    \"\"\"\n    pass",
                                                            "lineno": 349
                                                        },
                                                        {
                                                            "name": "correct_data_inconsistencies",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def correct_data_inconsistencies(df: pd.DataFrame, correction_rules: dict) -> pd.DataFrame:\n    \"\"\"\n    Correct data inconsistencies in a DataFrame based on predefined rules.\n\n    This function applies a set of rules to standardize and correct inconsistent data entries,\n    ensuring data quality and uniformity.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        correction_rules (dict): A mapping from column names to correction functions or rules.\n\n    Returns:\n        pd.DataFrame: DataFrame with corrected data inconsistencies.\n\n    Edge Cases:\n        If a column does not have a corresponding rule, it is left unchanged.\n    \"\"\"\n    pass",
                                                            "lineno": 367
                                                        },
                                                        {
                                                            "name": "drop_null_columns",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def drop_null_columns(df: pd.DataFrame, threshold: float=1.0) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from a DataFrame that consist entirely of null values.\n\n    This function examines each column and drops those that have a fraction of non-null entries\n    lower than the specified threshold (typically 1.0 to indicate all values are null).\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold (float, optional): Fraction of non-null values required to retain the column.\n                                     Defaults to 1.0.\n\n    Returns:\n        pd.DataFrame: DataFrame with null-only columns removed.\n\n    Edge Cases:\n        If no columns meet the criteria for dropping, the original DataFrame is returned.\n    \"\"\"\n    pass",
                                                            "lineno": 386
                                                        },
                                                        {
                                                            "name": "reset_and_drop_old_index",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def reset_and_drop_old_index(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Reset the index of a DataFrame to default and drop the old index column if it exists.\n\n    This function reinitializes the DataFrame index and ensures any previous index column is discarded.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n\n    Returns:\n        pd.DataFrame: DataFrame with a new default index and without the old index column.\n\n    Edge Cases:\n        If there is no old index column to drop, the function behaves like a simple index reset.\n    \"\"\"\n    pass",
                                                            "lineno": 406
                                                        },
                                                        {
                                                            "name": "replace_with_default_value",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def replace_with_default_value(df: pd.DataFrame, default_mapping: dict) -> pd.DataFrame:\n    \"\"\"\n    Replace specified values in a DataFrame with default values.\n\n    This function uses a mapping of column names to default replacement values to\n    substitute certain data entries in a DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        default_mapping (dict): Dictionary where keys are column names and values are the default value to apply.\n\n    Returns:\n        pd.DataFrame: DataFrame with specified values replaced by defaults.\n\n    Edge Cases:\n        If a column in the mapping does not exist in the DataFrame, it is ignored.\n    \"\"\"\n    pass",
                                                            "lineno": 423
                                                        },
                                                        {
                                                            "name": "drop_rows",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/initial_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def drop_rows(df: pd.DataFrame, condition) -> pd.DataFrame:\n    \"\"\"\n    Drop rows from a DataFrame that meet a specified condition.\n\n    This function removes rows based on a provided condition, which can be a boolean mask or a callable\n    that takes a row and returns a boolean value.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        condition: A boolean array, boolean Series, or callable that defines the drop condition.\n\n    Returns:\n        pd.DataFrame: DataFrame with rows dropped based on the condition.\n\n    Edge Cases:\n        If the condition does not match any rows, the original DataFrame is returned.\n    \"\"\"\n    pass",
                                                            "lineno": 442
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "supplementary_cleaning.py",
                                                    "path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                    "code": "import pandas as pd\n\nclass OutlierFilter:\n    \"\"\"\n    Provides methods for identifying and handling outliers in a DataFrame.\n\n    This class supports multiple techniques including filtering by percentile, identifying high outliers,\n    threshold based filtering, IQR-based removal, and generalized outlier identification.\n    \"\"\"\n\n    def filter_by_percentile(self, df: pd.DataFrame, percentile: float) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Filters rows in the DataFrame based on a specified percentile threshold.\n        \n        Extended Explanation:\n            This method removes or marks rows in the DataFrame that fall below or above a given percentile value,\n            depending on the implementation of thresholding logic. It is useful for mitigating the influence of extreme values.\n        \n        Args:\n            df (pd.DataFrame): The input DataFrame to filter.\n            percentile (float): The percentile threshold (between 0 and 100) to use for filtering.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with rows filtered based on the percentile.\n        \n        Raises:\n            ValueError: If the percentile is not between 0 and 100.\n        \"\"\"\n        pass\n\n    def filter_high_outliers(self, df: pd.DataFrame, high_threshold: float) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Filters out rows that contain exceptionally high outlier values.\n        \n        Extended Explanation:\n            This method detects and filters rows where the values exceed a specified high threshold.\n            It can be applied to a specific column or across the DataFrame if aggregated.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            high_threshold (float): The threshold above which values are considered high outliers.\n        \n        Returns:\n            pd.DataFrame: DataFrame with high outliers removed or flagged.\n        \n        Raises:\n            ValueError: If high_threshold is not a positive number.\n        \"\"\"\n        pass\n\n    def filter_by_threshold(self, df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Filters rows based on a fixed numeric threshold.\n        \n        Extended Explanation:\n            This method removes rows where specified numeric values exceed or fall short of a given threshold,\n            depending on business requirements.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            threshold (float): Numeric threshold for filtering.\n        \n        Returns:\n            pd.DataFrame: DataFrame filtered according to the threshold.\n        \n        Raises:\n            ValueError: If threshold is non-numeric.\n        \"\"\"\n        pass\n\n    def remove_based_on_iqr(self, df: pd.DataFrame, factor: float=1.5) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes outliers using the IQR method.\n        \n        Extended Explanation:\n            This method calculates the Interquartile Range (IQR) and filters out rows that\n            lie beyond the range defined by the specified factor multiplied by the IQR.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            factor (float): The multiplier for the IQR to define the acceptable range.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with outliers removed based on the IQR method.\n        \n        Raises:\n            ValueError: If factor is not a positive number.\n        \"\"\"\n        pass\n\n    def identify_outliers(self, df: pd.DataFrame, method: str='standard') -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Identifies outliers in the DataFrame using a specified detection method.\n        \n        Extended Explanation:\n            This method labels or flags rows that are potential outliers following the specified \n            detection strategy (e.g., standard deviation, modified z-score, etc.). No rows are removed.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            method (str): The method used for detecting outliers. Defaults to 'standard'.\n        \n        Returns:\n            pd.DataFrame: DataFrame with an additional column flagging outliers.\n        \n        Raises:\n            ValueError: If the specified method is not supported.\n        \"\"\"\n        pass\n\nclass DuplicateCleaner:\n    \"\"\"\n    Implements methods for dealing with duplicate data in a DataFrame.\n\n    This class covers keeping the first occurrence of duplicate records, dropping duplicate columns,\n    removing exact duplicate rows, and handling near-duplicate rows through fuzzy deduplication techniques.\n    \"\"\"\n\n    def keep_first(self, df: pd.DataFrame, subset: list=None) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Keeps the first occurrence of duplicate rows and removes subsequent duplicates.\n        \n        Extended Explanation:\n            This method checks for duplicate rows based on all columns or a specified subset of columns,\n            and retains only the first occurrence while removing others.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing possible duplicate rows.\n            subset (list, optional): List of column names to consider for identifying duplicates.\n        \n        Returns:\n            pd.DataFrame: DataFrame with duplicates removed, preserving the first occurrence.\n        \n        Raises:\n            ValueError: If 'df' is not a valid DataFrame.\n        \"\"\"\n        pass\n\n    def drop_duplicate_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes columns that are exact duplicates of each other.\n        \n        Extended Explanation:\n            This method examines the DataFrame for columns that have identical content and drops the redundant ones.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n        \n        Returns:\n            pd.DataFrame: DataFrame with duplicate columns removed.\n        \n        Raises:\n            ValueError: If the DataFrame is empty or improperly formatted.\n        \"\"\"\n        pass\n\n    def remove_exact_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes rows that are exactly duplicated.\n        \n        Extended Explanation:\n            This method identifies rows that are completely identical across all columns and removes the duplicates.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to be de-duplicated.\n        \n        Returns:\n            pd.DataFrame: DataFrame with exact duplicate rows removed.\n        \n        Raises:\n            ValueError: If 'df' has unexpected structure.\n        \"\"\"\n        pass\n\n    def fuzzy_deduplication(self, df: pd.DataFrame, similarity_threshold: float=0.8) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Applies fuzzy matching to identify and remove near-duplicate rows.\n        \n        Extended Explanation:\n            This method uses a similarity threshold to determine which rows are sufficiently similar \n            to be considered duplicates and then removes or flags them.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame to process.\n            similarity_threshold (float): Similarity threshold (between 0 and 1); values closer to 1 denote stricter matching.\n        \n        Returns:\n            pd.DataFrame: DataFrame with near-duplicates handled.\n        \n        Raises:\n            ValueError: If similarity_threshold is outside the [0, 1] range.\n        \"\"\"\n        pass\n\nclass DataConversionScaler:\n    \"\"\"\n    Provides conversion and scaling operations for numeric data in a DataFrame.\n\n    This class includes methods for converting data types to integers and ordinals,\n    applying decimal scaling, and standardizing data using robust scaling techniques.\n    \"\"\"\n\n    def convert_to_integer(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Converts specified columns in a DataFrame to integer type.\n        \n        Extended Explanation:\n            This method is used to ensure that numeric data is stored as integers. It processes\n            the provided columns and converts any float or string representations of numbers to integers.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing the columns to convert.\n            columns (list): List of column names to be converted to integer type.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with the specified columns converted to integer.\n        \n        Raises:\n            TypeError: If conversion cannot be performed due to incompatible data types.\n        \"\"\"\n        pass\n\n    def convert_to_ordinal(self, df: pd.DataFrame, columns: list, mapping: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Converts specified categorical columns to ordinal values.\n        \n        Extended Explanation:\n            This method applies a mapping dictionary to convert nominal or categorical data into ordinal form,\n            which can facilitate further numeric processing.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing categorical columns.\n            columns (list): List of column names to convert.\n            mapping (dict): Dictionary mapping original values to ordinal integers.\n        \n        Returns:\n            pd.DataFrame: DataFrame with specified columns converted to ordinal values.\n        \n        Raises:\n            KeyError: If a column value is missing from the mapping.\n        \"\"\"\n        pass\n\n    def apply_decimal_scaling(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Scales numeric columns using decimal scaling.\n        \n        Extended Explanation:\n            This method normalizes numeric data by shifting the decimal point. This is useful for adjusting the magnitude\n            of numbers without changing their distribution.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing numerical columns.\n            columns (list): List of columns to be scaled.\n        \n        Returns:\n            pd.DataFrame: DataFrame with scaled numeric columns.\n        \n        Raises:\n            ValueError: If columns contain non-numeric values.\n        \"\"\"\n        pass\n\n    def standardize_using_robust_scaling(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Standardizes data using techniques that are robust to outliers.\n        \n        Extended Explanation:\n            Rather than using mean and standard deviation, this method employs robust statistical measures\n            such as the median and IQR to reduce the influence of extreme values.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing columns to be standardized.\n            columns (list): List of column names to standardize.\n        \n        Returns:\n            pd.DataFrame: DataFrame with robust-scaled values.\n        \n        Raises:\n            ValueError: If provided columns contain non-numeric data.\n        \"\"\"\n        pass\n\nclass CategoricalCleaner:\n    \"\"\"\n    Provides methods for cleaning and standardizing categorical or textual data in a DataFrame.\n\n    This includes operations to standardize categorical formats, remove special characters using regex,\n    and fill missing or problematic entries with either custom values or zero.\n    \"\"\"\n\n    def standardize_categorical_data(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Standardizes the representation of categorical data in the DataFrame.\n        \n        Extended Explanation:\n            This method adjusts the string formats in the specified columns to a consistent standard, which\n            may involve trimming spaces, changing case, or other normalization procedures.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            columns (list): List of columns containing categorical data to standardize.\n        \n        Returns:\n            pd.DataFrame: DataFrame with standardized categorical data.\n        \n        Raises:\n            ValueError: If any of the specified columns are not of a string type.\n        \"\"\"\n        pass\n\n    def remove_special_characters(self, df: pd.DataFrame, columns: list, pattern: str='[^a-zA-Z0-9\\\\s]') -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes special characters from the specified string columns.\n        \n        Extended Explanation:\n            Using a regular expression pattern, this method cleans text data by removing characters that\n            do not match the allowed set defined by the pattern.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing textual data.\n            columns (list): List of column names to clean.\n            pattern (str): Regex pattern specifying allowed characters. Defaults to alphanumeric and whitespace.\n        \n        Returns:\n            pd.DataFrame: DataFrame with special characters removed from specified columns.\n        \n        Raises:\n            re.error: If the provided regex pattern is invalid.\n        \"\"\"\n        pass\n\n    def fill_with_custom_value(self, df: pd.DataFrame, columns: list, custom_value) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Fills missing or problematic entries in specified columns with a custom value.\n        \n        Extended Explanation:\n            This method replaces NaNs or erroneous entries within the given columns with a predetermined custom value.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame with missing or problematic entries.\n            columns (list): List of columns to fill.\n            custom_value: The value to use for filling missing entries.\n        \n        Returns:\n            pd.DataFrame: DataFrame with missing entries replaced by the custom value.\n        \n        Raises:\n            TypeError: If the custom_value is of the wrong type for the target column.\n        \"\"\"\n        pass\n\n    def fill_with_zero(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Fills missing or invalid values in designated columns with zero.\n        \n        Extended Explanation:\n            This method scans the specified columns for NaN or other placeholders for missing data and fills them with zero.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to process.\n            columns (list): List of column names where zeros will replace missing values.\n        \n        Returns:\n            pd.DataFrame: DataFrame with missing values filled with zero.\n        \n        Raises:\n            ValueError: If the operation fails due to type incompatibilities.\n        \"\"\"\n        pass\n\nclass ConsistencyCorrector:\n    \"\"\"\n    Provides methods for correcting inconsistencies and errors in data formatting.\n\n    This class includes functions that adjust data to fixed formats,\n    correct typographical errors, enforce consistency across entries, and resolve conflicts.\n    \"\"\"\n\n    def fix_inconsistent_data_formats(self, df: pd.DataFrame, columns: list, format_rules: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Standardizes data formats in specified columns based on provided rules.\n        \n        Extended Explanation:\n            This method applies a set of formatting rules to the designated columns to ensure the data\n            adheres to consistent standards.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to standardize.\n            columns (list): List of column names to adjust.\n            format_rules (dict): Mapping of column names to formatting rules.\n        \n        Returns:\n            pd.DataFrame: DataFrame with standardized formats.\n        \n        Raises:\n            KeyError: If a column is missing from the provided format_rules.\n        \"\"\"\n        pass\n\n    def correct_typos(self, df: pd.DataFrame, columns: list, typo_dict: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Corrects typographical errors in specified textual columns.\n        \n        Extended Explanation:\n            This method replaces common typos with their correct forms using a dictionary mapping.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            columns (list): List of columns to inspect for typos.\n            typo_dict (dict): Dictionary mapping incorrect spellings to correct ones.\n        \n        Returns:\n            pd.DataFrame: DataFrame with typos corrected.\n        \n        Raises:\n            ValueError: If typo_dict is empty or columns are missing.\n        \"\"\"\n        pass\n\n    def enforce_consistency_rules(self, df: pd.DataFrame, rules: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Enforces predefined consistency rules across the DataFrame.\n        \n        Extended Explanation:\n            This method applies a series of rules to ensure that data across columns adheres to\n            consistent formats and logical relationships.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to process.\n            rules (dict): Dictionary containing consistency rules to apply.\n        \n        Returns:\n            pd.DataFrame: DataFrame with consistency rules enforced.\n        \n        Raises:\n            ValueError: If rules are improperly defined.\n        \"\"\"\n        pass\n\n    def correct_data_entry_errors(self, df: pd.DataFrame, columns: list, error_corrections: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Corrects known data entry errors in specified columns.\n        \n        Extended Explanation:\n            This method uses an error correction mapping to replace incorrect entries due to data entry errors.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing errors.\n            columns (list): List of columns to correct.\n            error_corrections (dict): Dictionary mapping erroneous values to correct ones.\n        \n        Returns:\n            pd.DataFrame: DataFrame with data entry errors corrected.\n        \n        Raises:\n            KeyError: If an expected correction key is missing.\n        \"\"\"\n        pass\n\n    def resolve_conflicting_data(self, df: pd.DataFrame, conflict_resolution_rules: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Resolves conflicting data entries using specified rules.\n        \n        Extended Explanation:\n            This method applies conflict resolution strategies to deal with data contradictions,\n            ensuring that the resulting DataFrame has coherent and consistent data.\n        \n        Args:\n            df (pd.DataFrame): DataFrame with conflicting entries.\n            conflict_resolution_rules (dict): Dictionary of rules to resolve conflicts.\n        \n        Returns:\n            pd.DataFrame: DataFrame with resolved conflicting data.\n        \n        Raises:\n            ValueError: If conflict_resolution_rules do not cover all conflicts.\n        \"\"\"\n        pass\n\nclass ZeroColumnHandler:\n    \"\"\"\n    Provides methods for identifying and handling columns composed entirely or predominantly of zeros.\n\n    This class includes methods to detect columns with a high proportion of zeroes and to remove columns which contain \n    only zero values.\n    \"\"\"\n\n    def identify_zero_columns(self, df: pd.DataFrame, threshold: float=0.0) -> list:\n        \"\"\"\n        Summary:\n            Identifies columns in the DataFrame where the proportion of zero values meets or exceeds the threshold.\n        \n        Extended Explanation:\n            This method evaluates each column in the DataFrame and returns a list of column names that have a zero-value\n            proportion greater than or equal to the provided threshold.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to inspect.\n            threshold (float, optional): The minimum proportion of zeros required to classify a column as zero-dominated.\n                                         Defaults to 0.0 (all zeros).\n        \n        Returns:\n            list: A list of column names that meet the zero column criteria.\n        \n        Raises:\n            ValueError: If the threshold is not between 0 and 1.\n        \"\"\"\n        pass\n\n    def drop_columns_with_all_zeros(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Drops columns from the DataFrame that contain only zeros.\n        \n        Extended Explanation:\n            This method scans each column and removes those where every element is zero. This is useful for cleaning\n            datasets that have redundant or non-informative zero-only columns.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with columns containing only zeros removed.\n        \n        Raises:\n            ValueError: If no columns can be dropped due to incompatible data types.\n        \"\"\"\n        pass\n\ndef apply_active_learning_strategy(df: 'pd.DataFrame', strategy_params: dict) -> 'pd.DataFrame':\n    \"\"\"\n    Summary:\n        Applies an active learning strategy to modify the DataFrame for sample selection or re-weighting.\n    \n    Extended Explanation:\n        This function integrates active learning techniques into the data cleaning process. It processes the provided DataFrame \n        using parameters that define how samples should be selected or weighted. The resulting DataFrame is modified to better \n        suit active learning workflows.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame on which to apply the active learning strategy.\n        strategy_params (dict): A dictionary containing parameters for the active learning strategy, such as sampling \n                                criteria or re-weighting factors.\n    \n    Returns:\n        pd.DataFrame: A modified DataFrame adjusted according to the active learning strategy.\n    \n    Raises:\n        ValueError: If strategy_params is missing required keys or contains invalid values.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "OutlierFilter",
                                                            "unit_type": "class",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class OutlierFilter:\n    \"\"\"\n    Provides methods for identifying and handling outliers in a DataFrame.\n\n    This class supports multiple techniques including filtering by percentile, identifying high outliers,\n    threshold based filtering, IQR-based removal, and generalized outlier identification.\n    \"\"\"\n\n    def filter_by_percentile(self, df: pd.DataFrame, percentile: float) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Filters rows in the DataFrame based on a specified percentile threshold.\n        \n        Extended Explanation:\n            This method removes or marks rows in the DataFrame that fall below or above a given percentile value,\n            depending on the implementation of thresholding logic. It is useful for mitigating the influence of extreme values.\n        \n        Args:\n            df (pd.DataFrame): The input DataFrame to filter.\n            percentile (float): The percentile threshold (between 0 and 100) to use for filtering.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with rows filtered based on the percentile.\n        \n        Raises:\n            ValueError: If the percentile is not between 0 and 100.\n        \"\"\"\n        pass\n\n    def filter_high_outliers(self, df: pd.DataFrame, high_threshold: float) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Filters out rows that contain exceptionally high outlier values.\n        \n        Extended Explanation:\n            This method detects and filters rows where the values exceed a specified high threshold.\n            It can be applied to a specific column or across the DataFrame if aggregated.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            high_threshold (float): The threshold above which values are considered high outliers.\n        \n        Returns:\n            pd.DataFrame: DataFrame with high outliers removed or flagged.\n        \n        Raises:\n            ValueError: If high_threshold is not a positive number.\n        \"\"\"\n        pass\n\n    def filter_by_threshold(self, df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Filters rows based on a fixed numeric threshold.\n        \n        Extended Explanation:\n            This method removes rows where specified numeric values exceed or fall short of a given threshold,\n            depending on business requirements.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            threshold (float): Numeric threshold for filtering.\n        \n        Returns:\n            pd.DataFrame: DataFrame filtered according to the threshold.\n        \n        Raises:\n            ValueError: If threshold is non-numeric.\n        \"\"\"\n        pass\n\n    def remove_based_on_iqr(self, df: pd.DataFrame, factor: float=1.5) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes outliers using the IQR method.\n        \n        Extended Explanation:\n            This method calculates the Interquartile Range (IQR) and filters out rows that\n            lie beyond the range defined by the specified factor multiplied by the IQR.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            factor (float): The multiplier for the IQR to define the acceptable range.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with outliers removed based on the IQR method.\n        \n        Raises:\n            ValueError: If factor is not a positive number.\n        \"\"\"\n        pass\n\n    def identify_outliers(self, df: pd.DataFrame, method: str='standard') -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Identifies outliers in the DataFrame using a specified detection method.\n        \n        Extended Explanation:\n            This method labels or flags rows that are potential outliers following the specified \n            detection strategy (e.g., standard deviation, modified z-score, etc.). No rows are removed.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            method (str): The method used for detecting outliers. Defaults to 'standard'.\n        \n        Returns:\n            pd.DataFrame: DataFrame with an additional column flagging outliers.\n        \n        Raises:\n            ValueError: If the specified method is not supported.\n        \"\"\"\n        pass",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "filter_by_percentile",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "OutlierFilter",
                                                            "extra": {},
                                                            "code": "def filter_by_percentile(self, df: pd.DataFrame, percentile: float) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Filters rows in the DataFrame based on a specified percentile threshold.\n        \n        Extended Explanation:\n            This method removes or marks rows in the DataFrame that fall below or above a given percentile value,\n            depending on the implementation of thresholding logic. It is useful for mitigating the influence of extreme values.\n        \n        Args:\n            df (pd.DataFrame): The input DataFrame to filter.\n            percentile (float): The percentile threshold (between 0 and 100) to use for filtering.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with rows filtered based on the percentile.\n        \n        Raises:\n            ValueError: If the percentile is not between 0 and 100.\n        \"\"\"\n    pass",
                                                            "lineno": 11
                                                        },
                                                        {
                                                            "name": "filter_high_outliers",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "OutlierFilter",
                                                            "extra": {},
                                                            "code": "def filter_high_outliers(self, df: pd.DataFrame, high_threshold: float) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Filters out rows that contain exceptionally high outlier values.\n        \n        Extended Explanation:\n            This method detects and filters rows where the values exceed a specified high threshold.\n            It can be applied to a specific column or across the DataFrame if aggregated.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            high_threshold (float): The threshold above which values are considered high outliers.\n        \n        Returns:\n            pd.DataFrame: DataFrame with high outliers removed or flagged.\n        \n        Raises:\n            ValueError: If high_threshold is not a positive number.\n        \"\"\"\n    pass",
                                                            "lineno": 32
                                                        },
                                                        {
                                                            "name": "filter_by_threshold",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "OutlierFilter",
                                                            "extra": {},
                                                            "code": "def filter_by_threshold(self, df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Filters rows based on a fixed numeric threshold.\n        \n        Extended Explanation:\n            This method removes rows where specified numeric values exceed or fall short of a given threshold,\n            depending on business requirements.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            threshold (float): Numeric threshold for filtering.\n        \n        Returns:\n            pd.DataFrame: DataFrame filtered according to the threshold.\n        \n        Raises:\n            ValueError: If threshold is non-numeric.\n        \"\"\"\n    pass",
                                                            "lineno": 53
                                                        },
                                                        {
                                                            "name": "remove_based_on_iqr",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "OutlierFilter",
                                                            "extra": {},
                                                            "code": "def remove_based_on_iqr(self, df: pd.DataFrame, factor: float=1.5) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Removes outliers using the IQR method.\n        \n        Extended Explanation:\n            This method calculates the Interquartile Range (IQR) and filters out rows that\n            lie beyond the range defined by the specified factor multiplied by the IQR.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            factor (float): The multiplier for the IQR to define the acceptable range.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with outliers removed based on the IQR method.\n        \n        Raises:\n            ValueError: If factor is not a positive number.\n        \"\"\"\n    pass",
                                                            "lineno": 74
                                                        },
                                                        {
                                                            "name": "identify_outliers",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "OutlierFilter",
                                                            "extra": {},
                                                            "code": "def identify_outliers(self, df: pd.DataFrame, method: str='standard') -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Identifies outliers in the DataFrame using a specified detection method.\n        \n        Extended Explanation:\n            This method labels or flags rows that are potential outliers following the specified \n            detection strategy (e.g., standard deviation, modified z-score, etc.). No rows are removed.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            method (str): The method used for detecting outliers. Defaults to 'standard'.\n        \n        Returns:\n            pd.DataFrame: DataFrame with an additional column flagging outliers.\n        \n        Raises:\n            ValueError: If the specified method is not supported.\n        \"\"\"\n    pass",
                                                            "lineno": 95
                                                        },
                                                        {
                                                            "name": "DuplicateCleaner",
                                                            "unit_type": "class",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class DuplicateCleaner:\n    \"\"\"\n    Implements methods for dealing with duplicate data in a DataFrame.\n\n    This class covers keeping the first occurrence of duplicate records, dropping duplicate columns,\n    removing exact duplicate rows, and handling near-duplicate rows through fuzzy deduplication techniques.\n    \"\"\"\n\n    def keep_first(self, df: pd.DataFrame, subset: list=None) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Keeps the first occurrence of duplicate rows and removes subsequent duplicates.\n        \n        Extended Explanation:\n            This method checks for duplicate rows based on all columns or a specified subset of columns,\n            and retains only the first occurrence while removing others.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing possible duplicate rows.\n            subset (list, optional): List of column names to consider for identifying duplicates.\n        \n        Returns:\n            pd.DataFrame: DataFrame with duplicates removed, preserving the first occurrence.\n        \n        Raises:\n            ValueError: If 'df' is not a valid DataFrame.\n        \"\"\"\n        pass\n\n    def drop_duplicate_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes columns that are exact duplicates of each other.\n        \n        Extended Explanation:\n            This method examines the DataFrame for columns that have identical content and drops the redundant ones.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n        \n        Returns:\n            pd.DataFrame: DataFrame with duplicate columns removed.\n        \n        Raises:\n            ValueError: If the DataFrame is empty or improperly formatted.\n        \"\"\"\n        pass\n\n    def remove_exact_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes rows that are exactly duplicated.\n        \n        Extended Explanation:\n            This method identifies rows that are completely identical across all columns and removes the duplicates.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to be de-duplicated.\n        \n        Returns:\n            pd.DataFrame: DataFrame with exact duplicate rows removed.\n        \n        Raises:\n            ValueError: If 'df' has unexpected structure.\n        \"\"\"\n        pass\n\n    def fuzzy_deduplication(self, df: pd.DataFrame, similarity_threshold: float=0.8) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Applies fuzzy matching to identify and remove near-duplicate rows.\n        \n        Extended Explanation:\n            This method uses a similarity threshold to determine which rows are sufficiently similar \n            to be considered duplicates and then removes or flags them.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame to process.\n            similarity_threshold (float): Similarity threshold (between 0 and 1); values closer to 1 denote stricter matching.\n        \n        Returns:\n            pd.DataFrame: DataFrame with near-duplicates handled.\n        \n        Raises:\n            ValueError: If similarity_threshold is outside the [0, 1] range.\n        \"\"\"\n        pass",
                                                            "lineno": 116
                                                        },
                                                        {
                                                            "name": "keep_first",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "DuplicateCleaner",
                                                            "extra": {},
                                                            "code": "def keep_first(self, df: pd.DataFrame, subset: list=None) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Keeps the first occurrence of duplicate rows and removes subsequent duplicates.\n        \n        Extended Explanation:\n            This method checks for duplicate rows based on all columns or a specified subset of columns,\n            and retains only the first occurrence while removing others.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing possible duplicate rows.\n            subset (list, optional): List of column names to consider for identifying duplicates.\n        \n        Returns:\n            pd.DataFrame: DataFrame with duplicates removed, preserving the first occurrence.\n        \n        Raises:\n            ValueError: If 'df' is not a valid DataFrame.\n        \"\"\"\n    pass",
                                                            "lineno": 124
                                                        },
                                                        {
                                                            "name": "drop_duplicate_columns",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "DuplicateCleaner",
                                                            "extra": {},
                                                            "code": "def drop_duplicate_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Removes columns that are exact duplicates of each other.\n        \n        Extended Explanation:\n            This method examines the DataFrame for columns that have identical content and drops the redundant ones.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n        \n        Returns:\n            pd.DataFrame: DataFrame with duplicate columns removed.\n        \n        Raises:\n            ValueError: If the DataFrame is empty or improperly formatted.\n        \"\"\"\n    pass",
                                                            "lineno": 145
                                                        },
                                                        {
                                                            "name": "remove_exact_duplicates",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "DuplicateCleaner",
                                                            "extra": {},
                                                            "code": "def remove_exact_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Removes rows that are exactly duplicated.\n        \n        Extended Explanation:\n            This method identifies rows that are completely identical across all columns and removes the duplicates.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to be de-duplicated.\n        \n        Returns:\n            pd.DataFrame: DataFrame with exact duplicate rows removed.\n        \n        Raises:\n            ValueError: If 'df' has unexpected structure.\n        \"\"\"\n    pass",
                                                            "lineno": 164
                                                        },
                                                        {
                                                            "name": "fuzzy_deduplication",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "DuplicateCleaner",
                                                            "extra": {},
                                                            "code": "def fuzzy_deduplication(self, df: pd.DataFrame, similarity_threshold: float=0.8) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Applies fuzzy matching to identify and remove near-duplicate rows.\n        \n        Extended Explanation:\n            This method uses a similarity threshold to determine which rows are sufficiently similar \n            to be considered duplicates and then removes or flags them.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame to process.\n            similarity_threshold (float): Similarity threshold (between 0 and 1); values closer to 1 denote stricter matching.\n        \n        Returns:\n            pd.DataFrame: DataFrame with near-duplicates handled.\n        \n        Raises:\n            ValueError: If similarity_threshold is outside the [0, 1] range.\n        \"\"\"\n    pass",
                                                            "lineno": 183
                                                        },
                                                        {
                                                            "name": "DataConversionScaler",
                                                            "unit_type": "class",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class DataConversionScaler:\n    \"\"\"\n    Provides conversion and scaling operations for numeric data in a DataFrame.\n\n    This class includes methods for converting data types to integers and ordinals,\n    applying decimal scaling, and standardizing data using robust scaling techniques.\n    \"\"\"\n\n    def convert_to_integer(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Converts specified columns in a DataFrame to integer type.\n        \n        Extended Explanation:\n            This method is used to ensure that numeric data is stored as integers. It processes\n            the provided columns and converts any float or string representations of numbers to integers.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing the columns to convert.\n            columns (list): List of column names to be converted to integer type.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with the specified columns converted to integer.\n        \n        Raises:\n            TypeError: If conversion cannot be performed due to incompatible data types.\n        \"\"\"\n        pass\n\n    def convert_to_ordinal(self, df: pd.DataFrame, columns: list, mapping: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Converts specified categorical columns to ordinal values.\n        \n        Extended Explanation:\n            This method applies a mapping dictionary to convert nominal or categorical data into ordinal form,\n            which can facilitate further numeric processing.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing categorical columns.\n            columns (list): List of column names to convert.\n            mapping (dict): Dictionary mapping original values to ordinal integers.\n        \n        Returns:\n            pd.DataFrame: DataFrame with specified columns converted to ordinal values.\n        \n        Raises:\n            KeyError: If a column value is missing from the mapping.\n        \"\"\"\n        pass\n\n    def apply_decimal_scaling(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Scales numeric columns using decimal scaling.\n        \n        Extended Explanation:\n            This method normalizes numeric data by shifting the decimal point. This is useful for adjusting the magnitude\n            of numbers without changing their distribution.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing numerical columns.\n            columns (list): List of columns to be scaled.\n        \n        Returns:\n            pd.DataFrame: DataFrame with scaled numeric columns.\n        \n        Raises:\n            ValueError: If columns contain non-numeric values.\n        \"\"\"\n        pass\n\n    def standardize_using_robust_scaling(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Standardizes data using techniques that are robust to outliers.\n        \n        Extended Explanation:\n            Rather than using mean and standard deviation, this method employs robust statistical measures\n            such as the median and IQR to reduce the influence of extreme values.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing columns to be standardized.\n            columns (list): List of column names to standardize.\n        \n        Returns:\n            pd.DataFrame: DataFrame with robust-scaled values.\n        \n        Raises:\n            ValueError: If provided columns contain non-numeric data.\n        \"\"\"\n        pass",
                                                            "lineno": 204
                                                        },
                                                        {
                                                            "name": "convert_to_integer",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "DataConversionScaler",
                                                            "extra": {},
                                                            "code": "def convert_to_integer(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Converts specified columns in a DataFrame to integer type.\n        \n        Extended Explanation:\n            This method is used to ensure that numeric data is stored as integers. It processes\n            the provided columns and converts any float or string representations of numbers to integers.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing the columns to convert.\n            columns (list): List of column names to be converted to integer type.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with the specified columns converted to integer.\n        \n        Raises:\n            TypeError: If conversion cannot be performed due to incompatible data types.\n        \"\"\"\n    pass",
                                                            "lineno": 212
                                                        },
                                                        {
                                                            "name": "convert_to_ordinal",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "DataConversionScaler",
                                                            "extra": {},
                                                            "code": "def convert_to_ordinal(self, df: pd.DataFrame, columns: list, mapping: dict) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Converts specified categorical columns to ordinal values.\n        \n        Extended Explanation:\n            This method applies a mapping dictionary to convert nominal or categorical data into ordinal form,\n            which can facilitate further numeric processing.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing categorical columns.\n            columns (list): List of column names to convert.\n            mapping (dict): Dictionary mapping original values to ordinal integers.\n        \n        Returns:\n            pd.DataFrame: DataFrame with specified columns converted to ordinal values.\n        \n        Raises:\n            KeyError: If a column value is missing from the mapping.\n        \"\"\"\n    pass",
                                                            "lineno": 233
                                                        },
                                                        {
                                                            "name": "apply_decimal_scaling",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "DataConversionScaler",
                                                            "extra": {},
                                                            "code": "def apply_decimal_scaling(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Scales numeric columns using decimal scaling.\n        \n        Extended Explanation:\n            This method normalizes numeric data by shifting the decimal point. This is useful for adjusting the magnitude\n            of numbers without changing their distribution.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing numerical columns.\n            columns (list): List of columns to be scaled.\n        \n        Returns:\n            pd.DataFrame: DataFrame with scaled numeric columns.\n        \n        Raises:\n            ValueError: If columns contain non-numeric values.\n        \"\"\"\n    pass",
                                                            "lineno": 255
                                                        },
                                                        {
                                                            "name": "standardize_using_robust_scaling",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "DataConversionScaler",
                                                            "extra": {},
                                                            "code": "def standardize_using_robust_scaling(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Standardizes data using techniques that are robust to outliers.\n        \n        Extended Explanation:\n            Rather than using mean and standard deviation, this method employs robust statistical measures\n            such as the median and IQR to reduce the influence of extreme values.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing columns to be standardized.\n            columns (list): List of column names to standardize.\n        \n        Returns:\n            pd.DataFrame: DataFrame with robust-scaled values.\n        \n        Raises:\n            ValueError: If provided columns contain non-numeric data.\n        \"\"\"\n    pass",
                                                            "lineno": 276
                                                        },
                                                        {
                                                            "name": "CategoricalCleaner",
                                                            "unit_type": "class",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class CategoricalCleaner:\n    \"\"\"\n    Provides methods for cleaning and standardizing categorical or textual data in a DataFrame.\n\n    This includes operations to standardize categorical formats, remove special characters using regex,\n    and fill missing or problematic entries with either custom values or zero.\n    \"\"\"\n\n    def standardize_categorical_data(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Standardizes the representation of categorical data in the DataFrame.\n        \n        Extended Explanation:\n            This method adjusts the string formats in the specified columns to a consistent standard, which\n            may involve trimming spaces, changing case, or other normalization procedures.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            columns (list): List of columns containing categorical data to standardize.\n        \n        Returns:\n            pd.DataFrame: DataFrame with standardized categorical data.\n        \n        Raises:\n            ValueError: If any of the specified columns are not of a string type.\n        \"\"\"\n        pass\n\n    def remove_special_characters(self, df: pd.DataFrame, columns: list, pattern: str='[^a-zA-Z0-9\\\\s]') -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Removes special characters from the specified string columns.\n        \n        Extended Explanation:\n            Using a regular expression pattern, this method cleans text data by removing characters that\n            do not match the allowed set defined by the pattern.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing textual data.\n            columns (list): List of column names to clean.\n            pattern (str): Regex pattern specifying allowed characters. Defaults to alphanumeric and whitespace.\n        \n        Returns:\n            pd.DataFrame: DataFrame with special characters removed from specified columns.\n        \n        Raises:\n            re.error: If the provided regex pattern is invalid.\n        \"\"\"\n        pass\n\n    def fill_with_custom_value(self, df: pd.DataFrame, columns: list, custom_value) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Fills missing or problematic entries in specified columns with a custom value.\n        \n        Extended Explanation:\n            This method replaces NaNs or erroneous entries within the given columns with a predetermined custom value.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame with missing or problematic entries.\n            columns (list): List of columns to fill.\n            custom_value: The value to use for filling missing entries.\n        \n        Returns:\n            pd.DataFrame: DataFrame with missing entries replaced by the custom value.\n        \n        Raises:\n            TypeError: If the custom_value is of the wrong type for the target column.\n        \"\"\"\n        pass\n\n    def fill_with_zero(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Fills missing or invalid values in designated columns with zero.\n        \n        Extended Explanation:\n            This method scans the specified columns for NaN or other placeholders for missing data and fills them with zero.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to process.\n            columns (list): List of column names where zeros will replace missing values.\n        \n        Returns:\n            pd.DataFrame: DataFrame with missing values filled with zero.\n        \n        Raises:\n            ValueError: If the operation fails due to type incompatibilities.\n        \"\"\"\n        pass",
                                                            "lineno": 297
                                                        },
                                                        {
                                                            "name": "standardize_categorical_data",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "CategoricalCleaner",
                                                            "extra": {},
                                                            "code": "def standardize_categorical_data(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Standardizes the representation of categorical data in the DataFrame.\n        \n        Extended Explanation:\n            This method adjusts the string formats in the specified columns to a consistent standard, which\n            may involve trimming spaces, changing case, or other normalization procedures.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n            columns (list): List of columns containing categorical data to standardize.\n        \n        Returns:\n            pd.DataFrame: DataFrame with standardized categorical data.\n        \n        Raises:\n            ValueError: If any of the specified columns are not of a string type.\n        \"\"\"\n    pass",
                                                            "lineno": 305
                                                        },
                                                        {
                                                            "name": "remove_special_characters",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "CategoricalCleaner",
                                                            "extra": {},
                                                            "code": "def remove_special_characters(self, df: pd.DataFrame, columns: list, pattern: str='[^a-zA-Z0-9\\\\s]') -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Removes special characters from the specified string columns.\n        \n        Extended Explanation:\n            Using a regular expression pattern, this method cleans text data by removing characters that\n            do not match the allowed set defined by the pattern.\n        \n        Args:\n            df (pd.DataFrame): DataFrame containing textual data.\n            columns (list): List of column names to clean.\n            pattern (str): Regex pattern specifying allowed characters. Defaults to alphanumeric and whitespace.\n        \n        Returns:\n            pd.DataFrame: DataFrame with special characters removed from specified columns.\n        \n        Raises:\n            re.error: If the provided regex pattern is invalid.\n        \"\"\"\n    pass",
                                                            "lineno": 326
                                                        },
                                                        {
                                                            "name": "fill_with_custom_value",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "CategoricalCleaner",
                                                            "extra": {},
                                                            "code": "def fill_with_custom_value(self, df: pd.DataFrame, columns: list, custom_value) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Fills missing or problematic entries in specified columns with a custom value.\n        \n        Extended Explanation:\n            This method replaces NaNs or erroneous entries within the given columns with a predetermined custom value.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame with missing or problematic entries.\n            columns (list): List of columns to fill.\n            custom_value: The value to use for filling missing entries.\n        \n        Returns:\n            pd.DataFrame: DataFrame with missing entries replaced by the custom value.\n        \n        Raises:\n            TypeError: If the custom_value is of the wrong type for the target column.\n        \"\"\"\n    pass",
                                                            "lineno": 348
                                                        },
                                                        {
                                                            "name": "fill_with_zero",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "CategoricalCleaner",
                                                            "extra": {},
                                                            "code": "def fill_with_zero(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Fills missing or invalid values in designated columns with zero.\n        \n        Extended Explanation:\n            This method scans the specified columns for NaN or other placeholders for missing data and fills them with zero.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to process.\n            columns (list): List of column names where zeros will replace missing values.\n        \n        Returns:\n            pd.DataFrame: DataFrame with missing values filled with zero.\n        \n        Raises:\n            ValueError: If the operation fails due to type incompatibilities.\n        \"\"\"\n    pass",
                                                            "lineno": 369
                                                        },
                                                        {
                                                            "name": "ConsistencyCorrector",
                                                            "unit_type": "class",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class ConsistencyCorrector:\n    \"\"\"\n    Provides methods for correcting inconsistencies and errors in data formatting.\n\n    This class includes functions that adjust data to fixed formats,\n    correct typographical errors, enforce consistency across entries, and resolve conflicts.\n    \"\"\"\n\n    def fix_inconsistent_data_formats(self, df: pd.DataFrame, columns: list, format_rules: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Standardizes data formats in specified columns based on provided rules.\n        \n        Extended Explanation:\n            This method applies a set of formatting rules to the designated columns to ensure the data\n            adheres to consistent standards.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to standardize.\n            columns (list): List of column names to adjust.\n            format_rules (dict): Mapping of column names to formatting rules.\n        \n        Returns:\n            pd.DataFrame: DataFrame with standardized formats.\n        \n        Raises:\n            KeyError: If a column is missing from the provided format_rules.\n        \"\"\"\n        pass\n\n    def correct_typos(self, df: pd.DataFrame, columns: list, typo_dict: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Corrects typographical errors in specified textual columns.\n        \n        Extended Explanation:\n            This method replaces common typos with their correct forms using a dictionary mapping.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            columns (list): List of columns to inspect for typos.\n            typo_dict (dict): Dictionary mapping incorrect spellings to correct ones.\n        \n        Returns:\n            pd.DataFrame: DataFrame with typos corrected.\n        \n        Raises:\n            ValueError: If typo_dict is empty or columns are missing.\n        \"\"\"\n        pass\n\n    def enforce_consistency_rules(self, df: pd.DataFrame, rules: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Enforces predefined consistency rules across the DataFrame.\n        \n        Extended Explanation:\n            This method applies a series of rules to ensure that data across columns adheres to\n            consistent formats and logical relationships.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to process.\n            rules (dict): Dictionary containing consistency rules to apply.\n        \n        Returns:\n            pd.DataFrame: DataFrame with consistency rules enforced.\n        \n        Raises:\n            ValueError: If rules are improperly defined.\n        \"\"\"\n        pass\n\n    def correct_data_entry_errors(self, df: pd.DataFrame, columns: list, error_corrections: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Corrects known data entry errors in specified columns.\n        \n        Extended Explanation:\n            This method uses an error correction mapping to replace incorrect entries due to data entry errors.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing errors.\n            columns (list): List of columns to correct.\n            error_corrections (dict): Dictionary mapping erroneous values to correct ones.\n        \n        Returns:\n            pd.DataFrame: DataFrame with data entry errors corrected.\n        \n        Raises:\n            KeyError: If an expected correction key is missing.\n        \"\"\"\n        pass\n\n    def resolve_conflicting_data(self, df: pd.DataFrame, conflict_resolution_rules: dict) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Resolves conflicting data entries using specified rules.\n        \n        Extended Explanation:\n            This method applies conflict resolution strategies to deal with data contradictions,\n            ensuring that the resulting DataFrame has coherent and consistent data.\n        \n        Args:\n            df (pd.DataFrame): DataFrame with conflicting entries.\n            conflict_resolution_rules (dict): Dictionary of rules to resolve conflicts.\n        \n        Returns:\n            pd.DataFrame: DataFrame with resolved conflicting data.\n        \n        Raises:\n            ValueError: If conflict_resolution_rules do not cover all conflicts.\n        \"\"\"\n        pass",
                                                            "lineno": 389
                                                        },
                                                        {
                                                            "name": "fix_inconsistent_data_formats",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "ConsistencyCorrector",
                                                            "extra": {},
                                                            "code": "def fix_inconsistent_data_formats(self, df: pd.DataFrame, columns: list, format_rules: dict) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Standardizes data formats in specified columns based on provided rules.\n        \n        Extended Explanation:\n            This method applies a set of formatting rules to the designated columns to ensure the data\n            adheres to consistent standards.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to standardize.\n            columns (list): List of column names to adjust.\n            format_rules (dict): Mapping of column names to formatting rules.\n        \n        Returns:\n            pd.DataFrame: DataFrame with standardized formats.\n        \n        Raises:\n            KeyError: If a column is missing from the provided format_rules.\n        \"\"\"\n    pass",
                                                            "lineno": 397
                                                        },
                                                        {
                                                            "name": "correct_typos",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "ConsistencyCorrector",
                                                            "extra": {},
                                                            "code": "def correct_typos(self, df: pd.DataFrame, columns: list, typo_dict: dict) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Corrects typographical errors in specified textual columns.\n        \n        Extended Explanation:\n            This method replaces common typos with their correct forms using a dictionary mapping.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame.\n            columns (list): List of columns to inspect for typos.\n            typo_dict (dict): Dictionary mapping incorrect spellings to correct ones.\n        \n        Returns:\n            pd.DataFrame: DataFrame with typos corrected.\n        \n        Raises:\n            ValueError: If typo_dict is empty or columns are missing.\n        \"\"\"\n    pass",
                                                            "lineno": 419
                                                        },
                                                        {
                                                            "name": "enforce_consistency_rules",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "ConsistencyCorrector",
                                                            "extra": {},
                                                            "code": "def enforce_consistency_rules(self, df: pd.DataFrame, rules: dict) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Enforces predefined consistency rules across the DataFrame.\n        \n        Extended Explanation:\n            This method applies a series of rules to ensure that data across columns adheres to\n            consistent formats and logical relationships.\n        \n        Args:\n            df (pd.DataFrame): DataFrame to process.\n            rules (dict): Dictionary containing consistency rules to apply.\n        \n        Returns:\n            pd.DataFrame: DataFrame with consistency rules enforced.\n        \n        Raises:\n            ValueError: If rules are improperly defined.\n        \"\"\"\n    pass",
                                                            "lineno": 440
                                                        },
                                                        {
                                                            "name": "correct_data_entry_errors",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "ConsistencyCorrector",
                                                            "extra": {},
                                                            "code": "def correct_data_entry_errors(self, df: pd.DataFrame, columns: list, error_corrections: dict) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Corrects known data entry errors in specified columns.\n        \n        Extended Explanation:\n            This method uses an error correction mapping to replace incorrect entries due to data entry errors.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing errors.\n            columns (list): List of columns to correct.\n            error_corrections (dict): Dictionary mapping erroneous values to correct ones.\n        \n        Returns:\n            pd.DataFrame: DataFrame with data entry errors corrected.\n        \n        Raises:\n            KeyError: If an expected correction key is missing.\n        \"\"\"\n    pass",
                                                            "lineno": 461
                                                        },
                                                        {
                                                            "name": "resolve_conflicting_data",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "ConsistencyCorrector",
                                                            "extra": {},
                                                            "code": "def resolve_conflicting_data(self, df: pd.DataFrame, conflict_resolution_rules: dict) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Resolves conflicting data entries using specified rules.\n        \n        Extended Explanation:\n            This method applies conflict resolution strategies to deal with data contradictions,\n            ensuring that the resulting DataFrame has coherent and consistent data.\n        \n        Args:\n            df (pd.DataFrame): DataFrame with conflicting entries.\n            conflict_resolution_rules (dict): Dictionary of rules to resolve conflicts.\n        \n        Returns:\n            pd.DataFrame: DataFrame with resolved conflicting data.\n        \n        Raises:\n            ValueError: If conflict_resolution_rules do not cover all conflicts.\n        \"\"\"\n    pass",
                                                            "lineno": 482
                                                        },
                                                        {
                                                            "name": "ZeroColumnHandler",
                                                            "unit_type": "class",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class ZeroColumnHandler:\n    \"\"\"\n    Provides methods for identifying and handling columns composed entirely or predominantly of zeros.\n\n    This class includes methods to detect columns with a high proportion of zeroes and to remove columns which contain \n    only zero values.\n    \"\"\"\n\n    def identify_zero_columns(self, df: pd.DataFrame, threshold: float=0.0) -> list:\n        \"\"\"\n        Summary:\n            Identifies columns in the DataFrame where the proportion of zero values meets or exceeds the threshold.\n        \n        Extended Explanation:\n            This method evaluates each column in the DataFrame and returns a list of column names that have a zero-value\n            proportion greater than or equal to the provided threshold.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to inspect.\n            threshold (float, optional): The minimum proportion of zeros required to classify a column as zero-dominated.\n                                         Defaults to 0.0 (all zeros).\n        \n        Returns:\n            list: A list of column names that meet the zero column criteria.\n        \n        Raises:\n            ValueError: If the threshold is not between 0 and 1.\n        \"\"\"\n        pass\n\n    def drop_columns_with_all_zeros(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Summary:\n            Drops columns from the DataFrame that contain only zeros.\n        \n        Extended Explanation:\n            This method scans each column and removes those where every element is zero. This is useful for cleaning\n            datasets that have redundant or non-informative zero-only columns.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with columns containing only zeros removed.\n        \n        Raises:\n            ValueError: If no columns can be dropped due to incompatible data types.\n        \"\"\"\n        pass",
                                                            "lineno": 503
                                                        },
                                                        {
                                                            "name": "identify_zero_columns",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "ZeroColumnHandler",
                                                            "extra": {},
                                                            "code": "def identify_zero_columns(self, df: pd.DataFrame, threshold: float=0.0) -> list:\n    \"\"\"\n        Summary:\n            Identifies columns in the DataFrame where the proportion of zero values meets or exceeds the threshold.\n        \n        Extended Explanation:\n            This method evaluates each column in the DataFrame and returns a list of column names that have a zero-value\n            proportion greater than or equal to the provided threshold.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to inspect.\n            threshold (float, optional): The minimum proportion of zeros required to classify a column as zero-dominated.\n                                         Defaults to 0.0 (all zeros).\n        \n        Returns:\n            list: A list of column names that meet the zero column criteria.\n        \n        Raises:\n            ValueError: If the threshold is not between 0 and 1.\n        \"\"\"\n    pass",
                                                            "lineno": 511
                                                        },
                                                        {
                                                            "name": "drop_columns_with_all_zeros",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": "ZeroColumnHandler",
                                                            "extra": {},
                                                            "code": "def drop_columns_with_all_zeros(self, df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Summary:\n            Drops columns from the DataFrame that contain only zeros.\n        \n        Extended Explanation:\n            This method scans each column and removes those where every element is zero. This is useful for cleaning\n            datasets that have redundant or non-informative zero-only columns.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame to process.\n        \n        Returns:\n            pd.DataFrame: A DataFrame with columns containing only zeros removed.\n        \n        Raises:\n            ValueError: If no columns can be dropped due to incompatible data types.\n        \"\"\"\n    pass",
                                                            "lineno": 533
                                                        },
                                                        {
                                                            "name": "apply_active_learning_strategy",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def apply_active_learning_strategy(df: 'pd.DataFrame', strategy_params: dict) -> 'pd.DataFrame':\n    \"\"\"\n    Summary:\n        Applies an active learning strategy to modify the DataFrame for sample selection or re-weighting.\n    \n    Extended Explanation:\n        This function integrates active learning techniques into the data cleaning process. It processes the provided DataFrame \n        using parameters that define how samples should be selected or weighted. The resulting DataFrame is modified to better \n        suit active learning workflows.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame on which to apply the active learning strategy.\n        strategy_params (dict): A dictionary containing parameters for the active learning strategy, such as sampling \n                                criteria or re-weighting factors.\n    \n    Returns:\n        pd.DataFrame: A modified DataFrame adjusted according to the active learning strategy.\n    \n    Raises:\n        ValueError: If strategy_params is missing required keys or contains invalid values.\n    \"\"\"\n    pass",
                                                            "lineno": 553
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "imputation",
                                            "path": "./src/data_engineering/data_preparation/imputation",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "imputation_methods.py",
                                                    "path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                    "code": "import pandas as pd\n\ndef group_based_imputation(df: pd.DataFrame, group_col: str, target_col: str, strategy: str='mean') -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column by computing grouped statistics.\n    \n    This function imputes missing values in a specified target column within each group defined by 'group_col'.\n    The imputation strategy can be set to either 'mean' or 'median' to perform the corresponding imputation.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        group_col (str): Column name to group the data by.\n        target_col (str): The target column in which missing values are to be imputed.\n        strategy (str): The imputation strategy. Expected values are \"mean\" or \"median\". Defaults to \"mean\".\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the group-based strategy.\n        \n    Edge Cases:\n        - If the group or target columns do not exist, processing should be handled upstream.\n        - Assumes that the groups are non-empty and the target column is numeric.\n    \"\"\"\n    pass\n\ndef simple_imputation(df: pd.DataFrame, target_col: str, fill_value) -> pd.DataFrame:\n    \"\"\"\n    Perform a simple imputation on the target column using a constant fill value.\n    \n    This function replaces missing values in a specified column with a provided constant.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The name of the column in which to impute missing data.\n        fill_value: The constant value to use for imputing missing entries.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column replaced by the fill_value.\n        \n    Assumptions:\n        - The fill_value is appropriate for the data type of the target column.\n    \"\"\"\n    pass\n\ndef identify_missing(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Identify missing values in the DataFrame and return a DataFrame indicating the positions of missing data.\n    \n    This function scans the entire DataFrame to detect locations (rows and columns) with missing data.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame to be analyzed for missing values.\n        \n    Returns:\n        pd.DataFrame: A boolean DataFrame of the same shape as input, where True indicates a missing value.\n        \n    Edge Cases:\n        - An empty DataFrame is returned if no data is present.\n    \"\"\"\n    pass\n\ndef weighted_mean_imputation(df: pd.DataFrame, target_col: str, weight_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using a weighted mean computed from a specified weight column.\n    \n    The weighted mean is calculated considering the weights provided, and missing values in the target column\n    are replaced with the computed weighted mean.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The column in which to impute missing values.\n        weight_col (str): The column containing weights for calculating the weighted mean.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the weighted mean.\n        \n    Assumptions:\n        - Both target_col and weight_col exist in the DataFrame and contain numeric data.\n    \"\"\"\n    pass\n\ndef delete_columns(df: pd.DataFrame, missing_threshold: float=0.5) -> pd.DataFrame:\n    \"\"\"\n    Delete columns from the DataFrame based on a missing data threshold.\n    \n    This function drops columns where the proportion of missing values exceeds the specified threshold.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with potential missing data.\n        missing_threshold (float): The maximum allowed fraction of missing data for a column to be retained.\n                                  Columns with a higher fraction of missing data will be deleted. Defaults to 0.5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with columns removed if they exceeded the missing data threshold.\n        \n    Edge Cases:\n        - If missing_threshold is set to a value outside [0,1], behavior is undefined.\n    \"\"\"\n    pass\n\ndef local_median_imputation(df: pd.DataFrame, target_col: str, neighborhood: int=5) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column by computing the median over a local neighborhood.\n    \n    This function replaces missing values with the median computed from a specified number of neighboring entries.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        target_col (str): The column in which missing data is to be imputed.\n        neighborhood (int): The number of adjacent data points to consider for computing the local median. Defaults to 5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using a local median.\n        \n    Assumptions:\n        - The DataFrame is sorted appropriately such that a local neighborhood is meaningful.\n    \"\"\"\n    pass\n\ndef global_median_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using the global median.\n    \n    This function computes the median of the entire column (ignoring missing entries) and replaces missing values with it.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing data.\n        target_col (str): The name of the column where missing values will be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed with its global median.\n        \n    Constraints:\n        - Assumes that the target column contains numeric data.\n    \"\"\"\n    pass\n\ndef conditional_imputation(df: pd.DataFrame, target_col: str, condition: dict, strategy: str='mean') -> pd.DataFrame:\n    \"\"\"\n    Perform conditional imputation on the target column based on provided conditions.\n    \n    Depending on the chosen strategy, either the mean or median is computed on subsets of data that satisfy\n    the condition specified by a dictionary of column-value pairs. Missing values are imputed accordingly.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        target_col (str): The column in which missing values will be imputed.\n        condition (dict): A dictionary where keys are column names and values define the condition to filter the DataFrame.\n        strategy (str): The imputation strategy. Expected values are \"mean\" or \"median\". Defaults to \"mean\".\n        \n    Returns:\n        pd.DataFrame: A DataFrame with conditionally imputed values in the target column.\n        \n    Assumptions:\n        - The condition provided effectively segments the DataFrame into meaningful subsets.\n    \"\"\"\n    pass\n\ndef pairwise_deletion(df: pd.DataFrame, target_cols: list) -> pd.DataFrame:\n    \"\"\"\n    Apply pairwise deletion to handle missing data across specified columns.\n    \n    This function removes rows based on the presence of missing values in a pairwise manner from the specified columns.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing data.\n        target_cols (list): List of column names on which the pairwise deletion should be based.\n        \n    Returns:\n        pd.DataFrame: A DataFrame where rows with missing data in specified column pairs are removed.\n        \n    Edge Cases:\n        - If target_cols is empty, the function will return the original DataFrame.\n    \"\"\"\n    pass\n\ndef binary_mode_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing binary data in the target column using the mode.\n    \n    This function determines the most frequent binary value in the target column and uses it to fill missing entries.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing binary data.\n        target_col (str): The name of the binary column to be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the binary target column imputed with its mode.\n        \n    Constraints:\n        - Assumes that the target column only contains two distinct values aside from missing values.\n    \"\"\"\n    pass\n\ndef polynomial_interpolation_imputation(df: pd.DataFrame, target_col: str, degree: int=2) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using polynomial interpolation.\n    \n    This function fits a polynomial of a specified degree to the non-missing data and uses it to estimate missing values.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with gaps in the target column.\n        target_col (str): The column to interpolate.\n        degree (int): The degree of the polynomial to be used for interpolation. Defaults to 2.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed by polynomial interpolation.\n        \n    Remarks:\n        - Suitable for time series or sequential data where polynomial trends are expected.\n    \"\"\"\n    pass\n\ndef conditional_mode_imputation(df: pd.DataFrame, target_col: str, condition: dict) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column conditionally using the mode.\n    \n    The function segments the DataFrame based on given conditions and fills missing values with the most frequent value\n    within each segment.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing data.\n        target_col (str): The column to be imputed.\n        condition (dict): A dictionary specifying column names and their desired values to filter the DataFrame.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values conditionally imputed using the mode.\n        \n    Edge Cases:\n        - If the condition does not segregate the data, the global mode might be used.\n    \"\"\"\n    pass\n\ndef optimized_mode_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Perform mode imputation on large datasets with optimization considerations.\n    \n    This function imputes missing values in the target column by computing the mode in a manner optimized for large datasets,\n    potentially using chunk processing or memory-efficient algorithms.\n    \n    Args:\n        df (pd.DataFrame): The large input DataFrame with missing values.\n        target_col (str): The column on which to perform mode imputation.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values imputed using an optimized mode computation.\n        \n    Considerations:\n        - Designed for performance and memory efficiency on very large DataFrames.\n    \"\"\"\n    pass\n\ndef linear_interpolation_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using linear interpolation.\n    \n    The function linearly interpolates missing values based on surrounding data points in the target column.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The name of the column for linear interpolation.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed via linear interpolation.\n        \n    Constraints:\n        - Assumes the data is ordered so that linear interpolation is meaningful.\n    \"\"\"\n    pass\n\ndef mode_imputation(df: pd.DataFrame, target_col: str, subset: list=None) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using mode imputation.\n    \n    This function computes the mode either for the entire column or for a specified subset of data\n    (if 'subset' is provided) and fills missing values with the most frequent value.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing data.\n        target_col (str): The column in which to perform mode imputation.\n        subset (list, optional): A list of column names to consider when computing the mode. If None, the mode is\n                                 computed using the entire column. Defaults to None.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the mode.\n        \n    Assumptions:\n        - The target column contains categorical or numerical data for which mode is an appropriate statistic.\n    \"\"\"\n    pass\n\ndef global_mean_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using the global mean.\n    \n    This function computes the mean of the target column over the entire DataFrame (ignoring missing data)\n    and fills missing values with this mean.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The column to be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the global mean.\n        \n    Constraints:\n        - Assumes that the target column contains numeric data.\n    \"\"\"\n    pass\n\ndef flag_outliers(df: pd.DataFrame, target_col: str, method: str='IQR', factor: float=1.5) -> pd.DataFrame:\n    \"\"\"\n    Flag outlier values in the target column using a specified method.\n    \n    This function marks outliers in the DataFrame based on the specified statistical method (e.g., IQR).\n    The flagged outliers can be used for further inspection or processing.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        target_col (str): The column in which to flag outliers.\n        method (str): The method to detect outliers (e.g., \"IQR\"). Defaults to \"IQR\".\n        factor (float): The multiplier applied in the outlier detection method. Defaults to 1.5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with an additional boolean column (or modified target column) indicating outlier status.\n        \n    Remarks:\n        - The implementation should clearly document how outliers are flagged.\n    \"\"\"\n    pass\n\ndef flag_missing_rows(df: pd.DataFrame, threshold: float=0.2) -> pd.DataFrame:\n    \"\"\"\n    Flag rows with a high proportion of missing values.\n    \n    This function adds a boolean indicator to each row in the DataFrame informing whether the row has \n    missing data exceeding a specified threshold.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        threshold (float): The fraction of missing values in a row required to flag it as problematic. Defaults to 0.2.\n        \n    Returns:\n        pd.DataFrame: The original DataFrame augmented with an additional column indicating rows with excessive missing values.\n        \n    Constraints:\n        - Assumes the threshold is a value between 0 and 1.\n    \"\"\"\n    pass\n\ndef threshold_based_removal(df: pd.DataFrame, target_col: str, threshold: float) -> pd.DataFrame:\n    \"\"\"\n    Remove entries from the target column if their value exceeds a specified threshold.\n    \n    This function deletes rows or marks values for removal if they do not meet the threshold criterion.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        target_col (str): The column to check against the threshold.\n        threshold (float): The threshold value that determines if an entry should be removed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with rows removed or flagged where the target column exceeds the threshold.\n        \n    Edge Cases:\n        - Behavior when no rows meet the threshold should be clearly documented.\n    \"\"\"\n    pass\n\ndef categorical_imputation(df: pd.DataFrame, target_col: str, strategy: str='mode') -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in categorical columns using a specified strategy.\n    \n    This function imputes missing values for categorical data. By default, it uses the mode, but other strategies\n    can be specified if needed.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing categorical data.\n        target_col (str): The categorical column to impute.\n        strategy (str): The imputation strategy to use (e.g., \"mode\", \"constant\"). Defaults to \"mode\".\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the categorical column imputed.\n        \n    Assumptions:\n        - The target column contains categorical data.\n    \"\"\"\n    pass\n\ndef model_based_imputation(df: pd.DataFrame, target_col: str, model, features: list) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values using a predictive model.\n    \n    This function leverages a given model to predict missing values in the target column based on other related features.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The column where imputation is required.\n        model: A predictive model instance that implements fit/predict.\n        features (list): A list of column names to be used as predictors for the model.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with the target column imputed using model predictions.\n        \n    Assumptions:\n        - The provided model follows a standard interface (fit and predict methods).\n    \"\"\"\n    pass\n\ndef column_median_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using the median computed for that specific column.\n    \n    This function calculates the median value of the target column (ignoring missing values)\n    and fills missing entries with that median.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing data.\n        target_col (str): The name of the column to be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column filled using its median.\n        \n    Constraints:\n        - The target column is assumed to be numeric.\n    \"\"\"\n    pass\n\ndef delete_incomplete_records(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Delete records (rows) from the DataFrame that contain any missing values.\n    \n    This function removes all rows where any column has a missing entry.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with all rows containing missing values removed.\n        \n    Edge Cases:\n        - If the DataFrame is entirely missing values or empty, the result should be an empty DataFrame.\n    \"\"\"\n    pass\n\ndef drop_columns_based_on_percentage(df: pd.DataFrame, percentage: float) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from the DataFrame where the percentage of missing values exceeds a specified limit.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        percentage (float): The maximum allowed percentage (between 0 and 100) of missing values in a column.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with columns dropped if their missing data percentage exceeds the specified limit.\n        \n    Constraints:\n        - The value of percentage should be within the range 0 to 100.\n    \"\"\"\n    pass\n\ndef knn_imputation(df: pd.DataFrame, target_col: str, n_neighbors: int=5) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in a target column using the K-Nearest Neighbors algorithm.\n    \n    This function identifies the k-nearest rows with non-missing values in the target column and imputes\n    missing values by aggregating these neighbors (e.g., by taking their mean).\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        target_col (str): The column in which to impute missing values.\n        n_neighbors (int): The number of neighbors to consider for imputation. Defaults to 5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using KNN.\n        \n    Assumptions:\n        - The DataFrame is appropriately scaled so that the distance metric in KNN is meaningful.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "group_based_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def group_based_imputation(df: pd.DataFrame, group_col: str, target_col: str, strategy: str='mean') -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column by computing grouped statistics.\n    \n    This function imputes missing values in a specified target column within each group defined by 'group_col'.\n    The imputation strategy can be set to either 'mean' or 'median' to perform the corresponding imputation.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        group_col (str): Column name to group the data by.\n        target_col (str): The target column in which missing values are to be imputed.\n        strategy (str): The imputation strategy. Expected values are \"mean\" or \"median\". Defaults to \"mean\".\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the group-based strategy.\n        \n    Edge Cases:\n        - If the group or target columns do not exist, processing should be handled upstream.\n        - Assumes that the groups are non-empty and the target column is numeric.\n    \"\"\"\n    pass",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "simple_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def simple_imputation(df: pd.DataFrame, target_col: str, fill_value) -> pd.DataFrame:\n    \"\"\"\n    Perform a simple imputation on the target column using a constant fill value.\n    \n    This function replaces missing values in a specified column with a provided constant.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The name of the column in which to impute missing data.\n        fill_value: The constant value to use for imputing missing entries.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column replaced by the fill_value.\n        \n    Assumptions:\n        - The fill_value is appropriate for the data type of the target column.\n    \"\"\"\n    pass",
                                                            "lineno": 25
                                                        },
                                                        {
                                                            "name": "identify_missing",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def identify_missing(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Identify missing values in the DataFrame and return a DataFrame indicating the positions of missing data.\n    \n    This function scans the entire DataFrame to detect locations (rows and columns) with missing data.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame to be analyzed for missing values.\n        \n    Returns:\n        pd.DataFrame: A boolean DataFrame of the same shape as input, where True indicates a missing value.\n        \n    Edge Cases:\n        - An empty DataFrame is returned if no data is present.\n    \"\"\"\n    pass",
                                                            "lineno": 44
                                                        },
                                                        {
                                                            "name": "weighted_mean_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def weighted_mean_imputation(df: pd.DataFrame, target_col: str, weight_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using a weighted mean computed from a specified weight column.\n    \n    The weighted mean is calculated considering the weights provided, and missing values in the target column\n    are replaced with the computed weighted mean.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The column in which to impute missing values.\n        weight_col (str): The column containing weights for calculating the weighted mean.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the weighted mean.\n        \n    Assumptions:\n        - Both target_col and weight_col exist in the DataFrame and contain numeric data.\n    \"\"\"\n    pass",
                                                            "lineno": 61
                                                        },
                                                        {
                                                            "name": "delete_columns",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def delete_columns(df: pd.DataFrame, missing_threshold: float=0.5) -> pd.DataFrame:\n    \"\"\"\n    Delete columns from the DataFrame based on a missing data threshold.\n    \n    This function drops columns where the proportion of missing values exceeds the specified threshold.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with potential missing data.\n        missing_threshold (float): The maximum allowed fraction of missing data for a column to be retained.\n                                  Columns with a higher fraction of missing data will be deleted. Defaults to 0.5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with columns removed if they exceeded the missing data threshold.\n        \n    Edge Cases:\n        - If missing_threshold is set to a value outside [0,1], behavior is undefined.\n    \"\"\"\n    pass",
                                                            "lineno": 81
                                                        },
                                                        {
                                                            "name": "local_median_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def local_median_imputation(df: pd.DataFrame, target_col: str, neighborhood: int=5) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column by computing the median over a local neighborhood.\n    \n    This function replaces missing values with the median computed from a specified number of neighboring entries.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        target_col (str): The column in which missing data is to be imputed.\n        neighborhood (int): The number of adjacent data points to consider for computing the local median. Defaults to 5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using a local median.\n        \n    Assumptions:\n        - The DataFrame is sorted appropriately such that a local neighborhood is meaningful.\n    \"\"\"\n    pass",
                                                            "lineno": 100
                                                        },
                                                        {
                                                            "name": "global_median_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def global_median_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using the global median.\n    \n    This function computes the median of the entire column (ignoring missing entries) and replaces missing values with it.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing data.\n        target_col (str): The name of the column where missing values will be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed with its global median.\n        \n    Constraints:\n        - Assumes that the target column contains numeric data.\n    \"\"\"\n    pass",
                                                            "lineno": 119
                                                        },
                                                        {
                                                            "name": "conditional_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def conditional_imputation(df: pd.DataFrame, target_col: str, condition: dict, strategy: str='mean') -> pd.DataFrame:\n    \"\"\"\n    Perform conditional imputation on the target column based on provided conditions.\n    \n    Depending on the chosen strategy, either the mean or median is computed on subsets of data that satisfy\n    the condition specified by a dictionary of column-value pairs. Missing values are imputed accordingly.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        target_col (str): The column in which missing values will be imputed.\n        condition (dict): A dictionary where keys are column names and values define the condition to filter the DataFrame.\n        strategy (str): The imputation strategy. Expected values are \"mean\" or \"median\". Defaults to \"mean\".\n        \n    Returns:\n        pd.DataFrame: A DataFrame with conditionally imputed values in the target column.\n        \n    Assumptions:\n        - The condition provided effectively segments the DataFrame into meaningful subsets.\n    \"\"\"\n    pass",
                                                            "lineno": 137
                                                        },
                                                        {
                                                            "name": "pairwise_deletion",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def pairwise_deletion(df: pd.DataFrame, target_cols: list) -> pd.DataFrame:\n    \"\"\"\n    Apply pairwise deletion to handle missing data across specified columns.\n    \n    This function removes rows based on the presence of missing values in a pairwise manner from the specified columns.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing data.\n        target_cols (list): List of column names on which the pairwise deletion should be based.\n        \n    Returns:\n        pd.DataFrame: A DataFrame where rows with missing data in specified column pairs are removed.\n        \n    Edge Cases:\n        - If target_cols is empty, the function will return the original DataFrame.\n    \"\"\"\n    pass",
                                                            "lineno": 158
                                                        },
                                                        {
                                                            "name": "binary_mode_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def binary_mode_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing binary data in the target column using the mode.\n    \n    This function determines the most frequent binary value in the target column and uses it to fill missing entries.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing binary data.\n        target_col (str): The name of the binary column to be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the binary target column imputed with its mode.\n        \n    Constraints:\n        - Assumes that the target column only contains two distinct values aside from missing values.\n    \"\"\"\n    pass",
                                                            "lineno": 176
                                                        },
                                                        {
                                                            "name": "polynomial_interpolation_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def polynomial_interpolation_imputation(df: pd.DataFrame, target_col: str, degree: int=2) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using polynomial interpolation.\n    \n    This function fits a polynomial of a specified degree to the non-missing data and uses it to estimate missing values.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with gaps in the target column.\n        target_col (str): The column to interpolate.\n        degree (int): The degree of the polynomial to be used for interpolation. Defaults to 2.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed by polynomial interpolation.\n        \n    Remarks:\n        - Suitable for time series or sequential data where polynomial trends are expected.\n    \"\"\"\n    pass",
                                                            "lineno": 194
                                                        },
                                                        {
                                                            "name": "conditional_mode_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def conditional_mode_imputation(df: pd.DataFrame, target_col: str, condition: dict) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column conditionally using the mode.\n    \n    The function segments the DataFrame based on given conditions and fills missing values with the most frequent value\n    within each segment.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing data.\n        target_col (str): The column to be imputed.\n        condition (dict): A dictionary specifying column names and their desired values to filter the DataFrame.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values conditionally imputed using the mode.\n        \n    Edge Cases:\n        - If the condition does not segregate the data, the global mode might be used.\n    \"\"\"\n    pass",
                                                            "lineno": 213
                                                        },
                                                        {
                                                            "name": "optimized_mode_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def optimized_mode_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Perform mode imputation on large datasets with optimization considerations.\n    \n    This function imputes missing values in the target column by computing the mode in a manner optimized for large datasets,\n    potentially using chunk processing or memory-efficient algorithms.\n    \n    Args:\n        df (pd.DataFrame): The large input DataFrame with missing values.\n        target_col (str): The column on which to perform mode imputation.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values imputed using an optimized mode computation.\n        \n    Considerations:\n        - Designed for performance and memory efficiency on very large DataFrames.\n    \"\"\"\n    pass",
                                                            "lineno": 233
                                                        },
                                                        {
                                                            "name": "linear_interpolation_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def linear_interpolation_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using linear interpolation.\n    \n    The function linearly interpolates missing values based on surrounding data points in the target column.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The name of the column for linear interpolation.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed via linear interpolation.\n        \n    Constraints:\n        - Assumes the data is ordered so that linear interpolation is meaningful.\n    \"\"\"\n    pass",
                                                            "lineno": 252
                                                        },
                                                        {
                                                            "name": "mode_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def mode_imputation(df: pd.DataFrame, target_col: str, subset: list=None) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using mode imputation.\n    \n    This function computes the mode either for the entire column or for a specified subset of data\n    (if 'subset' is provided) and fills missing values with the most frequent value.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing data.\n        target_col (str): The column in which to perform mode imputation.\n        subset (list, optional): A list of column names to consider when computing the mode. If None, the mode is\n                                 computed using the entire column. Defaults to None.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the mode.\n        \n    Assumptions:\n        - The target column contains categorical or numerical data for which mode is an appropriate statistic.\n    \"\"\"\n    pass",
                                                            "lineno": 270
                                                        },
                                                        {
                                                            "name": "global_mean_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def global_mean_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using the global mean.\n    \n    This function computes the mean of the target column over the entire DataFrame (ignoring missing data)\n    and fills missing values with this mean.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The column to be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using the global mean.\n        \n    Constraints:\n        - Assumes that the target column contains numeric data.\n    \"\"\"\n    pass",
                                                            "lineno": 291
                                                        },
                                                        {
                                                            "name": "flag_outliers",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def flag_outliers(df: pd.DataFrame, target_col: str, method: str='IQR', factor: float=1.5) -> pd.DataFrame:\n    \"\"\"\n    Flag outlier values in the target column using a specified method.\n    \n    This function marks outliers in the DataFrame based on the specified statistical method (e.g., IQR).\n    The flagged outliers can be used for further inspection or processing.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        target_col (str): The column in which to flag outliers.\n        method (str): The method to detect outliers (e.g., \"IQR\"). Defaults to \"IQR\".\n        factor (float): The multiplier applied in the outlier detection method. Defaults to 1.5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with an additional boolean column (or modified target column) indicating outlier status.\n        \n    Remarks:\n        - The implementation should clearly document how outliers are flagged.\n    \"\"\"\n    pass",
                                                            "lineno": 310
                                                        },
                                                        {
                                                            "name": "flag_missing_rows",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def flag_missing_rows(df: pd.DataFrame, threshold: float=0.2) -> pd.DataFrame:\n    \"\"\"\n    Flag rows with a high proportion of missing values.\n    \n    This function adds a boolean indicator to each row in the DataFrame informing whether the row has \n    missing data exceeding a specified threshold.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        threshold (float): The fraction of missing values in a row required to flag it as problematic. Defaults to 0.2.\n        \n    Returns:\n        pd.DataFrame: The original DataFrame augmented with an additional column indicating rows with excessive missing values.\n        \n    Constraints:\n        - Assumes the threshold is a value between 0 and 1.\n    \"\"\"\n    pass",
                                                            "lineno": 331
                                                        },
                                                        {
                                                            "name": "threshold_based_removal",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def threshold_based_removal(df: pd.DataFrame, target_col: str, threshold: float) -> pd.DataFrame:\n    \"\"\"\n    Remove entries from the target column if their value exceeds a specified threshold.\n    \n    This function deletes rows or marks values for removal if they do not meet the threshold criterion.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        target_col (str): The column to check against the threshold.\n        threshold (float): The threshold value that determines if an entry should be removed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with rows removed or flagged where the target column exceeds the threshold.\n        \n    Edge Cases:\n        - Behavior when no rows meet the threshold should be clearly documented.\n    \"\"\"\n    pass",
                                                            "lineno": 350
                                                        },
                                                        {
                                                            "name": "categorical_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def categorical_imputation(df: pd.DataFrame, target_col: str, strategy: str='mode') -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in categorical columns using a specified strategy.\n    \n    This function imputes missing values for categorical data. By default, it uses the mode, but other strategies\n    can be specified if needed.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing categorical data.\n        target_col (str): The categorical column to impute.\n        strategy (str): The imputation strategy to use (e.g., \"mode\", \"constant\"). Defaults to \"mode\".\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the categorical column imputed.\n        \n    Assumptions:\n        - The target column contains categorical data.\n    \"\"\"\n    pass",
                                                            "lineno": 369
                                                        },
                                                        {
                                                            "name": "model_based_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def model_based_imputation(df: pd.DataFrame, target_col: str, model, features: list) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values using a predictive model.\n    \n    This function leverages a given model to predict missing values in the target column based on other related features.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing missing values.\n        target_col (str): The column where imputation is required.\n        model: A predictive model instance that implements fit/predict.\n        features (list): A list of column names to be used as predictors for the model.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with the target column imputed using model predictions.\n        \n    Assumptions:\n        - The provided model follows a standard interface (fit and predict methods).\n    \"\"\"\n    pass",
                                                            "lineno": 389
                                                        },
                                                        {
                                                            "name": "column_median_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def column_median_imputation(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in the target column using the median computed for that specific column.\n    \n    This function calculates the median value of the target column (ignoring missing values)\n    and fills missing entries with that median.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing data.\n        target_col (str): The name of the column to be imputed.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column filled using its median.\n        \n    Constraints:\n        - The target column is assumed to be numeric.\n    \"\"\"\n    pass",
                                                            "lineno": 409
                                                        },
                                                        {
                                                            "name": "delete_incomplete_records",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def delete_incomplete_records(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Delete records (rows) from the DataFrame that contain any missing values.\n    \n    This function removes all rows where any column has a missing entry.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with all rows containing missing values removed.\n        \n    Edge Cases:\n        - If the DataFrame is entirely missing values or empty, the result should be an empty DataFrame.\n    \"\"\"\n    pass",
                                                            "lineno": 428
                                                        },
                                                        {
                                                            "name": "drop_columns_based_on_percentage",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def drop_columns_based_on_percentage(df: pd.DataFrame, percentage: float) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from the DataFrame where the percentage of missing values exceeds a specified limit.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        percentage (float): The maximum allowed percentage (between 0 and 100) of missing values in a column.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with columns dropped if their missing data percentage exceeds the specified limit.\n        \n    Constraints:\n        - The value of percentage should be within the range 0 to 100.\n    \"\"\"\n    pass",
                                                            "lineno": 445
                                                        },
                                                        {
                                                            "name": "knn_imputation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/imputation/imputation_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def knn_imputation(df: pd.DataFrame, target_col: str, n_neighbors: int=5) -> pd.DataFrame:\n    \"\"\"\n    Impute missing values in a target column using the K-Nearest Neighbors algorithm.\n    \n    This function identifies the k-nearest rows with non-missing values in the target column and imputes\n    missing values by aggregating these neighbors (e.g., by taking their mean).\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with missing values.\n        target_col (str): The column in which to impute missing values.\n        n_neighbors (int): The number of neighbors to consider for imputation. Defaults to 5.\n        \n    Returns:\n        pd.DataFrame: A DataFrame with missing values in the target column imputed using KNN.\n        \n    Assumptions:\n        - The DataFrame is appropriately scaled so that the distance metric in KNN is meaningful.\n    \"\"\"\n    pass",
                                                            "lineno": 461
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "feature_engineering",
                                            "path": "./src/data_engineering/data_preparation/feature_engineering",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "combine_features.py",
                                                    "path": "./src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                                                    "code": "from typing import List, Dict, Callable, Optional\nimport pandas as pd\nfrom typing import List, Union\nfrom typing import List\n\ndef combine_numerical_features(numerical_dfs: List[pd.DataFrame]) -> pd.DataFrame:\n    \"\"\"\n    Combine multiple numerical feature DataFrames into a single DataFrame containing numerical features.\n    \n    This function takes a list of DataFrames that each contain numerical feature columns and merges them\n    by aligning their indices. It is designed to consolidate numerical features into one DataFrame for\n    subsequent modeling steps.\n    \n    Args:\n        numerical_dfs (List[pd.DataFrame]): A list of DataFrames, each containing numerical feature columns.\n    \n    Returns:\n        pd.DataFrame: A DataFrame resulting from the combination of the numerical feature DataFrames.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes all DataFrames have compatible indices for merging.\n    \"\"\"\n    pass\n\ndef concatenate_features(feature_dfs: List[pd.DataFrame], axis: int=1) -> pd.DataFrame:\n    \"\"\"\n    Concatenate multiple feature DataFrames along a specified axis.\n    \n    This function concatenates a list of DataFrames, which contain features that need to be combined into\n    a single DataFrame. It supports concatenation along either rows or columns based on the provided axis.\n    \n    Args:\n        feature_dfs (List[pd.DataFrame]): A list of DataFrames containing feature columns.\n        axis (int, optional): The axis along which to concatenate. Default is 1 (columns).\n    \n    Returns:\n        pd.DataFrame: A concatenated DataFrame containing all combined features.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes that the DataFrames are dimensionally compatible along the non-concatenation axis.\n    \"\"\"\n    pass\n\ndef combine_categorical_features(categorical_dfs: List[pd.DataFrame]) -> pd.DataFrame:\n    \"\"\"\n    Combine multiple categorical feature DataFrames into a single DataFrame.\n    \n    This function merges several DataFrames that contain categorical features. It ensures that the data\n    types remain consistent and that the categorical features are properly combined for further processing.\n    \n    Args:\n        categorical_dfs (List[pd.DataFrame]): A list of DataFrames, each with categorical feature columns.\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the combined categorical features.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes that the DataFrames have a common index or merging logic that allows them to be combined.\n    \"\"\"\n    pass\n\ndef aggregate_features(df: pd.DataFrame, group_by: Optional[List[str]], agg_functions: Dict[str, Callable]) -> pd.DataFrame:\n    \"\"\"\n    Aggregate features in a DataFrame based on specified grouping keys and aggregation functions.\n    \n    This function performs aggregation on a DataFrame by grouping the data using the provided key(s) and \n    then applying a set of aggregation functions. It is useful for summarizing feature values and deriving \n    aggregated representations of data for modeling.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing features to be aggregated.\n        group_by (Optional[List[str]]): Column names to group by; if None, aggregation is performed on the entire DataFrame.\n        agg_functions (Dict[str, Callable]): A dictionary mapping column names to aggregation functions (e.g., sum, mean).\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the aggregated features.\n    \n    Edge Cases:\n        - If group_by is None, aggregation is applied across the entire DataFrame.\n        - An empty DataFrame is returned if the input DataFrame is empty.\n    \"\"\"\n    pass\n\ndef merge_feature_sets(feature_dfs: List[pd.DataFrame], on: Union[str, List[str]]) -> pd.DataFrame:\n    \"\"\"\n    Merge multiple feature DataFrames based on a common key or set of keys.\n    \n    This function executes a merge (join) operation on a list of DataFrames containing features using the specified \n    key(s). It is aimed at integrating feature sets from different sources into a single cohesive DataFrame.\n    \n    Args:\n        feature_dfs (List[pd.DataFrame]): A list of DataFrames with features to be merged.\n        on (Union[str, List[str]]): A column name or list of column names to be used as join keys.\n    \n    Returns:\n        pd.DataFrame: A DataFrame resulting from the merge of the input feature sets.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes all DataFrames in the list contain the specified join key(s).\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import List, Dict, Callable, Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List, Dict, Callable, Optional",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "from typing import List, Union",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List, Union",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "from typing import List",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "combine_numerical_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def combine_numerical_features(numerical_dfs: List[pd.DataFrame]) -> pd.DataFrame:\n    \"\"\"\n    Combine multiple numerical feature DataFrames into a single DataFrame containing numerical features.\n    \n    This function takes a list of DataFrames that each contain numerical feature columns and merges them\n    by aligning their indices. It is designed to consolidate numerical features into one DataFrame for\n    subsequent modeling steps.\n    \n    Args:\n        numerical_dfs (List[pd.DataFrame]): A list of DataFrames, each containing numerical feature columns.\n    \n    Returns:\n        pd.DataFrame: A DataFrame resulting from the combination of the numerical feature DataFrames.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes all DataFrames have compatible indices for merging.\n    \"\"\"\n    pass",
                                                            "lineno": 6
                                                        },
                                                        {
                                                            "name": "concatenate_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def concatenate_features(feature_dfs: List[pd.DataFrame], axis: int=1) -> pd.DataFrame:\n    \"\"\"\n    Concatenate multiple feature DataFrames along a specified axis.\n    \n    This function concatenates a list of DataFrames, which contain features that need to be combined into\n    a single DataFrame. It supports concatenation along either rows or columns based on the provided axis.\n    \n    Args:\n        feature_dfs (List[pd.DataFrame]): A list of DataFrames containing feature columns.\n        axis (int, optional): The axis along which to concatenate. Default is 1 (columns).\n    \n    Returns:\n        pd.DataFrame: A concatenated DataFrame containing all combined features.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes that the DataFrames are dimensionally compatible along the non-concatenation axis.\n    \"\"\"\n    pass",
                                                            "lineno": 26
                                                        },
                                                        {
                                                            "name": "combine_categorical_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def combine_categorical_features(categorical_dfs: List[pd.DataFrame]) -> pd.DataFrame:\n    \"\"\"\n    Combine multiple categorical feature DataFrames into a single DataFrame.\n    \n    This function merges several DataFrames that contain categorical features. It ensures that the data\n    types remain consistent and that the categorical features are properly combined for further processing.\n    \n    Args:\n        categorical_dfs (List[pd.DataFrame]): A list of DataFrames, each with categorical feature columns.\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the combined categorical features.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes that the DataFrames have a common index or merging logic that allows them to be combined.\n    \"\"\"\n    pass",
                                                            "lineno": 46
                                                        },
                                                        {
                                                            "name": "aggregate_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def aggregate_features(df: pd.DataFrame, group_by: Optional[List[str]], agg_functions: Dict[str, Callable]) -> pd.DataFrame:\n    \"\"\"\n    Aggregate features in a DataFrame based on specified grouping keys and aggregation functions.\n    \n    This function performs aggregation on a DataFrame by grouping the data using the provided key(s) and \n    then applying a set of aggregation functions. It is useful for summarizing feature values and deriving \n    aggregated representations of data for modeling.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing features to be aggregated.\n        group_by (Optional[List[str]]): Column names to group by; if None, aggregation is performed on the entire DataFrame.\n        agg_functions (Dict[str, Callable]): A dictionary mapping column names to aggregation functions (e.g., sum, mean).\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the aggregated features.\n    \n    Edge Cases:\n        - If group_by is None, aggregation is applied across the entire DataFrame.\n        - An empty DataFrame is returned if the input DataFrame is empty.\n    \"\"\"\n    pass",
                                                            "lineno": 65
                                                        },
                                                        {
                                                            "name": "merge_feature_sets",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/combine_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def merge_feature_sets(feature_dfs: List[pd.DataFrame], on: Union[str, List[str]]) -> pd.DataFrame:\n    \"\"\"\n    Merge multiple feature DataFrames based on a common key or set of keys.\n    \n    This function executes a merge (join) operation on a list of DataFrames containing features using the specified \n    key(s). It is aimed at integrating feature sets from different sources into a single cohesive DataFrame.\n    \n    Args:\n        feature_dfs (List[pd.DataFrame]): A list of DataFrames with features to be merged.\n        on (Union[str, List[str]]): A column name or list of column names to be used as join keys.\n    \n    Returns:\n        pd.DataFrame: A DataFrame resulting from the merge of the input feature sets.\n    \n    Edge Cases:\n        - Returns an empty DataFrame if the input list is empty.\n        - Assumes all DataFrames in the list contain the specified join key(s).\n    \"\"\"\n    pass",
                                                            "lineno": 87
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "create_features.py",
                                                    "path": "./src/data_engineering/data_preparation/feature_engineering/create_features.py",
                                                    "code": "import pandas as pd\nfrom typing import List\n\ndef derive_new_metrics(feature_data: pd.DataFrame, metric_params: dict) -> pd.DataFrame:\n    \"\"\"\n    Derive new metric features from the provided data based on specified parameters.\n\n    This function computes new metrics using the input feature_data and a dictionary of parameters\n    defining the metric calculations. This might include aggregations or transformations to derive\n    summary or performance measures not originally present in the data.\n\n    Args:\n        feature_data (pd.DataFrame): The input DataFrame containing features.\n        metric_params (dict): A dictionary specifying how to compute new metrics. Keys could be the\n                              new metric names and values could be functions or specifications of the\n                              operations to perform.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the new derived metric features added.\n\n    Edge Cases:\n        - If metric_params is empty, the function may return the original DataFrame.\n        - Assumes feature_data is a valid DataFrame.\n    \"\"\"\n    pass\n\ndef create_time_based_features(data: pd.DataFrame, time_column: str, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Create time-based features from a specified time column in the DataFrame.\n\n    This function extracts various temporal features (such as hour, day, month, weekday, etc.)\n    from the provided time_column and appends them as new features in the output DataFrame.\n    Additional keyword arguments may define specific extraction rules or configurations.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the time column.\n        time_column (str): The name of the column in the DataFrame that holds datetime information.\n        **kwargs: Additional parameters such as timezone adjustments or custom extraction options.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the new time-based features appended.\n\n    Edge Cases:\n        - If time_column does not exist in data, the function should handle it appropriately.\n        - Assumes the time_column is in a parseable datetime format.\n    \"\"\"\n    pass\n\ndef generate_ratios(feature_data: pd.DataFrame, ratio_definitions: dict) -> pd.DataFrame:\n    \"\"\"\n    Generate ratio features based on specified numerator-denominator pairs.\n\n    This function calculates new features by computing ratios for selected columns in the\n    feature_data. The ratio_definitions dictionary maps new feature names to a tuple of two strings,\n    each representing the column names for the numerator and denominator.\n\n    Args:\n        feature_data (pd.DataFrame): The input DataFrame containing the features.\n        ratio_definitions (dict): A dictionary where each key is the name of the new ratio feature and\n                                  the value is a tuple (numerator_column, denominator_column).\n\n    Returns:\n        pd.DataFrame: A DataFrame with the newly generated ratio features added.\n\n    Edge Cases:\n        - If any denominator is zero or missing, the function should handle division errors.\n        - If ratio_definitions is empty, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef create_location_based_features(data: pd.DataFrame, location_column: str, additional_params: dict=None) -> pd.DataFrame:\n    \"\"\"\n    Create location-based features from a specified location column in the DataFrame.\n\n    This function extracts and/or transforms the location data (e.g., geographical coordinates, \n    addresses, regions) found in the location_column and generates new features such as distance measures, \n    region flags, or spatial clusters. Additional parameters can guide specific feature extraction methods.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing location data.\n        location_column (str): The name of the column that holds location information.\n        additional_params (dict, optional): A dictionary with extra options to guide the feature \n                                            extraction process. Defaults to None.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the newly created location-based features appended.\n\n    Edge Cases:\n        - If location_column is missing or contains invalid entries, the function\n          should handle such cases gracefully.\n    \"\"\"\n    pass\n\ndef create_date_features(data: pd.DataFrame, date_column: str, date_format: str='%Y-%m-%d') -> pd.DataFrame:\n    \"\"\"\n    Create new features by extracting elements from a date field in the DataFrame.\n\n    This function converts the given date_column into various derived features such as year, month, day,\n    weekday, and other date-related components based on the specified date_format. These new features can \n    be utilized in downstream analyses.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing the date information.\n        date_column (str): The name of the column containing date strings or datetime objects.\n        date_format (str, optional): The format in which dates are structured if they are in string form.\n                                     Defaults to '%Y-%m-%d'.\n\n    Returns:\n        pd.DataFrame: A DataFrame with newly created features derived from the date column.\n\n    Edge Cases:\n        - If date_column does not exist or dates are not parsable using the given format, the function\n          should report an error or skip feature creation.\n    \"\"\"\n    pass\n\ndef create_external_data_features(main_data: pd.DataFrame, external_data: pd.DataFrame, join_keys: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Create new features by integrating external data with the main DataFrame.\n\n    This function merges the main_data with external_data based on the specified join_keys to augment the \n    feature set with external information. This can be used to incorporate additional context or metadata \n    into the main dataset.\n\n    Args:\n        main_data (pd.DataFrame): The primary DataFrame containing the original features.\n        external_data (pd.DataFrame): The external DataFrame that contains supplementary features.\n        join_keys (List[str]): A list of column names used to join the two DataFrames.\n\n    Returns:\n        pd.DataFrame: A DataFrame resulting from the merge process, enriched with external features.\n\n    Edge Cases:\n        - If the join_keys are not present in both DataFrames, the function should handle the error.\n    \"\"\"\n    pass\n\ndef create_existing_data_features(data: pd.DataFrame, feature_instructions: dict) -> pd.DataFrame:\n    \"\"\"\n    Create new features by transforming or combining existing features within the DataFrame.\n\n    This function applies a set of specified transformations (outlined in feature_instructions) to the \n    DataFrame in order to generate new, derived features from the existing ones. These instructions \n    could include arithmetic operations, concatenations, or other custom transformations.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the original features.\n        feature_instructions (dict): A dictionary defining how to transform or combine existing features.\n                                     The keys might refer to new feature names, and the values indicate the\n                                     operations or source columns involved.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the newly created features from existing data added.\n\n    Edge Cases:\n        - If feature_instructions is empty, the function could return the original DataFrame.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/create_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import List",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/create_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "derive_new_metrics",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/create_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def derive_new_metrics(feature_data: pd.DataFrame, metric_params: dict) -> pd.DataFrame:\n    \"\"\"\n    Derive new metric features from the provided data based on specified parameters.\n\n    This function computes new metrics using the input feature_data and a dictionary of parameters\n    defining the metric calculations. This might include aggregations or transformations to derive\n    summary or performance measures not originally present in the data.\n\n    Args:\n        feature_data (pd.DataFrame): The input DataFrame containing features.\n        metric_params (dict): A dictionary specifying how to compute new metrics. Keys could be the\n                              new metric names and values could be functions or specifications of the\n                              operations to perform.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the new derived metric features added.\n\n    Edge Cases:\n        - If metric_params is empty, the function may return the original DataFrame.\n        - Assumes feature_data is a valid DataFrame.\n    \"\"\"\n    pass",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "create_time_based_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/create_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def create_time_based_features(data: pd.DataFrame, time_column: str, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Create time-based features from a specified time column in the DataFrame.\n\n    This function extracts various temporal features (such as hour, day, month, weekday, etc.)\n    from the provided time_column and appends them as new features in the output DataFrame.\n    Additional keyword arguments may define specific extraction rules or configurations.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the time column.\n        time_column (str): The name of the column in the DataFrame that holds datetime information.\n        **kwargs: Additional parameters such as timezone adjustments or custom extraction options.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the new time-based features appended.\n\n    Edge Cases:\n        - If time_column does not exist in data, the function should handle it appropriately.\n        - Assumes the time_column is in a parseable datetime format.\n    \"\"\"\n    pass",
                                                            "lineno": 27
                                                        },
                                                        {
                                                            "name": "generate_ratios",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/create_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def generate_ratios(feature_data: pd.DataFrame, ratio_definitions: dict) -> pd.DataFrame:\n    \"\"\"\n    Generate ratio features based on specified numerator-denominator pairs.\n\n    This function calculates new features by computing ratios for selected columns in the\n    feature_data. The ratio_definitions dictionary maps new feature names to a tuple of two strings,\n    each representing the column names for the numerator and denominator.\n\n    Args:\n        feature_data (pd.DataFrame): The input DataFrame containing the features.\n        ratio_definitions (dict): A dictionary where each key is the name of the new ratio feature and\n                                  the value is a tuple (numerator_column, denominator_column).\n\n    Returns:\n        pd.DataFrame: A DataFrame with the newly generated ratio features added.\n\n    Edge Cases:\n        - If any denominator is zero or missing, the function should handle division errors.\n        - If ratio_definitions is empty, the original DataFrame is returned.\n    \"\"\"\n    pass",
                                                            "lineno": 49
                                                        },
                                                        {
                                                            "name": "create_location_based_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/create_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def create_location_based_features(data: pd.DataFrame, location_column: str, additional_params: dict=None) -> pd.DataFrame:\n    \"\"\"\n    Create location-based features from a specified location column in the DataFrame.\n\n    This function extracts and/or transforms the location data (e.g., geographical coordinates, \n    addresses, regions) found in the location_column and generates new features such as distance measures, \n    region flags, or spatial clusters. Additional parameters can guide specific feature extraction methods.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing location data.\n        location_column (str): The name of the column that holds location information.\n        additional_params (dict, optional): A dictionary with extra options to guide the feature \n                                            extraction process. Defaults to None.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the newly created location-based features appended.\n\n    Edge Cases:\n        - If location_column is missing or contains invalid entries, the function\n          should handle such cases gracefully.\n    \"\"\"\n    pass",
                                                            "lineno": 71
                                                        },
                                                        {
                                                            "name": "create_date_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/create_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def create_date_features(data: pd.DataFrame, date_column: str, date_format: str='%Y-%m-%d') -> pd.DataFrame:\n    \"\"\"\n    Create new features by extracting elements from a date field in the DataFrame.\n\n    This function converts the given date_column into various derived features such as year, month, day,\n    weekday, and other date-related components based on the specified date_format. These new features can \n    be utilized in downstream analyses.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing the date information.\n        date_column (str): The name of the column containing date strings or datetime objects.\n        date_format (str, optional): The format in which dates are structured if they are in string form.\n                                     Defaults to '%Y-%m-%d'.\n\n    Returns:\n        pd.DataFrame: A DataFrame with newly created features derived from the date column.\n\n    Edge Cases:\n        - If date_column does not exist or dates are not parsable using the given format, the function\n          should report an error or skip feature creation.\n    \"\"\"\n    pass",
                                                            "lineno": 94
                                                        },
                                                        {
                                                            "name": "create_external_data_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/create_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def create_external_data_features(main_data: pd.DataFrame, external_data: pd.DataFrame, join_keys: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Create new features by integrating external data with the main DataFrame.\n\n    This function merges the main_data with external_data based on the specified join_keys to augment the \n    feature set with external information. This can be used to incorporate additional context or metadata \n    into the main dataset.\n\n    Args:\n        main_data (pd.DataFrame): The primary DataFrame containing the original features.\n        external_data (pd.DataFrame): The external DataFrame that contains supplementary features.\n        join_keys (List[str]): A list of column names used to join the two DataFrames.\n\n    Returns:\n        pd.DataFrame: A DataFrame resulting from the merge process, enriched with external features.\n\n    Edge Cases:\n        - If the join_keys are not present in both DataFrames, the function should handle the error.\n    \"\"\"\n    pass",
                                                            "lineno": 117
                                                        },
                                                        {
                                                            "name": "create_existing_data_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/create_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def create_existing_data_features(data: pd.DataFrame, feature_instructions: dict) -> pd.DataFrame:\n    \"\"\"\n    Create new features by transforming or combining existing features within the DataFrame.\n\n    This function applies a set of specified transformations (outlined in feature_instructions) to the \n    DataFrame in order to generate new, derived features from the existing ones. These instructions \n    could include arithmetic operations, concatenations, or other custom transformations.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the original features.\n        feature_instructions (dict): A dictionary defining how to transform or combine existing features.\n                                     The keys might refer to new feature names, and the values indicate the\n                                     operations or source columns involved.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the newly created features from existing data added.\n\n    Edge Cases:\n        - If feature_instructions is empty, the function could return the original DataFrame.\n    \"\"\"\n    pass",
                                                            "lineno": 138
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "feature_extraction.py",
                                                    "path": "./src/data_engineering/data_preparation/feature_engineering/feature_extraction.py",
                                                    "code": "from typing import Optional\nimport numpy as np\nfrom typing import List, Optional\nimport pandas as pd\n\ndef extract_categorical_features(data: pd.DataFrame, categorical_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Extract categorical features from the input DataFrame.\n\n    This function isolates categorical features from the provided DataFrame for use in downstream tasks such as encoding\n    or modeling. It either extracts user-specified columns or automatically identifies categorical types if none are provided.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the data from which to extract categorical features.\n        categorical_columns (Optional[List[str]]): A list of column names to extract as categorical features. \n            If None, the function will attempt to infer categorical columns automatically.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the extracted categorical features.\n\n    Raises:\n        ValueError: If the input DataFrame is empty or if the specified 'categorical_columns' are not found in the DataFrame.\n    \"\"\"\n    pass\n\ndef extract_numerical_features(data: pd.DataFrame, numerical_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Extract numerical features from the input DataFrame.\n\n    This function identifies and extracts numerical features from a DataFrame, which may then be utilized for scaling,\n    statistical analysis, or machine learning model training. It supports both automatic detection and explicit column specification.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing a mix of feature types.\n        numerical_columns (Optional[List[str]]): A list of column names representing numerical features to extract.\n            If None, the function will automatically detect numerical columns.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the extracted numerical features.\n\n    Raises:\n        ValueError: If the input DataFrame is empty or if no numerical columns are found based on the given criteria.\n    \"\"\"\n    pass\n\ndef extract_text_features(data: pd.DataFrame, text_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Extract features from text data in the input DataFrame.\n\n    This function processes the DataFrame to extract textual features, applying common text processing techniques such as \n    tokenization or embedding extraction. Users may supply a list of text_columns, or the function can attempt to infer text data.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing textual data.\n        text_columns (Optional[List[str]]): A list of column names that contain text data. \n            If None, the function will attempt to automatically identify text-containing columns.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing features extracted from the text data.\n\n    Raises:\n        ValueError: If the input DataFrame is empty, or if specified text columns are not present in the DataFrame.\n    \"\"\"\n    pass\n\ndef extract_image_features(image_array: np.ndarray, config: Optional[dict]=None) -> np.ndarray:\n    \"\"\"\n    Extract features from image data represented as a numpy array.\n\n    This function processes an image (represented as a numpy array) to extract meaningful features such as descriptors or embeddings.\n    An optional configuration dictionary allows for customization of the feature extraction process.\n\n    Args:\n        image_array (np.ndarray): The input image data in numpy array format.\n        config (Optional[dict]): A dictionary containing configuration parameters to customize the feature extraction.\n            If None, default extraction parameters are used.\n\n    Returns:\n        np.ndarray: A numpy array containing the extracted image features.\n\n    Raises:\n        ValueError: If the input image array is empty or does not conform to expected dimensions, or if the configuration \n                    is missing required keys for processing.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_extraction.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import numpy as np",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_extraction.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import numpy as np",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "from typing import List, Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_extraction.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List, Optional",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_extraction.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "extract_categorical_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_extraction.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def extract_categorical_features(data: pd.DataFrame, categorical_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Extract categorical features from the input DataFrame.\n\n    This function isolates categorical features from the provided DataFrame for use in downstream tasks such as encoding\n    or modeling. It either extracts user-specified columns or automatically identifies categorical types if none are provided.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the data from which to extract categorical features.\n        categorical_columns (Optional[List[str]]): A list of column names to extract as categorical features. \n            If None, the function will attempt to infer categorical columns automatically.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the extracted categorical features.\n\n    Raises:\n        ValueError: If the input DataFrame is empty or if the specified 'categorical_columns' are not found in the DataFrame.\n    \"\"\"\n    pass",
                                                            "lineno": 6
                                                        },
                                                        {
                                                            "name": "extract_numerical_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_extraction.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def extract_numerical_features(data: pd.DataFrame, numerical_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Extract numerical features from the input DataFrame.\n\n    This function identifies and extracts numerical features from a DataFrame, which may then be utilized for scaling,\n    statistical analysis, or machine learning model training. It supports both automatic detection and explicit column specification.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing a mix of feature types.\n        numerical_columns (Optional[List[str]]): A list of column names representing numerical features to extract.\n            If None, the function will automatically detect numerical columns.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the extracted numerical features.\n\n    Raises:\n        ValueError: If the input DataFrame is empty or if no numerical columns are found based on the given criteria.\n    \"\"\"\n    pass",
                                                            "lineno": 26
                                                        },
                                                        {
                                                            "name": "extract_text_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_extraction.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def extract_text_features(data: pd.DataFrame, text_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Extract features from text data in the input DataFrame.\n\n    This function processes the DataFrame to extract textual features, applying common text processing techniques such as \n    tokenization or embedding extraction. Users may supply a list of text_columns, or the function can attempt to infer text data.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing textual data.\n        text_columns (Optional[List[str]]): A list of column names that contain text data. \n            If None, the function will attempt to automatically identify text-containing columns.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing features extracted from the text data.\n\n    Raises:\n        ValueError: If the input DataFrame is empty, or if specified text columns are not present in the DataFrame.\n    \"\"\"\n    pass",
                                                            "lineno": 46
                                                        },
                                                        {
                                                            "name": "extract_image_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_extraction.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def extract_image_features(image_array: np.ndarray, config: Optional[dict]=None) -> np.ndarray:\n    \"\"\"\n    Extract features from image data represented as a numpy array.\n\n    This function processes an image (represented as a numpy array) to extract meaningful features such as descriptors or embeddings.\n    An optional configuration dictionary allows for customization of the feature extraction process.\n\n    Args:\n        image_array (np.ndarray): The input image data in numpy array format.\n        config (Optional[dict]): A dictionary containing configuration parameters to customize the feature extraction.\n            If None, default extraction parameters are used.\n\n    Returns:\n        np.ndarray: A numpy array containing the extracted image features.\n\n    Raises:\n        ValueError: If the input image array is empty or does not conform to expected dimensions, or if the configuration \n                    is missing required keys for processing.\n    \"\"\"\n    pass",
                                                            "lineno": 66
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "feature_scaling.py",
                                                    "path": "./src/data_engineering/data_preparation/feature_engineering/feature_scaling.py",
                                                    "code": "from typing import Optional, List\nfrom typing import Optional, List, Union\nimport pandas as pd\n\ndef scale_features_to_range(df: pd.DataFrame, feature_columns: Optional[List[str]]=None, min_value: Union[int, float]=0, max_value: Union[int, float]=1) -> pd.DataFrame:\n    \"\"\"\n    Scale features in a DataFrame to a specified range using min-max scaling.\n\n    This function implements the min-max scaling algorithm which rescales the data\n    in the specified feature columns to fit within the provided [min_value, max_value] range.\n    If 'feature_columns' is not provided, all numeric columns in the DataFrame will be scaled.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the features to scale.\n        feature_columns (Optional[List[str]]): A list of column names to be scaled.\n            If None, all numeric columns are considered.\n        min_value (Union[int, float]): The minimum value of the desired scale range. Default is 0.\n        max_value (Union[int, float]): The maximum value of the desired scale range. Default is 1.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the specified features scaled to the given range.\n\n    Edge Cases and Assumptions:\n        - The function assumes that 'df' contains numeric data for the selected columns.\n        - If feature_columns is provided but some columns are non-numeric, the behavior is undefined.\n        - The function does not modify the input DataFrame in-place; it returns a new DataFrame.\n    \"\"\"\n    pass\n\ndef standard_scale_features(df: pd.DataFrame, feature_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Standardize features in a DataFrame using standard scaling (z-score normalization).\n\n    This function applies standard scaling to the selected feature columns by subtracting\n    the mean and dividing by the standard deviation for each feature. If 'feature_columns' is\n    not provided, all numeric columns will be standardized.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the features to standardize.\n        feature_columns (Optional[List[str]]): A list of column names to be standardized.\n            If None, standardization will be applied to all numeric columns.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the specified features standardized.\n\n    Edge Cases and Assumptions:\n        - The function assumes that 'df' contains numeric data for the selected columns.\n        - Columns with zero variance may result in division by zero; these cases are not explicitly handled.\n        - The input DataFrame remains unchanged; a new DataFrame with standardized values is returned.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import Optional, List",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_scaling.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional, List",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import Optional, List, Union",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_scaling.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional, List, Union",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_scaling.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "scale_features_to_range",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_scaling.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def scale_features_to_range(df: pd.DataFrame, feature_columns: Optional[List[str]]=None, min_value: Union[int, float]=0, max_value: Union[int, float]=1) -> pd.DataFrame:\n    \"\"\"\n    Scale features in a DataFrame to a specified range using min-max scaling.\n\n    This function implements the min-max scaling algorithm which rescales the data\n    in the specified feature columns to fit within the provided [min_value, max_value] range.\n    If 'feature_columns' is not provided, all numeric columns in the DataFrame will be scaled.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the features to scale.\n        feature_columns (Optional[List[str]]): A list of column names to be scaled.\n            If None, all numeric columns are considered.\n        min_value (Union[int, float]): The minimum value of the desired scale range. Default is 0.\n        max_value (Union[int, float]): The maximum value of the desired scale range. Default is 1.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the specified features scaled to the given range.\n\n    Edge Cases and Assumptions:\n        - The function assumes that 'df' contains numeric data for the selected columns.\n        - If feature_columns is provided but some columns are non-numeric, the behavior is undefined.\n        - The function does not modify the input DataFrame in-place; it returns a new DataFrame.\n    \"\"\"\n    pass",
                                                            "lineno": 5
                                                        },
                                                        {
                                                            "name": "standard_scale_features",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_scaling.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def standard_scale_features(df: pd.DataFrame, feature_columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Standardize features in a DataFrame using standard scaling (z-score normalization).\n\n    This function applies standard scaling to the selected feature columns by subtracting\n    the mean and dividing by the standard deviation for each feature. If 'feature_columns' is\n    not provided, all numeric columns will be standardized.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the features to standardize.\n        feature_columns (Optional[List[str]]): A list of column names to be standardized.\n            If None, standardization will be applied to all numeric columns.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the specified features standardized.\n\n    Edge Cases and Assumptions:\n        - The function assumes that 'df' contains numeric data for the selected columns.\n        - Columns with zero variance may result in division by zero; these cases are not explicitly handled.\n        - The input DataFrame remains unchanged; a new DataFrame with standardized values is returned.\n    \"\"\"\n    pass",
                                                            "lineno": 30
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "interaction_terms.py",
                                                    "path": "./src/data_engineering/data_preparation/feature_engineering/interaction_terms.py",
                                                    "code": "from typing import List, Optional, Union\nimport pandas as pd\n\nclass InteractionTermsGenerator:\n    \"\"\"\n    InteractionTermsGenerator is a utility class that provides methods to generate interaction features\n    from a given DataFrame. It supports creating categorical interaction terms, polynomial terms,\n    multiplicative terms, and ratio terms. These transformations are commonly used in feature engineering \n    to capture non-linear relationships and interactions between features.\n\n    Methods:\n        create_categorical_interaction_terms(df, columns) -> pd.DataFrame:\n            Generates interaction terms between specified categorical columns.\n        \n        create_polynomial_terms(df, degree, include_bias) -> pd.DataFrame:\n            Generates polynomial features of a given degree based on numerical columns.\n            \n        create_multiplicative_terms(df, columns) -> pd.DataFrame:\n            Generates new features by computing the product of values across the given columns.\n            \n        create_ratio_terms(df, numerator_columns, denominator_columns, epsilon) -> pd.DataFrame:\n            Generates new features by computing the ratio between specified numerator and denominator columns.\n    \"\"\"\n\n    def create_categorical_interaction_terms(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n        \"\"\"\n        Generate categorical interaction terms by combining specified categorical columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing the categorical features.\n            columns (List[str]): List of column names to combine for creating interaction terms.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with new columns representing the categorical interaction terms.\n        \n        Edge Cases:\n            - Returns the original DataFrame if the list of columns is empty.\n            - Assumes that the provided columns exist in the DataFrame.\n        \"\"\"\n        pass\n\n    def create_polynomial_terms(self, df: pd.DataFrame, degree: int=2, include_bias: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Generate polynomial terms for numerical features up to the specified degree.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame with numerical features.\n            degree (int, optional): Degree of the polynomial features to be generated. Defaults to 2.\n            include_bias (bool, optional): If True, include a bias (intercept) column. Defaults to False.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with additional polynomial features.\n        \n        Edge Cases:\n            - Assumes that the DataFrame contains only numerical columns for transformation.\n            - If degree is less than 2, no additional interaction is created.\n        \"\"\"\n        pass\n\n    def create_multiplicative_terms(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n        \"\"\"\n        Create multiplicative interaction terms by multiplying the values of specified columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing features.\n            columns (List[str]): List of columns whose element-wise products are to be computed.\n            \n        Returns:\n            pd.DataFrame: A DataFrame enriched with multiplicative interaction features.\n        \n        Edge Cases:\n            - If fewer than two columns are provided, the function returns the DataFrame unmodified.\n            - Assumes numerical data for valid multiplication.\n        \"\"\"\n        pass\n\n    def create_ratio_terms(self, df: pd.DataFrame, numerator_columns: List[str], denominator_columns: List[str], epsilon: float=1e-08) -> pd.DataFrame:\n        \"\"\"\n        Create ratio terms by computing the element-wise ratios of the specified numerator and denominator columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical data.\n            numerator_columns (List[str]): List of column names to be used as numerators.\n            denominator_columns (List[str]): List of column names to be used as denominators.\n            epsilon (float, optional): A small number added to denominators to avoid division by zero. Defaults to 1e-8.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with new columns representing the ratio terms.\n        \n        Edge Cases:\n            - Assumes that for each ratio, corresponding numerator and denominator columns exist.\n            - If denominator values are zero, the epsilon value prevents division errors.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import List, Optional, Union",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/interaction_terms.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List, Optional, Union",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/interaction_terms.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "InteractionTermsGenerator",
                                                            "unit_type": "class",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/interaction_terms.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class InteractionTermsGenerator:\n    \"\"\"\n    InteractionTermsGenerator is a utility class that provides methods to generate interaction features\n    from a given DataFrame. It supports creating categorical interaction terms, polynomial terms,\n    multiplicative terms, and ratio terms. These transformations are commonly used in feature engineering \n    to capture non-linear relationships and interactions between features.\n\n    Methods:\n        create_categorical_interaction_terms(df, columns) -> pd.DataFrame:\n            Generates interaction terms between specified categorical columns.\n        \n        create_polynomial_terms(df, degree, include_bias) -> pd.DataFrame:\n            Generates polynomial features of a given degree based on numerical columns.\n            \n        create_multiplicative_terms(df, columns) -> pd.DataFrame:\n            Generates new features by computing the product of values across the given columns.\n            \n        create_ratio_terms(df, numerator_columns, denominator_columns, epsilon) -> pd.DataFrame:\n            Generates new features by computing the ratio between specified numerator and denominator columns.\n    \"\"\"\n\n    def create_categorical_interaction_terms(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n        \"\"\"\n        Generate categorical interaction terms by combining specified categorical columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing the categorical features.\n            columns (List[str]): List of column names to combine for creating interaction terms.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with new columns representing the categorical interaction terms.\n        \n        Edge Cases:\n            - Returns the original DataFrame if the list of columns is empty.\n            - Assumes that the provided columns exist in the DataFrame.\n        \"\"\"\n        pass\n\n    def create_polynomial_terms(self, df: pd.DataFrame, degree: int=2, include_bias: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Generate polynomial terms for numerical features up to the specified degree.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame with numerical features.\n            degree (int, optional): Degree of the polynomial features to be generated. Defaults to 2.\n            include_bias (bool, optional): If True, include a bias (intercept) column. Defaults to False.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with additional polynomial features.\n        \n        Edge Cases:\n            - Assumes that the DataFrame contains only numerical columns for transformation.\n            - If degree is less than 2, no additional interaction is created.\n        \"\"\"\n        pass\n\n    def create_multiplicative_terms(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n        \"\"\"\n        Create multiplicative interaction terms by multiplying the values of specified columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing features.\n            columns (List[str]): List of columns whose element-wise products are to be computed.\n            \n        Returns:\n            pd.DataFrame: A DataFrame enriched with multiplicative interaction features.\n        \n        Edge Cases:\n            - If fewer than two columns are provided, the function returns the DataFrame unmodified.\n            - Assumes numerical data for valid multiplication.\n        \"\"\"\n        pass\n\n    def create_ratio_terms(self, df: pd.DataFrame, numerator_columns: List[str], denominator_columns: List[str], epsilon: float=1e-08) -> pd.DataFrame:\n        \"\"\"\n        Create ratio terms by computing the element-wise ratios of the specified numerator and denominator columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical data.\n            numerator_columns (List[str]): List of column names to be used as numerators.\n            denominator_columns (List[str]): List of column names to be used as denominators.\n            epsilon (float, optional): A small number added to denominators to avoid division by zero. Defaults to 1e-8.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with new columns representing the ratio terms.\n        \n        Edge Cases:\n            - Assumes that for each ratio, corresponding numerator and denominator columns exist.\n            - If denominator values are zero, the epsilon value prevents division errors.\n        \"\"\"\n        pass",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "create_categorical_interaction_terms",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/interaction_terms.py",
                                                            "parent": "InteractionTermsGenerator",
                                                            "extra": {},
                                                            "code": "def create_categorical_interaction_terms(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n    \"\"\"\n        Generate categorical interaction terms by combining specified categorical columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing the categorical features.\n            columns (List[str]): List of column names to combine for creating interaction terms.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with new columns representing the categorical interaction terms.\n        \n        Edge Cases:\n            - Returns the original DataFrame if the list of columns is empty.\n            - Assumes that the provided columns exist in the DataFrame.\n        \"\"\"\n    pass",
                                                            "lineno": 25
                                                        },
                                                        {
                                                            "name": "create_polynomial_terms",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/interaction_terms.py",
                                                            "parent": "InteractionTermsGenerator",
                                                            "extra": {},
                                                            "code": "def create_polynomial_terms(self, df: pd.DataFrame, degree: int=2, include_bias: bool=False) -> pd.DataFrame:\n    \"\"\"\n        Generate polynomial terms for numerical features up to the specified degree.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame with numerical features.\n            degree (int, optional): Degree of the polynomial features to be generated. Defaults to 2.\n            include_bias (bool, optional): If True, include a bias (intercept) column. Defaults to False.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with additional polynomial features.\n        \n        Edge Cases:\n            - Assumes that the DataFrame contains only numerical columns for transformation.\n            - If degree is less than 2, no additional interaction is created.\n        \"\"\"\n    pass",
                                                            "lineno": 42
                                                        },
                                                        {
                                                            "name": "create_multiplicative_terms",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/interaction_terms.py",
                                                            "parent": "InteractionTermsGenerator",
                                                            "extra": {},
                                                            "code": "def create_multiplicative_terms(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n    \"\"\"\n        Create multiplicative interaction terms by multiplying the values of specified columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing features.\n            columns (List[str]): List of columns whose element-wise products are to be computed.\n            \n        Returns:\n            pd.DataFrame: A DataFrame enriched with multiplicative interaction features.\n        \n        Edge Cases:\n            - If fewer than two columns are provided, the function returns the DataFrame unmodified.\n            - Assumes numerical data for valid multiplication.\n        \"\"\"\n    pass",
                                                            "lineno": 60
                                                        },
                                                        {
                                                            "name": "create_ratio_terms",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/interaction_terms.py",
                                                            "parent": "InteractionTermsGenerator",
                                                            "extra": {},
                                                            "code": "def create_ratio_terms(self, df: pd.DataFrame, numerator_columns: List[str], denominator_columns: List[str], epsilon: float=1e-08) -> pd.DataFrame:\n    \"\"\"\n        Create ratio terms by computing the element-wise ratios of the specified numerator and denominator columns.\n        \n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical data.\n            numerator_columns (List[str]): List of column names to be used as numerators.\n            denominator_columns (List[str]): List of column names to be used as denominators.\n            epsilon (float, optional): A small number added to denominators to avoid division by zero. Defaults to 1e-8.\n            \n        Returns:\n            pd.DataFrame: A DataFrame with new columns representing the ratio terms.\n        \n        Edge Cases:\n            - Assumes that for each ratio, corresponding numerator and denominator columns exist.\n            - If denominator values are zero, the epsilon value prevents division errors.\n        \"\"\"\n    pass",
                                                            "lineno": 77
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "polynomial_features.py",
                                                    "path": "./src/data_engineering/data_preparation/feature_engineering/polynomial_features.py",
                                                    "code": "from typing import Optional\nimport pandas as pd\n\nclass PolynomialFeaturesGenerator:\n    \"\"\"\n    A generator for creating polynomial features from numerical data.\n\n    This class provides methods to generate higher-order polynomial terms\n    from a given DataFrame. It includes a generic method to create polynomial terms \n    up to any specified degree, a method specifically to create third-order terms,\n    and a method to generate second-order (degree 2) polynomial features.\n\n    Methods:\n        generate_higher_order_terms(df: pd.DataFrame, degree: int) -> pd.DataFrame:\n            Generates polynomial features up to the specified degree.\n        create_third_order_terms(df: pd.DataFrame) -> pd.DataFrame:\n            Generates polynomial features of exactly third order.\n        create_degree2_polynomial_features(df: pd.DataFrame) -> pd.DataFrame:\n            Generates polynomial features of degree 2.\n    \n    Note:\n        - It is assumed that the input DataFrame contains only numerical data.\n        - Preprocessing to handle non-numerical data should be done beforehand.\n    \"\"\"\n\n    def generate_higher_order_terms(self, df: pd.DataFrame, degree: int) -> pd.DataFrame:\n        \"\"\"\n        Generate polynomial features up to the specified degree.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n            degree (int): Maximum power for the polynomial expansion (>= 2).\n\n        Returns:\n            pd.DataFrame: DataFrame with the generated polynomial features appended.\n        \"\"\"\n        pass\n\n    def create_third_order_terms(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Create polynomial features of exactly third order.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n\n        Returns:\n            pd.DataFrame: DataFrame with third-order polynomial features added.\n        \"\"\"\n        pass\n\n    def create_degree2_polynomial_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate polynomial features of degree 2.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n\n        Returns:\n            pd.DataFrame: DataFrame with second-order (degree 2) polynomial features appended.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/polynomial_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/polynomial_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "PolynomialFeaturesGenerator",
                                                            "unit_type": "class",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/polynomial_features.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class PolynomialFeaturesGenerator:\n    \"\"\"\n    A generator for creating polynomial features from numerical data.\n\n    This class provides methods to generate higher-order polynomial terms\n    from a given DataFrame. It includes a generic method to create polynomial terms \n    up to any specified degree, a method specifically to create third-order terms,\n    and a method to generate second-order (degree 2) polynomial features.\n\n    Methods:\n        generate_higher_order_terms(df: pd.DataFrame, degree: int) -> pd.DataFrame:\n            Generates polynomial features up to the specified degree.\n        create_third_order_terms(df: pd.DataFrame) -> pd.DataFrame:\n            Generates polynomial features of exactly third order.\n        create_degree2_polynomial_features(df: pd.DataFrame) -> pd.DataFrame:\n            Generates polynomial features of degree 2.\n    \n    Note:\n        - It is assumed that the input DataFrame contains only numerical data.\n        - Preprocessing to handle non-numerical data should be done beforehand.\n    \"\"\"\n\n    def generate_higher_order_terms(self, df: pd.DataFrame, degree: int) -> pd.DataFrame:\n        \"\"\"\n        Generate polynomial features up to the specified degree.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n            degree (int): Maximum power for the polynomial expansion (>= 2).\n\n        Returns:\n            pd.DataFrame: DataFrame with the generated polynomial features appended.\n        \"\"\"\n        pass\n\n    def create_third_order_terms(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Create polynomial features of exactly third order.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n\n        Returns:\n            pd.DataFrame: DataFrame with third-order polynomial features added.\n        \"\"\"\n        pass\n\n    def create_degree2_polynomial_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate polynomial features of degree 2.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n\n        Returns:\n            pd.DataFrame: DataFrame with second-order (degree 2) polynomial features appended.\n        \"\"\"\n        pass",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "generate_higher_order_terms",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/polynomial_features.py",
                                                            "parent": "PolynomialFeaturesGenerator",
                                                            "extra": {},
                                                            "code": "def generate_higher_order_terms(self, df: pd.DataFrame, degree: int) -> pd.DataFrame:\n    \"\"\"\n        Generate polynomial features up to the specified degree.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n            degree (int): Maximum power for the polynomial expansion (>= 2).\n\n        Returns:\n            pd.DataFrame: DataFrame with the generated polynomial features appended.\n        \"\"\"\n    pass",
                                                            "lineno": 26
                                                        },
                                                        {
                                                            "name": "create_third_order_terms",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/polynomial_features.py",
                                                            "parent": "PolynomialFeaturesGenerator",
                                                            "extra": {},
                                                            "code": "def create_third_order_terms(self, df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Create polynomial features of exactly third order.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n\n        Returns:\n            pd.DataFrame: DataFrame with third-order polynomial features added.\n        \"\"\"\n    pass",
                                                            "lineno": 39
                                                        },
                                                        {
                                                            "name": "create_degree2_polynomial_features",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/polynomial_features.py",
                                                            "parent": "PolynomialFeaturesGenerator",
                                                            "extra": {},
                                                            "code": "def create_degree2_polynomial_features(self, df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Generate polynomial features of degree 2.\n\n        Args:\n            df (pd.DataFrame): Input DataFrame containing numerical features.\n\n        Returns:\n            pd.DataFrame: DataFrame with second-order (degree 2) polynomial features appended.\n        \"\"\"\n    pass",
                                                            "lineno": 51
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "feature_selection.py",
                                                    "path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                    "code": "from typing import List, Any\nfrom typing import List, Tuple, Any\nimport pandas as pd\n\nclass FilterFeatureSelector:\n    \"\"\"\n    A collection of filter-based feature selection methods.\n\n    This class implements multiple filtering techniques for feature selection such as:\n      - Select with chi-squared tests.\n      - Select with mutual information scores.\n      - Select features with high correlation to the target.\n      - Filter features based on low variance.\n      - Rank features by their computed importance.\n\n    Each method is designed to work with input pandas DataFrames and aligns with the\n    data flow expectations of the system.\n    \"\"\"\n\n    def select_with_chi_squared(self, X: pd.DataFrame, y: Any, k: int) -> List[str]:\n        \"\"\"\n        Select the top 'k' features based on chi-squared test scores.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix with non-negative features.\n            y (Any): The target variable.\n            k (int): The number of top features to select.\n\n        Returns:\n            List[str]: A list of selected feature names based on chi-squared results.\n\n        Raises:\n            ValueError: If X contains negative values or if k is out of valid range.\n        \"\"\"\n        pass\n\n    def select_with_mutual_information(self, X: pd.DataFrame, y: Any, k: int) -> List[str]:\n        \"\"\"\n        Select the top 'k' features based on mutual information scores.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            y (Any): The target variable.\n            k (int): The number of top features to select.\n\n        Returns:\n            List[str]: A list of feature names selected based on mutual information.\n\n        Raises:\n            ValueError: If the mutual information calculation fails.\n        \"\"\"\n        pass\n\n    def select_high_correlation(self, X: pd.DataFrame, y: Any, threshold: float) -> List[str]:\n        \"\"\"\n        Select features that have a correlation with the target variable above a given threshold.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix with named columns.\n            y (Any): The target variable used for correlation computation.\n            threshold (float): The minimum correlation coefficient to consider. \n                               Should be between 0 and 1.\n\n        Returns:\n            List[str]: A list of feature names that exhibit high correlation with the target.\n\n        Raises:\n            ValueError: If threshold is not between 0 and 1.\n        \"\"\"\n        pass\n\n    def select_low_variance(self, X: pd.DataFrame, threshold: float) -> List[str]:\n        \"\"\"\n        Filter out features with variance lower than the specified threshold.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            threshold (float): The minimum variance required for a feature to be retained.\n\n        Returns:\n            List[str]: A list of feature names with variance exceeding the threshold.\n\n        Raises:\n            ValueError: If the threshold is negative.\n        \"\"\"\n        pass\n\n    def rank_features_by_importance(self, X: pd.DataFrame, y: Any) -> List[Tuple[str, float]]:\n        \"\"\"\n        Rank features based on their importance scores derived from a model-based approach.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            y (Any): The target variable.\n            \n        Returns:\n            List[Tuple[str, float]]: A list of tuples where each tuple contains a feature name and \n                                     its corresponding importance score, sorted in descending order.\n\n        Raises:\n            Exception: If feature importance cannot be computed.\n        \"\"\"\n        pass\n\ndef recursive_feature_elimination(X: pd.DataFrame, y: Any, estimator: Any, min_features_to_select: int=1) -> List[str]:\n    \"\"\"\n    Perform recursive feature elimination to select the most important features.\n\n    This function iteratively removes features from the input dataset based on the performance\n    of the provided estimator until the specified minimum number of features is reached.\n    It helps in reducing model complexity and enhancing model generalization.\n\n    Args:\n        X (pd.DataFrame): The input feature matrix with named columns.\n        y (Any): The target variable.\n        estimator (Any): A machine learning estimator that provides feature importance or coefficient attributes.\n        min_features_to_select (int, optional): The minimum number of features to retain. Defaults to 1.\n\n    Returns:\n        List[str]: A list of selected feature names.\n\n    Edge Cases:\n        - The estimator must support feature importance or provide coefficients.\n        - X is expected to have unique column names.\n    \"\"\"\n    pass\n\ndef wrapper_feature_selection(X: pd.DataFrame, y: Any, estimator: Any, cv: int=5) -> List[str]:\n    \"\"\"\n    Perform feature selection using a wrapper method based on model performance.\n\n    This function evaluates different subsets of features using cross-validation with the provided\n    estimator and selects the combination that optimizes the performance metric.\n    \n    Args:\n        X (pd.DataFrame): The input feature matrix.\n        y (Any): The target variable.\n        estimator (Any): A machine learning estimator used to evaluate feature subsets.\n        cv (int, optional): The number of cross-validation folds to use. Defaults to 5.\n\n    Returns:\n        List[str]: A list of feature names selected by the wrapper method.\n\n    Edge Cases:\n        - The estimator must be compatible with cross-validation.\n        - Computational cost may be high for large feature sets.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import List, Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List, Any",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import List, Tuple, Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List, Tuple, Any",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "FilterFeatureSelector",
                                                            "unit_type": "class",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class FilterFeatureSelector:\n    \"\"\"\n    A collection of filter-based feature selection methods.\n\n    This class implements multiple filtering techniques for feature selection such as:\n      - Select with chi-squared tests.\n      - Select with mutual information scores.\n      - Select features with high correlation to the target.\n      - Filter features based on low variance.\n      - Rank features by their computed importance.\n\n    Each method is designed to work with input pandas DataFrames and aligns with the\n    data flow expectations of the system.\n    \"\"\"\n\n    def select_with_chi_squared(self, X: pd.DataFrame, y: Any, k: int) -> List[str]:\n        \"\"\"\n        Select the top 'k' features based on chi-squared test scores.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix with non-negative features.\n            y (Any): The target variable.\n            k (int): The number of top features to select.\n\n        Returns:\n            List[str]: A list of selected feature names based on chi-squared results.\n\n        Raises:\n            ValueError: If X contains negative values or if k is out of valid range.\n        \"\"\"\n        pass\n\n    def select_with_mutual_information(self, X: pd.DataFrame, y: Any, k: int) -> List[str]:\n        \"\"\"\n        Select the top 'k' features based on mutual information scores.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            y (Any): The target variable.\n            k (int): The number of top features to select.\n\n        Returns:\n            List[str]: A list of feature names selected based on mutual information.\n\n        Raises:\n            ValueError: If the mutual information calculation fails.\n        \"\"\"\n        pass\n\n    def select_high_correlation(self, X: pd.DataFrame, y: Any, threshold: float) -> List[str]:\n        \"\"\"\n        Select features that have a correlation with the target variable above a given threshold.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix with named columns.\n            y (Any): The target variable used for correlation computation.\n            threshold (float): The minimum correlation coefficient to consider. \n                               Should be between 0 and 1.\n\n        Returns:\n            List[str]: A list of feature names that exhibit high correlation with the target.\n\n        Raises:\n            ValueError: If threshold is not between 0 and 1.\n        \"\"\"\n        pass\n\n    def select_low_variance(self, X: pd.DataFrame, threshold: float) -> List[str]:\n        \"\"\"\n        Filter out features with variance lower than the specified threshold.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            threshold (float): The minimum variance required for a feature to be retained.\n\n        Returns:\n            List[str]: A list of feature names with variance exceeding the threshold.\n\n        Raises:\n            ValueError: If the threshold is negative.\n        \"\"\"\n        pass\n\n    def rank_features_by_importance(self, X: pd.DataFrame, y: Any) -> List[Tuple[str, float]]:\n        \"\"\"\n        Rank features based on their importance scores derived from a model-based approach.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            y (Any): The target variable.\n            \n        Returns:\n            List[Tuple[str, float]]: A list of tuples where each tuple contains a feature name and \n                                     its corresponding importance score, sorted in descending order.\n\n        Raises:\n            Exception: If feature importance cannot be computed.\n        \"\"\"\n        pass",
                                                            "lineno": 5
                                                        },
                                                        {
                                                            "name": "select_with_chi_squared",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                            "parent": "FilterFeatureSelector",
                                                            "extra": {},
                                                            "code": "def select_with_chi_squared(self, X: pd.DataFrame, y: Any, k: int) -> List[str]:\n    \"\"\"\n        Select the top 'k' features based on chi-squared test scores.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix with non-negative features.\n            y (Any): The target variable.\n            k (int): The number of top features to select.\n\n        Returns:\n            List[str]: A list of selected feature names based on chi-squared results.\n\n        Raises:\n            ValueError: If X contains negative values or if k is out of valid range.\n        \"\"\"\n    pass",
                                                            "lineno": 20
                                                        },
                                                        {
                                                            "name": "select_with_mutual_information",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                            "parent": "FilterFeatureSelector",
                                                            "extra": {},
                                                            "code": "def select_with_mutual_information(self, X: pd.DataFrame, y: Any, k: int) -> List[str]:\n    \"\"\"\n        Select the top 'k' features based on mutual information scores.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            y (Any): The target variable.\n            k (int): The number of top features to select.\n\n        Returns:\n            List[str]: A list of feature names selected based on mutual information.\n\n        Raises:\n            ValueError: If the mutual information calculation fails.\n        \"\"\"\n    pass",
                                                            "lineno": 37
                                                        },
                                                        {
                                                            "name": "select_high_correlation",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                            "parent": "FilterFeatureSelector",
                                                            "extra": {},
                                                            "code": "def select_high_correlation(self, X: pd.DataFrame, y: Any, threshold: float) -> List[str]:\n    \"\"\"\n        Select features that have a correlation with the target variable above a given threshold.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix with named columns.\n            y (Any): The target variable used for correlation computation.\n            threshold (float): The minimum correlation coefficient to consider. \n                               Should be between 0 and 1.\n\n        Returns:\n            List[str]: A list of feature names that exhibit high correlation with the target.\n\n        Raises:\n            ValueError: If threshold is not between 0 and 1.\n        \"\"\"\n    pass",
                                                            "lineno": 54
                                                        },
                                                        {
                                                            "name": "select_low_variance",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                            "parent": "FilterFeatureSelector",
                                                            "extra": {},
                                                            "code": "def select_low_variance(self, X: pd.DataFrame, threshold: float) -> List[str]:\n    \"\"\"\n        Filter out features with variance lower than the specified threshold.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            threshold (float): The minimum variance required for a feature to be retained.\n\n        Returns:\n            List[str]: A list of feature names with variance exceeding the threshold.\n\n        Raises:\n            ValueError: If the threshold is negative.\n        \"\"\"\n    pass",
                                                            "lineno": 72
                                                        },
                                                        {
                                                            "name": "rank_features_by_importance",
                                                            "unit_type": "method",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                            "parent": "FilterFeatureSelector",
                                                            "extra": {},
                                                            "code": "def rank_features_by_importance(self, X: pd.DataFrame, y: Any) -> List[Tuple[str, float]]:\n    \"\"\"\n        Rank features based on their importance scores derived from a model-based approach.\n\n        Args:\n            X (pd.DataFrame): The input feature matrix.\n            y (Any): The target variable.\n            \n        Returns:\n            List[Tuple[str, float]]: A list of tuples where each tuple contains a feature name and \n                                     its corresponding importance score, sorted in descending order.\n\n        Raises:\n            Exception: If feature importance cannot be computed.\n        \"\"\"\n    pass",
                                                            "lineno": 88
                                                        },
                                                        {
                                                            "name": "recursive_feature_elimination",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def recursive_feature_elimination(X: pd.DataFrame, y: Any, estimator: Any, min_features_to_select: int=1) -> List[str]:\n    \"\"\"\n    Perform recursive feature elimination to select the most important features.\n\n    This function iteratively removes features from the input dataset based on the performance\n    of the provided estimator until the specified minimum number of features is reached.\n    It helps in reducing model complexity and enhancing model generalization.\n\n    Args:\n        X (pd.DataFrame): The input feature matrix with named columns.\n        y (Any): The target variable.\n        estimator (Any): A machine learning estimator that provides feature importance or coefficient attributes.\n        min_features_to_select (int, optional): The minimum number of features to retain. Defaults to 1.\n\n    Returns:\n        List[str]: A list of selected feature names.\n\n    Edge Cases:\n        - The estimator must support feature importance or provide coefficients.\n        - X is expected to have unique column names.\n    \"\"\"\n    pass",
                                                            "lineno": 105
                                                        },
                                                        {
                                                            "name": "wrapper_feature_selection",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/feature_engineering/feature_selection.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def wrapper_feature_selection(X: pd.DataFrame, y: Any, estimator: Any, cv: int=5) -> List[str]:\n    \"\"\"\n    Perform feature selection using a wrapper method based on model performance.\n\n    This function evaluates different subsets of features using cross-validation with the provided\n    estimator and selects the combination that optimizes the performance metric.\n    \n    Args:\n        X (pd.DataFrame): The input feature matrix.\n        y (Any): The target variable.\n        estimator (Any): A machine learning estimator used to evaluate feature subsets.\n        cv (int, optional): The number of cross-validation folds to use. Defaults to 5.\n\n    Returns:\n        List[str]: A list of feature names selected by the wrapper method.\n\n    Edge Cases:\n        - The estimator must be compatible with cross-validation.\n        - Computational cost may be high for large feature sets.\n    \"\"\"\n    pass",
                                                            "lineno": 128
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "aggregation",
                                            "path": "./src/data_engineering/data_preparation/aggregation",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "bucketization.py",
                                                    "path": "./src/data_engineering/data_preparation/aggregation/bucketization.py",
                                                    "code": "from typing import List\nimport pandas as pd\n\ndef bucketize_by_quantiles(df: pd.DataFrame, quantiles: List[float]) -> pd.DataFrame:\n    \"\"\"\n    Bucketize the DataFrame by quantiles.\n    \n    This function divides the data in a specified DataFrame column into buckets based on the provided quantile values.\n    It calculates the boundaries corresponding to the quantiles and assigns each row to a bucket derived from these boundaries.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n        quantiles (List[float]): A list of quantile values (ranging from 0 to 1) that determine the bucket boundaries.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column indicating the quantile bucket for each row.\n    \n    Edge Cases:\n        - An empty DataFrame will return an empty DataFrame.\n        - An empty quantiles list will result in no bucketing.\n        - The function assumes that the appropriate data column will be managed externally.\n    \"\"\"\n    pass\n\ndef dynamic_bucketization(df: pd.DataFrame, strategy: str, parameters: dict) -> pd.DataFrame:\n    \"\"\"\n    Dynamically bucketize the DataFrame based on a specified strategy.\n    \n    This function automatically determines bucket boundaries according to the distribution of the data.\n    The 'strategy' argument specifies the dynamic bucketing method to use (e.g., equal frequency or clustering-based),\n    while the 'parameters' dictionary provides any additional configuration needed for that strategy.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n        strategy (str): A string representing the dynamic bucketing strategy to apply.\n        parameters (dict): A dictionary of strategy-specific parameters for bucketization.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column that denotes the dynamic bucket assignment for each row.\n    \n    Edge Cases:\n        - An empty DataFrame will result in an empty DataFrame.\n        - If an unrecognized strategy is provided, the function may default to a no-op bucketing process.\n    \"\"\"\n    pass\n\ndef bucketize_by_range(df: pd.DataFrame, range_start: float, range_end: float, bucket_size: float) -> pd.DataFrame:\n    \"\"\"\n    Bucketize the DataFrame by a defined numerical range.\n    \n    This function segments the data into buckets based on a continuous range defined by a start value, an end value,\n    and a fixed bucket size. It creates equal intervals within the specified range and assigns each data row to its corresponding bucket.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the values to be bucketized.\n        range_start (float): The beginning value of the range.\n        range_end (float): The ending value of the range.\n        bucket_size (float): The fixed width of each bucket within the range.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column indicating the bucket assignment for each row.\n    \n    Edge Cases:\n        - If the range length is not an exact multiple of the bucket_size, the final bucket may be smaller.\n        - An empty DataFrame will return an empty DataFrame.\n        - Values outside the specified range may require special handling or may be excluded.\n    \"\"\"\n    pass\n\ndef bucketize_by_custom_bins(df: pd.DataFrame, bins: List[float]) -> pd.DataFrame:\n    \"\"\"\n    Bucketize the DataFrame using custom bin edges.\n    \n    This function applies user-defined bin edges to classify the data into discrete buckets.\n    It uses the provided list of bin boundaries to determine the bucket for each row in the DataFrame.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n        bins (List[float]): A list of floating-point numbers defining the custom bin edges.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column that indicates the bucket assignment based on the custom bins.\n    \n    Edge Cases:\n        - An empty list of bins will result in no bucketization.\n        - Non-monotonic or overlapping bin values may lead to undefined bucket assignments.\n        - An empty DataFrame will simply return an empty DataFrame.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import List",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/bucketization.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/bucketization.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "bucketize_by_quantiles",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/bucketization.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def bucketize_by_quantiles(df: pd.DataFrame, quantiles: List[float]) -> pd.DataFrame:\n    \"\"\"\n    Bucketize the DataFrame by quantiles.\n    \n    This function divides the data in a specified DataFrame column into buckets based on the provided quantile values.\n    It calculates the boundaries corresponding to the quantiles and assigns each row to a bucket derived from these boundaries.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n        quantiles (List[float]): A list of quantile values (ranging from 0 to 1) that determine the bucket boundaries.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column indicating the quantile bucket for each row.\n    \n    Edge Cases:\n        - An empty DataFrame will return an empty DataFrame.\n        - An empty quantiles list will result in no bucketing.\n        - The function assumes that the appropriate data column will be managed externally.\n    \"\"\"\n    pass",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "dynamic_bucketization",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/bucketization.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def dynamic_bucketization(df: pd.DataFrame, strategy: str, parameters: dict) -> pd.DataFrame:\n    \"\"\"\n    Dynamically bucketize the DataFrame based on a specified strategy.\n    \n    This function automatically determines bucket boundaries according to the distribution of the data.\n    The 'strategy' argument specifies the dynamic bucketing method to use (e.g., equal frequency or clustering-based),\n    while the 'parameters' dictionary provides any additional configuration needed for that strategy.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n        strategy (str): A string representing the dynamic bucketing strategy to apply.\n        parameters (dict): A dictionary of strategy-specific parameters for bucketization.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column that denotes the dynamic bucket assignment for each row.\n    \n    Edge Cases:\n        - An empty DataFrame will result in an empty DataFrame.\n        - If an unrecognized strategy is provided, the function may default to a no-op bucketing process.\n    \"\"\"\n    pass",
                                                            "lineno": 25
                                                        },
                                                        {
                                                            "name": "bucketize_by_range",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/bucketization.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def bucketize_by_range(df: pd.DataFrame, range_start: float, range_end: float, bucket_size: float) -> pd.DataFrame:\n    \"\"\"\n    Bucketize the DataFrame by a defined numerical range.\n    \n    This function segments the data into buckets based on a continuous range defined by a start value, an end value,\n    and a fixed bucket size. It creates equal intervals within the specified range and assigns each data row to its corresponding bucket.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the values to be bucketized.\n        range_start (float): The beginning value of the range.\n        range_end (float): The ending value of the range.\n        bucket_size (float): The fixed width of each bucket within the range.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column indicating the bucket assignment for each row.\n    \n    Edge Cases:\n        - If the range length is not an exact multiple of the bucket_size, the final bucket may be smaller.\n        - An empty DataFrame will return an empty DataFrame.\n        - Values outside the specified range may require special handling or may be excluded.\n    \"\"\"\n    pass",
                                                            "lineno": 47
                                                        },
                                                        {
                                                            "name": "bucketize_by_custom_bins",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/bucketization.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def bucketize_by_custom_bins(df: pd.DataFrame, bins: List[float]) -> pd.DataFrame:\n    \"\"\"\n    Bucketize the DataFrame using custom bin edges.\n    \n    This function applies user-defined bin edges to classify the data into discrete buckets.\n    It uses the provided list of bin boundaries to determine the bucket for each row in the DataFrame.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n        bins (List[float]): A list of floating-point numbers defining the custom bin edges.\n    \n    Returns:\n        pd.DataFrame: A new DataFrame with an added column that indicates the bucket assignment based on the custom bins.\n    \n    Edge Cases:\n        - An empty list of bins will result in no bucketization.\n        - Non-monotonic or overlapping bin values may lead to undefined bucket assignments.\n        - An empty DataFrame will simply return an empty DataFrame.\n    \"\"\"\n    pass",
                                                            "lineno": 70
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "aggregation.py",
                                                    "path": "./src/data_engineering/data_preparation/aggregation/aggregation.py",
                                                    "code": "from typing import List\nfrom typing import List, Optional\nimport pandas as pd\n\ndef aggregate_mean(df: pd.DataFrame, columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Compute the mean value for specified columns in the given DataFrame.\n\n    This function calculates the arithmetic mean of the specified columns.\n    If no columns are provided, it computes the mean for all numerical columns.\n    It is designed to assist in data aggregation profiling by summarizing data trends.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing data to aggregate.\n        columns (List[str], optional): List of column names for which the mean should be computed.\n            If None, mean is computed on all numerical columns.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the mean values for the specified columns.\n\n    Edge Cases:\n        - If the DataFrame is empty, the function will return an empty DataFrame.\n        - If columns are specified but some are not present in the DataFrame, the behavior is undefined.\n    \"\"\"\n    pass\n\ndef aggregate_median(df: pd.DataFrame, columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Compute the median value for specified columns in the given DataFrame.\n\n    This function calculates the median of the provided columns.\n    If columns are not explicitly provided, it calculates the median for every numerical column in the DataFrame.\n    This aggregated insight is useful for understanding the central tendency of data distributions.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing data for aggregation.\n        columns (List[str], optional): List of column names for which the median should be computed.\n            Defaults to None, meaning all numerical columns will be considered.\n\n    Returns:\n        pd.DataFrame: A DataFrame with median values for each specified column.\n\n    Edge Cases:\n        - An empty DataFrame will return an empty DataFrame.\n        - Missing columns specified in 'columns' may lead to unexpected behavior.\n    \"\"\"\n    pass\n\ndef group_and_count(df: pd.DataFrame, group_by: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Group the DataFrame by the specified columns and count the occurrences in each group.\n\n    The function performs a group by operation and returns the count of rows for each group.\n    This can be useful for data profiling purposes to identify the distribution of categorical\n    or discrete feature occurrences within the dataset.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame to be grouped.\n        group_by (List[str]): A list of columns to group the DataFrame by.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the count of rows for each group defined by the group_by columns.\n\n    Edge Cases:\n        - If no rows exist in the DataFrame, an empty DataFrame is returned.\n        - If group_by columns contain null values, the count might include these as a separate group.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import List",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/aggregation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import List, Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/aggregation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List, Optional",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/aggregation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "aggregate_mean",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/aggregation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def aggregate_mean(df: pd.DataFrame, columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Compute the mean value for specified columns in the given DataFrame.\n\n    This function calculates the arithmetic mean of the specified columns.\n    If no columns are provided, it computes the mean for all numerical columns.\n    It is designed to assist in data aggregation profiling by summarizing data trends.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing data to aggregate.\n        columns (List[str], optional): List of column names for which the mean should be computed.\n            If None, mean is computed on all numerical columns.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the mean values for the specified columns.\n\n    Edge Cases:\n        - If the DataFrame is empty, the function will return an empty DataFrame.\n        - If columns are specified but some are not present in the DataFrame, the behavior is undefined.\n    \"\"\"\n    pass",
                                                            "lineno": 5
                                                        },
                                                        {
                                                            "name": "aggregate_median",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/aggregation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def aggregate_median(df: pd.DataFrame, columns: Optional[List[str]]=None) -> pd.DataFrame:\n    \"\"\"\n    Compute the median value for specified columns in the given DataFrame.\n\n    This function calculates the median of the provided columns.\n    If columns are not explicitly provided, it calculates the median for every numerical column in the DataFrame.\n    This aggregated insight is useful for understanding the central tendency of data distributions.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing data for aggregation.\n        columns (List[str], optional): List of column names for which the median should be computed.\n            Defaults to None, meaning all numerical columns will be considered.\n\n    Returns:\n        pd.DataFrame: A DataFrame with median values for each specified column.\n\n    Edge Cases:\n        - An empty DataFrame will return an empty DataFrame.\n        - Missing columns specified in 'columns' may lead to unexpected behavior.\n    \"\"\"\n    pass",
                                                            "lineno": 27
                                                        },
                                                        {
                                                            "name": "group_and_count",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/aggregation.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def group_and_count(df: pd.DataFrame, group_by: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Group the DataFrame by the specified columns and count the occurrences in each group.\n\n    The function performs a group by operation and returns the count of rows for each group.\n    This can be useful for data profiling purposes to identify the distribution of categorical\n    or discrete feature occurrences within the dataset.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame to be grouped.\n        group_by (List[str]): A list of columns to group the DataFrame by.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the count of rows for each group defined by the group_by columns.\n\n    Edge Cases:\n        - If no rows exist in the DataFrame, an empty DataFrame is returned.\n        - If group_by columns contain null values, the count might include these as a separate group.\n    \"\"\"\n    pass",
                                                            "lineno": 49
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "zero_column.py",
                                                    "path": "./src/data_engineering/data_preparation/aggregation/zero_column.py",
                                                    "code": "import pandas as pd\nfrom typing import Optional\n\ndef drop_columns_with_mostly_zeros(df: pd.DataFrame, threshold: float=0.8) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from the DataFrame that contain mostly zeros.\n    \n    This function examines each column in the input DataFrame and drops those columns where\n    the proportion of zero values exceeds the specified threshold, indicating that the column\n    is mostly populated with zeros. This is useful as a preprocessing step for cleaning data,\n    particularly in scenarios where such columns may not contribute meaningful information.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be processed.\n        threshold (float, optional): The proportion of zero values in a column above which\n            the column will be dropped. The value should be in the range (0, 1]. Default is 0.8,\n            meaning columns with more than 80% zeros will be removed.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with columns that have mostly zeros removed.\n\n    Edge Cases:\n        - If the DataFrame is empty, the function returns an empty DataFrame.\n        - If no column meets the dropping criteria, the original DataFrame is returned.\n    \"\"\"\n    pass\n\ndef drop_columns_with_zero_variance(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from the DataFrame that have zero variance.\n    \n    This function identifies columns in the input DataFrame where all values are identical\n    (i.e., the variance is zero) and removes them. Such columns typically do not provide\n    useful discriminatory information in further data analysis or modeling, and hence,\n    their removal can help in reducing dimensionality.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be processed.\n    \n    Returns:\n        pd.DataFrame: A modified DataFrame with columns of zero variance removed.\n\n    Edge Cases:\n        - If the DataFrame is empty, the function returns an empty DataFrame.\n        - If no columns have zero variance, the original DataFrame is returned unchanged.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/zero_column.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/zero_column.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "drop_columns_with_mostly_zeros",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/zero_column.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def drop_columns_with_mostly_zeros(df: pd.DataFrame, threshold: float=0.8) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from the DataFrame that contain mostly zeros.\n    \n    This function examines each column in the input DataFrame and drops those columns where\n    the proportion of zero values exceeds the specified threshold, indicating that the column\n    is mostly populated with zeros. This is useful as a preprocessing step for cleaning data,\n    particularly in scenarios where such columns may not contribute meaningful information.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to be processed.\n        threshold (float, optional): The proportion of zero values in a column above which\n            the column will be dropped. The value should be in the range (0, 1]. Default is 0.8,\n            meaning columns with more than 80% zeros will be removed.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with columns that have mostly zeros removed.\n\n    Edge Cases:\n        - If the DataFrame is empty, the function returns an empty DataFrame.\n        - If no column meets the dropping criteria, the original DataFrame is returned.\n    \"\"\"\n    pass",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "drop_columns_with_zero_variance",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/zero_column.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def drop_columns_with_zero_variance(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Drop columns from the DataFrame that have zero variance.\n    \n    This function identifies columns in the input DataFrame where all values are identical\n    (i.e., the variance is zero) and removes them. Such columns typically do not provide\n    useful discriminatory information in further data analysis or modeling, and hence,\n    their removal can help in reducing dimensionality.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be processed.\n    \n    Returns:\n        pd.DataFrame: A modified DataFrame with columns of zero variance removed.\n\n    Edge Cases:\n        - If the DataFrame is empty, the function returns an empty DataFrame.\n        - If no columns have zero variance, the original DataFrame is returned unchanged.\n    \"\"\"\n    pass",
                                                            "lineno": 28
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "profiling.py",
                                                    "path": "./src/data_engineering/data_preparation/aggregation/profiling.py",
                                                    "code": "from typing import Dict, Any\nimport pandas as pd\n\ndef analyze_correlation(df: pd.DataFrame, method: str='pearson') -> Dict[str, Any]:\n    \"\"\"\n    Perform a correlation analysis on the provided DataFrame.\n\n    This function computes the correlation matrix for the numerical features in the DataFrame using the specified\n    correlation method (e.g., 'pearson', 'spearman', or 'kendall'). It returns a dictionary containing the correlation\n    matrix and additional insights such as pairs of features with high correlations. This analysis helps in data profiling\n    by identifying relationships between features.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing numerical features to analyze.\n        method (str, optional): The correlation method to use; valid values include 'pearson', 'spearman', and 'kendall'.\n                                Defaults to 'pearson'.\n\n    Returns:\n        Dict[str, Any]: A dictionary with the following keys:\n            - 'correlation_matrix': A DataFrame representing the computed correlation matrix.\n            - 'highly_correlated_pairs': A list of tuples indicating feature pairs with high correlation.\n            - 'summary': A summary of the correlation analysis providing further insights.\n\n    Edge Cases:\n        - If the DataFrame is empty or lacks sufficient numerical data, the function should return an empty dictionary.\n        - The function assumes that inputs have appropriate numeric data types for correlation calculation.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import Dict, Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/profiling.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Dict, Any",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/profiling.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "analyze_correlation",
                                                            "unit_type": "function",
                                                            "file_path": "./src/data_engineering/data_preparation/aggregation/profiling.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def analyze_correlation(df: pd.DataFrame, method: str='pearson') -> Dict[str, Any]:\n    \"\"\"\n    Perform a correlation analysis on the provided DataFrame.\n\n    This function computes the correlation matrix for the numerical features in the DataFrame using the specified\n    correlation method (e.g., 'pearson', 'spearman', or 'kendall'). It returns a dictionary containing the correlation\n    matrix and additional insights such as pairs of features with high correlations. This analysis helps in data profiling\n    by identifying relationships between features.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing numerical features to analyze.\n        method (str, optional): The correlation method to use; valid values include 'pearson', 'spearman', and 'kendall'.\n                                Defaults to 'pearson'.\n\n    Returns:\n        Dict[str, Any]: A dictionary with the following keys:\n            - 'correlation_matrix': A DataFrame representing the computed correlation matrix.\n            - 'highly_correlated_pairs': A list of tuples indicating feature pairs with high correlation.\n            - 'summary': A summary of the correlation analysis providing further insights.\n\n    Edge Cases:\n        - If the DataFrame is empty or lacks sufficient numerical data, the function should return an empty dictionary.\n        - The function assumes that inputs have appropriate numeric data types for correlation calculation.\n    \"\"\"\n    pass",
                                                            "lineno": 4
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "data_encoding",
                                    "path": "./src/data_engineering/data_encoding",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "label_encoding.py",
                                            "path": "./src/data_engineering/data_encoding/label_encoding.py",
                                            "code": "import pandas as pd\n\ndef frequency_based_label_encoding(df: pd.DataFrame, column: str) -> pd.DataFrame:\n    \"\"\"\n    Perform frequency-based label encoding on a specified column of a DataFrame.\n    \n    This function converts categorical labels in the provided column into numerical labels based\n    on the frequency of each unique category. Categories that appear more frequently are assigned lower\n    numerical values. This type of encoding is useful when the frequency order carries predictive significance.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the target column.\n        column (str): The name of the column containing categorical labels to encode.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with the specified column transformed into frequency-based numerical labels.\n    \n    Notes:\n        - The function assumes that the specified column contains categorical data.\n        - Handling of missing values or unseen categories must be considered in the implementation.\n    \"\"\"\n    pass\n\ndef ordinal_label_encoding(df: pd.DataFrame, column: str, categories: list) -> pd.DataFrame:\n    \"\"\"\n    Apply ordinal label encoding to a specified column of a DataFrame.\n    \n    This function maps categorical labels in the given column to integer values that reflect a user-specified\n    order. The order is provided by the 'categories' list, where the position of a category in the list\n    determines its ordinal value. This encoding is appropriate when there is an inherent ranking among the categories.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the target column.\n        column (str): The name of the column containing categorical labels to encode.\n        categories (list): A list of categories defining the desired order for ordinal encoding.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with the specified column transformed using ordinal label encoding.\n    \n    Notes:\n        - The function assumes that the provided 'categories' list covers all possible labels in the column.\n        - Users must ensure that the order specified in the 'categories' list accurately reflects the intended ranking.\n        - Behavior for handling missing or unexpected labels should be defined as needed.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_encoding/label_encoding.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "frequency_based_label_encoding",
                                                    "unit_type": "function",
                                                    "file_path": "./src/data_engineering/data_encoding/label_encoding.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def frequency_based_label_encoding(df: pd.DataFrame, column: str) -> pd.DataFrame:\n    \"\"\"\n    Perform frequency-based label encoding on a specified column of a DataFrame.\n    \n    This function converts categorical labels in the provided column into numerical labels based\n    on the frequency of each unique category. Categories that appear more frequently are assigned lower\n    numerical values. This type of encoding is useful when the frequency order carries predictive significance.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the target column.\n        column (str): The name of the column containing categorical labels to encode.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with the specified column transformed into frequency-based numerical labels.\n    \n    Notes:\n        - The function assumes that the specified column contains categorical data.\n        - Handling of missing values or unseen categories must be considered in the implementation.\n    \"\"\"\n    pass",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "ordinal_label_encoding",
                                                    "unit_type": "function",
                                                    "file_path": "./src/data_engineering/data_encoding/label_encoding.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def ordinal_label_encoding(df: pd.DataFrame, column: str, categories: list) -> pd.DataFrame:\n    \"\"\"\n    Apply ordinal label encoding to a specified column of a DataFrame.\n    \n    This function maps categorical labels in the given column to integer values that reflect a user-specified\n    order. The order is provided by the 'categories' list, where the position of a category in the list\n    determines its ordinal value. This encoding is appropriate when there is an inherent ranking among the categories.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame containing the target column.\n        column (str): The name of the column containing categorical labels to encode.\n        categories (list): A list of categories defining the desired order for ordinal encoding.\n    \n    Returns:\n        pd.DataFrame: A DataFrame with the specified column transformed using ordinal label encoding.\n    \n    Notes:\n        - The function assumes that the provided 'categories' list covers all possible labels in the column.\n        - Users must ensure that the order specified in the 'categories' list accurately reflects the intended ranking.\n        - Behavior for handling missing or unexpected labels should be defined as needed.\n    \"\"\"\n    pass",
                                                    "lineno": 24
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "onehot_encoding.py",
                                            "path": "./src/data_engineering/data_encoding/onehot_encoding.py",
                                            "code": "import pandas as pd\nfrom scipy import sparse\nfrom typing import List, Optional, Union\n\nclass CombinedOneHotEncoder:\n    \"\"\"\n    A combined one-hot encoder that supports both dense one-hot encoding and sparse matrix optimization.\n\n    This class provides an interface for encoding categorical features into a one-hot representation.\n    Users can choose to produce a dense output (a pandas DataFrame) or an optimized sparse matrix,\n    depending on the requirements for memory and computational efficiency.\n\n    Attributes:\n        sparse (bool): Determines the output format. If True, the transform method returns a sparse \n                       matrix optimized for performance. If False, it returns a dense pandas DataFrame.\n    \n    Methods:\n        fit(X: pd.DataFrame, columns: Optional[List[str]] = None) -> CombinedOneHotEncoder:\n            Fit the encoder on input data to learn the unique categories from specified columns.\n        \n        transform(X: pd.DataFrame) -> Union[pd.DataFrame, sparse.csr_matrix]:\n            Transform the input DataFrame into one-hot encoded format, producing either a dense DataFrame\n            or a sparse matrix based on the encoder configuration.\n        \n        fit_transform(X: pd.DataFrame, columns: Optional[List[str]] = None) -> Union[pd.DataFrame, sparse.csr_matrix]:\n            Fit the encoder and transform the data in one step.\n    \n    Args:\n        sparse (bool): A flag to determine whether to use sparse matrix optimization (True) or dense \n                       representation (False). Default is False.\n    \n    Returns:\n        An instance of CombinedOneHotEncoder.\n\n    Edge Cases and Constraints:\n        - The input DataFrame should contain the categorical columns to be encoded.\n        - If columns is None, the encoder may attempt to infer the categorical columns from the data.\n        - The transform method must handle cases where unknown categories are encountered gracefully.\n        - This interface does not implement the actual encoding logic; it provides the necessary structure.\n    \"\"\"\n\n    def __init__(self, sparse: bool=False):\n        self.sparse = sparse\n        pass\n\n    def fit(self, X: pd.DataFrame, columns: Optional[List[str]]=None) -> 'CombinedOneHotEncoder':\n        \"\"\"\n        Fit the encoder to the input DataFrame by learning the unique categories from specified columns.\n\n        Args:\n            X (pd.DataFrame): The input data containing categorical features.\n            columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, the encoder\n                                           may attempt to infer categorical columns automatically.\n\n        Returns:\n            CombinedOneHotEncoder: The fitted encoder instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> Union[pd.DataFrame, sparse.csr_matrix]:\n        \"\"\"\n        Transform the input DataFrame into a one-hot encoded format.\n\n        Based on the encoder's configuration, the output will either be a dense pandas DataFrame or \n        an optimized sparse matrix.\n\n        Args:\n            X (pd.DataFrame): The data to transform.\n\n        Returns:\n            Union[pd.DataFrame, sparse.csr_matrix]: The one-hot encoded representation of the input data.\n        \"\"\"\n        pass\n\n    def fit_transform(self, X: pd.DataFrame, columns: Optional[List[str]]=None) -> Union[pd.DataFrame, sparse.csr_matrix]:\n        \"\"\"\n        Fit the encoder to the data and then transform it into one-hot encoded format.\n\n        Args:\n            X (pd.DataFrame): The input data to fit and transform.\n            columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, it may infer columns.\n\n        Returns:\n            Union[pd.DataFrame, sparse.csr_matrix]: The transformed one-hot encoded data.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_encoding/onehot_encoding.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "from scipy import sparse",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_encoding/onehot_encoding.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from scipy import sparse",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "from typing import List, Optional, Union",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_encoding/onehot_encoding.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import List, Optional, Union",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "CombinedOneHotEncoder",
                                                    "unit_type": "class",
                                                    "file_path": "./src/data_engineering/data_encoding/onehot_encoding.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class CombinedOneHotEncoder:\n    \"\"\"\n    A combined one-hot encoder that supports both dense one-hot encoding and sparse matrix optimization.\n\n    This class provides an interface for encoding categorical features into a one-hot representation.\n    Users can choose to produce a dense output (a pandas DataFrame) or an optimized sparse matrix,\n    depending on the requirements for memory and computational efficiency.\n\n    Attributes:\n        sparse (bool): Determines the output format. If True, the transform method returns a sparse \n                       matrix optimized for performance. If False, it returns a dense pandas DataFrame.\n    \n    Methods:\n        fit(X: pd.DataFrame, columns: Optional[List[str]] = None) -> CombinedOneHotEncoder:\n            Fit the encoder on input data to learn the unique categories from specified columns.\n        \n        transform(X: pd.DataFrame) -> Union[pd.DataFrame, sparse.csr_matrix]:\n            Transform the input DataFrame into one-hot encoded format, producing either a dense DataFrame\n            or a sparse matrix based on the encoder configuration.\n        \n        fit_transform(X: pd.DataFrame, columns: Optional[List[str]] = None) -> Union[pd.DataFrame, sparse.csr_matrix]:\n            Fit the encoder and transform the data in one step.\n    \n    Args:\n        sparse (bool): A flag to determine whether to use sparse matrix optimization (True) or dense \n                       representation (False). Default is False.\n    \n    Returns:\n        An instance of CombinedOneHotEncoder.\n\n    Edge Cases and Constraints:\n        - The input DataFrame should contain the categorical columns to be encoded.\n        - If columns is None, the encoder may attempt to infer the categorical columns from the data.\n        - The transform method must handle cases where unknown categories are encountered gracefully.\n        - This interface does not implement the actual encoding logic; it provides the necessary structure.\n    \"\"\"\n\n    def __init__(self, sparse: bool=False):\n        self.sparse = sparse\n        pass\n\n    def fit(self, X: pd.DataFrame, columns: Optional[List[str]]=None) -> 'CombinedOneHotEncoder':\n        \"\"\"\n        Fit the encoder to the input DataFrame by learning the unique categories from specified columns.\n\n        Args:\n            X (pd.DataFrame): The input data containing categorical features.\n            columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, the encoder\n                                           may attempt to infer categorical columns automatically.\n\n        Returns:\n            CombinedOneHotEncoder: The fitted encoder instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> Union[pd.DataFrame, sparse.csr_matrix]:\n        \"\"\"\n        Transform the input DataFrame into a one-hot encoded format.\n\n        Based on the encoder's configuration, the output will either be a dense pandas DataFrame or \n        an optimized sparse matrix.\n\n        Args:\n            X (pd.DataFrame): The data to transform.\n\n        Returns:\n            Union[pd.DataFrame, sparse.csr_matrix]: The one-hot encoded representation of the input data.\n        \"\"\"\n        pass\n\n    def fit_transform(self, X: pd.DataFrame, columns: Optional[List[str]]=None) -> Union[pd.DataFrame, sparse.csr_matrix]:\n        \"\"\"\n        Fit the encoder to the data and then transform it into one-hot encoded format.\n\n        Args:\n            X (pd.DataFrame): The input data to fit and transform.\n            columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, it may infer columns.\n\n        Returns:\n            Union[pd.DataFrame, sparse.csr_matrix]: The transformed one-hot encoded data.\n        \"\"\"\n        pass",
                                                    "lineno": 5
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_encoding/onehot_encoding.py",
                                                    "parent": "CombinedOneHotEncoder",
                                                    "extra": {},
                                                    "code": "def __init__(self, sparse: bool=False):\n    self.sparse = sparse\n    pass",
                                                    "lineno": 42
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_encoding/onehot_encoding.py",
                                                    "parent": "CombinedOneHotEncoder",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame, columns: Optional[List[str]]=None) -> 'CombinedOneHotEncoder':\n    \"\"\"\n        Fit the encoder to the input DataFrame by learning the unique categories from specified columns.\n\n        Args:\n            X (pd.DataFrame): The input data containing categorical features.\n            columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, the encoder\n                                           may attempt to infer categorical columns automatically.\n\n        Returns:\n            CombinedOneHotEncoder: The fitted encoder instance.\n        \"\"\"\n    pass",
                                                    "lineno": 46
                                                },
                                                {
                                                    "name": "transform",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_encoding/onehot_encoding.py",
                                                    "parent": "CombinedOneHotEncoder",
                                                    "extra": {},
                                                    "code": "def transform(self, X: pd.DataFrame) -> Union[pd.DataFrame, sparse.csr_matrix]:\n    \"\"\"\n        Transform the input DataFrame into a one-hot encoded format.\n\n        Based on the encoder's configuration, the output will either be a dense pandas DataFrame or \n        an optimized sparse matrix.\n\n        Args:\n            X (pd.DataFrame): The data to transform.\n\n        Returns:\n            Union[pd.DataFrame, sparse.csr_matrix]: The one-hot encoded representation of the input data.\n        \"\"\"\n    pass",
                                                    "lineno": 60
                                                },
                                                {
                                                    "name": "fit_transform",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_encoding/onehot_encoding.py",
                                                    "parent": "CombinedOneHotEncoder",
                                                    "extra": {},
                                                    "code": "def fit_transform(self, X: pd.DataFrame, columns: Optional[List[str]]=None) -> Union[pd.DataFrame, sparse.csr_matrix]:\n    \"\"\"\n        Fit the encoder to the data and then transform it into one-hot encoded format.\n\n        Args:\n            X (pd.DataFrame): The input data to fit and transform.\n            columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, it may infer columns.\n\n        Returns:\n            Union[pd.DataFrame, sparse.csr_matrix]: The transformed one-hot encoded data.\n        \"\"\"\n    pass",
                                                    "lineno": 75
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "binary_encoding.py",
                                            "path": "./src/data_engineering/data_encoding/binary_encoding.py",
                                            "code": "from typing import Optional\nimport pandas as pd\n\ndef compress_binary_encoding(df: pd.DataFrame, column: str) -> pd.DataFrame:\n    \"\"\"\n    Apply compressed binary encoding to the specified column of a DataFrame.\n\n    This function transforms the binary encoding of a given column in the DataFrame\n    by applying compression techniques. It is intended to reduce storage overhead while\n    preserving the binary nature of the column's data. The function assumes the input column \n    contains data that can be represented in a binary format and performs a compression\n    operation that is tailored to binary encoded data.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the binary-encoded column.\n        column (str): The name of the column in the DataFrame to be compressed.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the specified column processed through compressed\n                      binary encoding. All other columns remain unchanged.\n\n    Edge Cases:\n        - If the specified column does not exist, the function should handle the situation appropriately.\n        - The function assumes that the column's data is already in a binary encodable format.\n        - No in-place modification is performed; a new DataFrame is returned.\n\n    Note:\n        This is a stub interface. Implementation logic is to be provided later.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "from typing import Optional",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_encoding/binary_encoding.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Optional",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_encoding/binary_encoding.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "compress_binary_encoding",
                                                    "unit_type": "function",
                                                    "file_path": "./src/data_engineering/data_encoding/binary_encoding.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def compress_binary_encoding(df: pd.DataFrame, column: str) -> pd.DataFrame:\n    \"\"\"\n    Apply compressed binary encoding to the specified column of a DataFrame.\n\n    This function transforms the binary encoding of a given column in the DataFrame\n    by applying compression techniques. It is intended to reduce storage overhead while\n    preserving the binary nature of the column's data. The function assumes the input column \n    contains data that can be represented in a binary format and performs a compression\n    operation that is tailored to binary encoded data.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the binary-encoded column.\n        column (str): The name of the column in the DataFrame to be compressed.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the specified column processed through compressed\n                      binary encoding. All other columns remain unchanged.\n\n    Edge Cases:\n        - If the specified column does not exist, the function should handle the situation appropriately.\n        - The function assumes that the column's data is already in a binary encodable format.\n        - No in-place modification is performed; a new DataFrame is returned.\n\n    Note:\n        This is a stub interface. Implementation logic is to be provided later.\n    \"\"\"\n    pass",
                                                    "lineno": 4
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "data_splitting",
                                    "path": "./src/data_engineering/data_splitting",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "time_based.py",
                                            "path": "./src/data_engineering/data_splitting/time_based.py",
                                            "code": "import pandas as pd\n\nclass TimeBasedSplitter:\n    \"\"\"\n    A splitter for performing time-based data splits according to different temporal granularities.\n\n    This class encapsulates multiple strategies to split a pandas DataFrame based on time attributes.\n    It provides methods to split the data by time zones, month, hour, day, and year.\n    \n    Each method expects a time-indexed pandas DataFrame and a specific column name holding the relevant time information.\n    The returned structure is a dictionary where the keys represent the extracted time units and the values are the corresponding\n    segments of the DataFrame.\n\n    Methods:\n        split_by_timezones(df: pd.DataFrame, timezone_column: str) -> dict:\n            Splits data based on time zone values.\n        split_by_month(df: pd.DataFrame, month_column: str) -> dict:\n            Splits data based on the month extracted from a datetime column.\n        split_by_hour(df: pd.DataFrame, hour_column: str) -> dict:\n            Splits data based on the hour extracted from a datetime column.\n        split_by_day(df: pd.DataFrame, day_column: str) -> dict:\n            Splits data based on the day extracted from a datetime column.\n        split_by_year(df: pd.DataFrame, year_column: str) -> dict:\n            Splits data based on the year extracted from a datetime column.\n    \n    Note:\n        - No splitting logic is implemented in this interface definition.\n        - The client is responsible for verifying that the DataFrame contains the required columns with appropriate data formats.\n    \"\"\"\n\n    def split_by_timezones(self, df: pd.DataFrame, timezone_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame based on time zone information provided in the specified column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            timezone_column (str): The name of the column with time zone data.\n        \n        Returns:\n            dict: A dictionary where keys are time zone identifiers and values are the corresponding DataFrame segments.\n        \"\"\"\n        pass\n\n    def split_by_month(self, df: pd.DataFrame, month_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the month extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            month_column (str): The name of the column from which the month information will be derived.\n        \n        Returns:\n            dict: A dictionary with months as keys and DataFrame segments as values.\n        \"\"\"\n        pass\n\n    def split_by_hour(self, df: pd.DataFrame, hour_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the hour extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            hour_column (str): The name of the column from which the hour information will be derived.\n        \n        Returns:\n            dict: A dictionary with hours as keys and corresponding DataFrame segments as values.\n        \"\"\"\n        pass\n\n    def split_by_day(self, df: pd.DataFrame, day_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the day extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            day_column (str): The name of the column from which day information will be derived.\n        \n        Returns:\n            dict: A dictionary with days as keys and corresponding DataFrame segments as values.\n        \"\"\"\n        pass\n\n    def split_by_year(self, df: pd.DataFrame, year_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the year extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            year_column (str): The name of the column from which year information will be derived.\n        \n        Returns:\n            dict: A dictionary with years as keys and corresponding DataFrame segments as values.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_splitting/time_based.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "TimeBasedSplitter",
                                                    "unit_type": "class",
                                                    "file_path": "./src/data_engineering/data_splitting/time_based.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class TimeBasedSplitter:\n    \"\"\"\n    A splitter for performing time-based data splits according to different temporal granularities.\n\n    This class encapsulates multiple strategies to split a pandas DataFrame based on time attributes.\n    It provides methods to split the data by time zones, month, hour, day, and year.\n    \n    Each method expects a time-indexed pandas DataFrame and a specific column name holding the relevant time information.\n    The returned structure is a dictionary where the keys represent the extracted time units and the values are the corresponding\n    segments of the DataFrame.\n\n    Methods:\n        split_by_timezones(df: pd.DataFrame, timezone_column: str) -> dict:\n            Splits data based on time zone values.\n        split_by_month(df: pd.DataFrame, month_column: str) -> dict:\n            Splits data based on the month extracted from a datetime column.\n        split_by_hour(df: pd.DataFrame, hour_column: str) -> dict:\n            Splits data based on the hour extracted from a datetime column.\n        split_by_day(df: pd.DataFrame, day_column: str) -> dict:\n            Splits data based on the day extracted from a datetime column.\n        split_by_year(df: pd.DataFrame, year_column: str) -> dict:\n            Splits data based on the year extracted from a datetime column.\n    \n    Note:\n        - No splitting logic is implemented in this interface definition.\n        - The client is responsible for verifying that the DataFrame contains the required columns with appropriate data formats.\n    \"\"\"\n\n    def split_by_timezones(self, df: pd.DataFrame, timezone_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame based on time zone information provided in the specified column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            timezone_column (str): The name of the column with time zone data.\n        \n        Returns:\n            dict: A dictionary where keys are time zone identifiers and values are the corresponding DataFrame segments.\n        \"\"\"\n        pass\n\n    def split_by_month(self, df: pd.DataFrame, month_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the month extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            month_column (str): The name of the column from which the month information will be derived.\n        \n        Returns:\n            dict: A dictionary with months as keys and DataFrame segments as values.\n        \"\"\"\n        pass\n\n    def split_by_hour(self, df: pd.DataFrame, hour_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the hour extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            hour_column (str): The name of the column from which the hour information will be derived.\n        \n        Returns:\n            dict: A dictionary with hours as keys and corresponding DataFrame segments as values.\n        \"\"\"\n        pass\n\n    def split_by_day(self, df: pd.DataFrame, day_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the day extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            day_column (str): The name of the column from which day information will be derived.\n        \n        Returns:\n            dict: A dictionary with days as keys and corresponding DataFrame segments as values.\n        \"\"\"\n        pass\n\n    def split_by_year(self, df: pd.DataFrame, year_column: str) -> dict:\n        \"\"\"\n        Split the DataFrame into groups based on the year extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            year_column (str): The name of the column from which year information will be derived.\n        \n        Returns:\n            dict: A dictionary with years as keys and corresponding DataFrame segments as values.\n        \"\"\"\n        pass",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "split_by_timezones",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/time_based.py",
                                                    "parent": "TimeBasedSplitter",
                                                    "extra": {},
                                                    "code": "def split_by_timezones(self, df: pd.DataFrame, timezone_column: str) -> dict:\n    \"\"\"\n        Split the DataFrame based on time zone information provided in the specified column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            timezone_column (str): The name of the column with time zone data.\n        \n        Returns:\n            dict: A dictionary where keys are time zone identifiers and values are the corresponding DataFrame segments.\n        \"\"\"\n    pass",
                                                    "lineno": 31
                                                },
                                                {
                                                    "name": "split_by_month",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/time_based.py",
                                                    "parent": "TimeBasedSplitter",
                                                    "extra": {},
                                                    "code": "def split_by_month(self, df: pd.DataFrame, month_column: str) -> dict:\n    \"\"\"\n        Split the DataFrame into groups based on the month extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            month_column (str): The name of the column from which the month information will be derived.\n        \n        Returns:\n            dict: A dictionary with months as keys and DataFrame segments as values.\n        \"\"\"\n    pass",
                                                    "lineno": 44
                                                },
                                                {
                                                    "name": "split_by_hour",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/time_based.py",
                                                    "parent": "TimeBasedSplitter",
                                                    "extra": {},
                                                    "code": "def split_by_hour(self, df: pd.DataFrame, hour_column: str) -> dict:\n    \"\"\"\n        Split the DataFrame into groups based on the hour extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            hour_column (str): The name of the column from which the hour information will be derived.\n        \n        Returns:\n            dict: A dictionary with hours as keys and corresponding DataFrame segments as values.\n        \"\"\"\n    pass",
                                                    "lineno": 57
                                                },
                                                {
                                                    "name": "split_by_day",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/time_based.py",
                                                    "parent": "TimeBasedSplitter",
                                                    "extra": {},
                                                    "code": "def split_by_day(self, df: pd.DataFrame, day_column: str) -> dict:\n    \"\"\"\n        Split the DataFrame into groups based on the day extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            day_column (str): The name of the column from which day information will be derived.\n        \n        Returns:\n            dict: A dictionary with days as keys and corresponding DataFrame segments as values.\n        \"\"\"\n    pass",
                                                    "lineno": 70
                                                },
                                                {
                                                    "name": "split_by_year",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/time_based.py",
                                                    "parent": "TimeBasedSplitter",
                                                    "extra": {},
                                                    "code": "def split_by_year(self, df: pd.DataFrame, year_column: str) -> dict:\n    \"\"\"\n        Split the DataFrame into groups based on the year extracted from a datetime column.\n        \n        Args:\n            df (pd.DataFrame): The pandas DataFrame containing time-based data.\n            year_column (str): The name of the column from which year information will be derived.\n        \n        Returns:\n            dict: A dictionary with years as keys and corresponding DataFrame segments as values.\n        \"\"\"\n    pass",
                                                    "lineno": 83
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "event_based.py",
                                            "path": "./src/data_engineering/data_splitting/event_based.py",
                                            "code": "import pandas as pd\nfrom typing import Dict\n\nclass EventBasedSplitter:\n    \"\"\"\n    A class that provides various methods for splitting a DataFrame based on event-related criteria.\n    \n    This class encapsulates three event-based splitting strategies:\n    1. Splitting the dataset based on event count.\n    2. Splitting the dataset based on event frequency.\n    3. Splitting the dataset based on event types.\n    \n    Each method takes a DataFrame and relevant parameters to determine how the data is segmented.\n    \n    Methods:\n        split_by_event_count: Splits the DataFrame based on the count of events.\n        split_by_event_frequency: Splits the DataFrame based on the frequency of events.\n        split_by_event_types: Splits the DataFrame based on event type categorization.\n    \"\"\"\n\n    def split_by_event_count(self, df: pd.DataFrame, event_count_column: str, count_threshold: int) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Splits the DataFrame based on the count of events recorded in a specific column.\n        \n        This method divides the input DataFrame into multiple segments based on a threshold value applied \n        to the event count column. Different segments may reflect varying event participation quantities.\n        \n        Args:\n            df (pd.DataFrame): The input data containing events.\n            event_count_column (str): The column name that holds the event count.\n            count_threshold (int): The threshold value used to determine splitting logic.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary where keys represent split segment identifiers and values \n            are the corresponding DataFrame segments.\n        \"\"\"\n        pass\n\n    def split_by_event_frequency(self, df: pd.DataFrame, frequency_column: str, frequency_threshold: float) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Splits the DataFrame based on the frequency of events observed in a given column.\n        \n        The method segments the DataFrame into multiple groups based on whether the event frequency \n        exceeds or falls below a given threshold. This is useful for categorizing events based on how \n        often they occur.\n        \n        Args:\n            df (pd.DataFrame): The input DataFrame containing event frequency information.\n            frequency_column (str): The column name that contains the event frequency data.\n            frequency_threshold (float): The threshold frequency to determine how the dataset is split.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary mapping segment names to their corresponding DataFrame splits.\n        \"\"\"\n        pass\n\n    def split_by_event_types(self, df: pd.DataFrame, event_type_column: str) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Splits the DataFrame based on distinct event types as specified in a given column.\n        \n        This method categorizes the input data into multiple segments, each corresponding to a unique\n        event type found in the specified column. It facilitates analysis per event category.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing event data.\n            event_type_column (str): The column name that classifies the types of events.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary where each key is an event type and the value is the \n            subset of the DataFrame corresponding to that event type.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_splitting/event_based.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "from typing import Dict",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_splitting/event_based.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Dict",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "EventBasedSplitter",
                                                    "unit_type": "class",
                                                    "file_path": "./src/data_engineering/data_splitting/event_based.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class EventBasedSplitter:\n    \"\"\"\n    A class that provides various methods for splitting a DataFrame based on event-related criteria.\n    \n    This class encapsulates three event-based splitting strategies:\n    1. Splitting the dataset based on event count.\n    2. Splitting the dataset based on event frequency.\n    3. Splitting the dataset based on event types.\n    \n    Each method takes a DataFrame and relevant parameters to determine how the data is segmented.\n    \n    Methods:\n        split_by_event_count: Splits the DataFrame based on the count of events.\n        split_by_event_frequency: Splits the DataFrame based on the frequency of events.\n        split_by_event_types: Splits the DataFrame based on event type categorization.\n    \"\"\"\n\n    def split_by_event_count(self, df: pd.DataFrame, event_count_column: str, count_threshold: int) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Splits the DataFrame based on the count of events recorded in a specific column.\n        \n        This method divides the input DataFrame into multiple segments based on a threshold value applied \n        to the event count column. Different segments may reflect varying event participation quantities.\n        \n        Args:\n            df (pd.DataFrame): The input data containing events.\n            event_count_column (str): The column name that holds the event count.\n            count_threshold (int): The threshold value used to determine splitting logic.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary where keys represent split segment identifiers and values \n            are the corresponding DataFrame segments.\n        \"\"\"\n        pass\n\n    def split_by_event_frequency(self, df: pd.DataFrame, frequency_column: str, frequency_threshold: float) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Splits the DataFrame based on the frequency of events observed in a given column.\n        \n        The method segments the DataFrame into multiple groups based on whether the event frequency \n        exceeds or falls below a given threshold. This is useful for categorizing events based on how \n        often they occur.\n        \n        Args:\n            df (pd.DataFrame): The input DataFrame containing event frequency information.\n            frequency_column (str): The column name that contains the event frequency data.\n            frequency_threshold (float): The threshold frequency to determine how the dataset is split.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary mapping segment names to their corresponding DataFrame splits.\n        \"\"\"\n        pass\n\n    def split_by_event_types(self, df: pd.DataFrame, event_type_column: str) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Splits the DataFrame based on distinct event types as specified in a given column.\n        \n        This method categorizes the input data into multiple segments, each corresponding to a unique\n        event type found in the specified column. It facilitates analysis per event category.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing event data.\n            event_type_column (str): The column name that classifies the types of events.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary where each key is an event type and the value is the \n            subset of the DataFrame corresponding to that event type.\n        \"\"\"\n        pass",
                                                    "lineno": 4
                                                },
                                                {
                                                    "name": "split_by_event_count",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/event_based.py",
                                                    "parent": "EventBasedSplitter",
                                                    "extra": {},
                                                    "code": "def split_by_event_count(self, df: pd.DataFrame, event_count_column: str, count_threshold: int) -> Dict[str, pd.DataFrame]:\n    \"\"\"\n        Splits the DataFrame based on the count of events recorded in a specific column.\n        \n        This method divides the input DataFrame into multiple segments based on a threshold value applied \n        to the event count column. Different segments may reflect varying event participation quantities.\n        \n        Args:\n            df (pd.DataFrame): The input data containing events.\n            event_count_column (str): The column name that holds the event count.\n            count_threshold (int): The threshold value used to determine splitting logic.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary where keys represent split segment identifiers and values \n            are the corresponding DataFrame segments.\n        \"\"\"\n    pass",
                                                    "lineno": 21
                                                },
                                                {
                                                    "name": "split_by_event_frequency",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/event_based.py",
                                                    "parent": "EventBasedSplitter",
                                                    "extra": {},
                                                    "code": "def split_by_event_frequency(self, df: pd.DataFrame, frequency_column: str, frequency_threshold: float) -> Dict[str, pd.DataFrame]:\n    \"\"\"\n        Splits the DataFrame based on the frequency of events observed in a given column.\n        \n        The method segments the DataFrame into multiple groups based on whether the event frequency \n        exceeds or falls below a given threshold. This is useful for categorizing events based on how \n        often they occur.\n        \n        Args:\n            df (pd.DataFrame): The input DataFrame containing event frequency information.\n            frequency_column (str): The column name that contains the event frequency data.\n            frequency_threshold (float): The threshold frequency to determine how the dataset is split.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary mapping segment names to their corresponding DataFrame splits.\n        \"\"\"\n    pass",
                                                    "lineno": 39
                                                },
                                                {
                                                    "name": "split_by_event_types",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/event_based.py",
                                                    "parent": "EventBasedSplitter",
                                                    "extra": {},
                                                    "code": "def split_by_event_types(self, df: pd.DataFrame, event_type_column: str) -> Dict[str, pd.DataFrame]:\n    \"\"\"\n        Splits the DataFrame based on distinct event types as specified in a given column.\n        \n        This method categorizes the input data into multiple segments, each corresponding to a unique\n        event type found in the specified column. It facilitates analysis per event category.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing event data.\n            event_type_column (str): The column name that classifies the types of events.\n        \n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary where each key is an event type and the value is the \n            subset of the DataFrame corresponding to that event type.\n        \"\"\"\n    pass",
                                                    "lineno": 57
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "cv_temporal.py",
                                            "path": "./src/data_engineering/data_splitting/cv_temporal.py",
                                            "code": "from typing import List, Tuple\nimport pandas as pd\n\nclass CVTemporalSplitter:\n    \"\"\"\n    Provides cross-validation splitting strategies for temporal data with various approaches.\n    \n    This class implements three methods to generate cross-validation splits that respect the temporal \n    structure of the data. It supports:\n      - Time-series cross-validation: Splitting based on chronological order.\n      - Cross-validation with event windows: Creating splits based on event-based windows.\n      - Cross-validation with time windows: Dividing data into splits based on predefined time intervals.\n    \n    Each method returns a list of tuples, where each tuple contains two lists:\n      - The first list contains indices for the training set.\n      - The second list contains indices for the testing set.\n      \n    Methods:\n        time_series_cv: Generates splits for time-series cross-validation.\n        event_windows_cv: Generates splits based on event windows.\n        time_windows_cv: Generates splits based on time windows.\n    \"\"\"\n\n    def time_series_cv(self, X: pd.DataFrame, n_splits: int=5) -> List[Tuple[List[int], List[int]]]:\n        \"\"\"\n        Perform time-series cross-validation by splitting data sequentially.\n        \n        This method assumes that the input DataFrame X is ordered by time. The function generates\n        n_splits pairs of training and testing indices, where each successive split uses later portions \n        of the data for testing.\n        \n        Args:\n            X (pd.DataFrame): The input data ordered chronologically.\n            n_splits (int): The number of splits to generate. Default is 5.\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices.\n        \"\"\"\n        pass\n\n    def event_windows_cv(self, X: pd.DataFrame, event_column: str, window_size: int) -> List[Tuple[List[int], List[int]]]:\n        \"\"\"\n        Perform cross-validation using event windows.\n        \n        This method splits the DataFrame based on event occurrences specified in the event_column.\n        Each window is determined by a fixed number of events (window_size), allowing the cross-validation\n        process to account for clustered or bursty events and their implications on data distribution.\n        \n        Args:\n            X (pd.DataFrame): The input data that includes an event indicator column.\n            event_column (str): The name of the column that specifies events.\n            window_size (int): The number of events to include in each window.\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing indices for training and testing sets.\n        \"\"\"\n        pass\n\n    def time_windows_cv(self, X: pd.DataFrame, time_column: str, window_duration) -> List[Tuple[List[int], List[int]]]:\n        \"\"\"\n        Perform cross-validation using predefined time windows.\n        \n        This method splits the DataFrame based on a time column. Each split is created by segmenting the data \n        into intervals determined by the window_duration parameter. This allows for evaluation across \n        consistent time intervals in the data.\n        \n        Args:\n            X (pd.DataFrame): The input data containing a time-based column.\n            time_column (str): The name of the column representing time.\n            window_duration: The duration of each time window (e.g., an integer representing days or a pandas offset).\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "from typing import List, Tuple",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_splitting/cv_temporal.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import List, Tuple",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_splitting/cv_temporal.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "CVTemporalSplitter",
                                                    "unit_type": "class",
                                                    "file_path": "./src/data_engineering/data_splitting/cv_temporal.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class CVTemporalSplitter:\n    \"\"\"\n    Provides cross-validation splitting strategies for temporal data with various approaches.\n    \n    This class implements three methods to generate cross-validation splits that respect the temporal \n    structure of the data. It supports:\n      - Time-series cross-validation: Splitting based on chronological order.\n      - Cross-validation with event windows: Creating splits based on event-based windows.\n      - Cross-validation with time windows: Dividing data into splits based on predefined time intervals.\n    \n    Each method returns a list of tuples, where each tuple contains two lists:\n      - The first list contains indices for the training set.\n      - The second list contains indices for the testing set.\n      \n    Methods:\n        time_series_cv: Generates splits for time-series cross-validation.\n        event_windows_cv: Generates splits based on event windows.\n        time_windows_cv: Generates splits based on time windows.\n    \"\"\"\n\n    def time_series_cv(self, X: pd.DataFrame, n_splits: int=5) -> List[Tuple[List[int], List[int]]]:\n        \"\"\"\n        Perform time-series cross-validation by splitting data sequentially.\n        \n        This method assumes that the input DataFrame X is ordered by time. The function generates\n        n_splits pairs of training and testing indices, where each successive split uses later portions \n        of the data for testing.\n        \n        Args:\n            X (pd.DataFrame): The input data ordered chronologically.\n            n_splits (int): The number of splits to generate. Default is 5.\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices.\n        \"\"\"\n        pass\n\n    def event_windows_cv(self, X: pd.DataFrame, event_column: str, window_size: int) -> List[Tuple[List[int], List[int]]]:\n        \"\"\"\n        Perform cross-validation using event windows.\n        \n        This method splits the DataFrame based on event occurrences specified in the event_column.\n        Each window is determined by a fixed number of events (window_size), allowing the cross-validation\n        process to account for clustered or bursty events and their implications on data distribution.\n        \n        Args:\n            X (pd.DataFrame): The input data that includes an event indicator column.\n            event_column (str): The name of the column that specifies events.\n            window_size (int): The number of events to include in each window.\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing indices for training and testing sets.\n        \"\"\"\n        pass\n\n    def time_windows_cv(self, X: pd.DataFrame, time_column: str, window_duration) -> List[Tuple[List[int], List[int]]]:\n        \"\"\"\n        Perform cross-validation using predefined time windows.\n        \n        This method splits the DataFrame based on a time column. Each split is created by segmenting the data \n        into intervals determined by the window_duration parameter. This allows for evaluation across \n        consistent time intervals in the data.\n        \n        Args:\n            X (pd.DataFrame): The input data containing a time-based column.\n            time_column (str): The name of the column representing time.\n            window_duration: The duration of each time window (e.g., an integer representing days or a pandas offset).\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices.\n        \"\"\"\n        pass",
                                                    "lineno": 4
                                                },
                                                {
                                                    "name": "time_series_cv",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/cv_temporal.py",
                                                    "parent": "CVTemporalSplitter",
                                                    "extra": {},
                                                    "code": "def time_series_cv(self, X: pd.DataFrame, n_splits: int=5) -> List[Tuple[List[int], List[int]]]:\n    \"\"\"\n        Perform time-series cross-validation by splitting data sequentially.\n        \n        This method assumes that the input DataFrame X is ordered by time. The function generates\n        n_splits pairs of training and testing indices, where each successive split uses later portions \n        of the data for testing.\n        \n        Args:\n            X (pd.DataFrame): The input data ordered chronologically.\n            n_splits (int): The number of splits to generate. Default is 5.\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices.\n        \"\"\"\n    pass",
                                                    "lineno": 24
                                                },
                                                {
                                                    "name": "event_windows_cv",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/cv_temporal.py",
                                                    "parent": "CVTemporalSplitter",
                                                    "extra": {},
                                                    "code": "def event_windows_cv(self, X: pd.DataFrame, event_column: str, window_size: int) -> List[Tuple[List[int], List[int]]]:\n    \"\"\"\n        Perform cross-validation using event windows.\n        \n        This method splits the DataFrame based on event occurrences specified in the event_column.\n        Each window is determined by a fixed number of events (window_size), allowing the cross-validation\n        process to account for clustered or bursty events and their implications on data distribution.\n        \n        Args:\n            X (pd.DataFrame): The input data that includes an event indicator column.\n            event_column (str): The name of the column that specifies events.\n            window_size (int): The number of events to include in each window.\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing indices for training and testing sets.\n        \"\"\"\n    pass",
                                                    "lineno": 41
                                                },
                                                {
                                                    "name": "time_windows_cv",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/cv_temporal.py",
                                                    "parent": "CVTemporalSplitter",
                                                    "extra": {},
                                                    "code": "def time_windows_cv(self, X: pd.DataFrame, time_column: str, window_duration) -> List[Tuple[List[int], List[int]]]:\n    \"\"\"\n        Perform cross-validation using predefined time windows.\n        \n        This method splits the DataFrame based on a time column. Each split is created by segmenting the data \n        into intervals determined by the window_duration parameter. This allows for evaluation across \n        consistent time intervals in the data.\n        \n        Args:\n            X (pd.DataFrame): The input data containing a time-based column.\n            time_column (str): The name of the column representing time.\n            window_duration: The duration of each time window (e.g., an integer representing days or a pandas offset).\n            \n        Returns:\n            List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices.\n        \"\"\"\n    pass",
                                                    "lineno": 59
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "train_test_temporal.py",
                                            "path": "./src/data_engineering/data_splitting/train_test_temporal.py",
                                            "code": "from typing import Dict, Any\nimport pandas as pd\n\nclass TrainTestTemporalSplitter:\n    \"\"\"\n    Provides train-test splitting strategies based on temporal criteria.\n\n    This interface allows splitting a dataset into training and testing portions based on three distinct\n    temporal features:\n      1. split by test period: Splitting based on a designated test period (e.g., a specific date or time span).\n      2. split by time intervals: Splitting by uniform or specified time intervals.\n      3. split by event intervals: Splitting based on event-triggered intervals in the data.\n\n    Methods in this class should accept the input data (typically in a pandas DataFrame) and splitting parameters,\n    and return a dictionary that contains at least two keys (commonly 'train' and 'test') pointing to the respective\n    DataFrame splits.\n\n    Assumptions:\n      - The input DataFrame contains a temporal index or column that can be leveraged for splitting.\n      - The split parameters are pre-validated and provided in a format understandable by each method.\n      - This interface is stateless; any stateful configuration should be managed externally.\n\n    Usage:\n      splitter = TrainTestTemporalSplitter()\n      splits = splitter.split_by_test_period(df, test_period=\"2021-01-01\")\n    \"\"\"\n\n    def split_by_test_period(self, df: pd.DataFrame, test_period: Any) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Split the input DataFrame into training and testing sets based on a specified test period.\n        \n        This method uses a user-defined test period (which could be a specific date or a time range) to\n        determine the boundary between training and testing data.\n\n        Args:\n            df (pd.DataFrame): The input dataset containing temporal data.\n            test_period (Any): The criterion defining the test period. This might be a date string, a tuple of dates,\n                               or any object representing the test period boundary.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n            the split DataFrames.\n\n        Edge Cases:\n            - If the test period does not match any records, the function should return an empty test set.\n            - It is assumed that the DataFrame's temporal column is pre-processed and valid.\n        \"\"\"\n        pass\n\n    def split_by_time_intervals(self, df: pd.DataFrame, interval_params: Dict[str, Any]) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Split the input DataFrame into training and testing sets based on specified time intervals.\n        \n        This method creates splits by dividing the dataset into segments defined by uniform or custom time intervals.\n        \n        Args:\n            df (pd.DataFrame): The input dataset containing time-related data.\n            interval_params (Dict[str, Any]): Parameters defining the time interval splits. For example, it may include\n                                              the interval duration, the start time, and the end time.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n            the resulting DataFrame splits based on time intervals.\n        \n        Constraints:\n            - It is assumed that the interval parameters are provided correctly.\n            - The DataFrame must contain a time-index or a dedicated time column.\n        \"\"\"\n        pass\n\n    def split_by_event_intervals(self, df: pd.DataFrame, event_intervals: Dict[str, Any]) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Split the input DataFrame into training and testing sets based on event-driven intervals.\n        \n        This method leverages events annotated in the DataFrame (or computed externally) to decide the split points,\n        where the events signify transitions between training and testing periods.\n        \n        Args:\n            df (pd.DataFrame): The input dataset that includes event-related data markers.\n            event_intervals (Dict[str, Any]): A dictionary containing parameters that define the event intervals. These\n                                              parameters might dictate how many events mark the split or conditions based\n                                              on event types.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' containing the respective splits determined \n            by event intervals.\n        \n        Edge Cases:\n            - The method should correctly handle cases where events are sparse or overly frequent.\n            - It assumes that the DataFrame includes valid and pre-processed event markers.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "from typing import Dict, Any",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_splitting/train_test_temporal.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Dict, Any",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_splitting/train_test_temporal.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "TrainTestTemporalSplitter",
                                                    "unit_type": "class",
                                                    "file_path": "./src/data_engineering/data_splitting/train_test_temporal.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class TrainTestTemporalSplitter:\n    \"\"\"\n    Provides train-test splitting strategies based on temporal criteria.\n\n    This interface allows splitting a dataset into training and testing portions based on three distinct\n    temporal features:\n      1. split by test period: Splitting based on a designated test period (e.g., a specific date or time span).\n      2. split by time intervals: Splitting by uniform or specified time intervals.\n      3. split by event intervals: Splitting based on event-triggered intervals in the data.\n\n    Methods in this class should accept the input data (typically in a pandas DataFrame) and splitting parameters,\n    and return a dictionary that contains at least two keys (commonly 'train' and 'test') pointing to the respective\n    DataFrame splits.\n\n    Assumptions:\n      - The input DataFrame contains a temporal index or column that can be leveraged for splitting.\n      - The split parameters are pre-validated and provided in a format understandable by each method.\n      - This interface is stateless; any stateful configuration should be managed externally.\n\n    Usage:\n      splitter = TrainTestTemporalSplitter()\n      splits = splitter.split_by_test_period(df, test_period=\"2021-01-01\")\n    \"\"\"\n\n    def split_by_test_period(self, df: pd.DataFrame, test_period: Any) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Split the input DataFrame into training and testing sets based on a specified test period.\n        \n        This method uses a user-defined test period (which could be a specific date or a time range) to\n        determine the boundary between training and testing data.\n\n        Args:\n            df (pd.DataFrame): The input dataset containing temporal data.\n            test_period (Any): The criterion defining the test period. This might be a date string, a tuple of dates,\n                               or any object representing the test period boundary.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n            the split DataFrames.\n\n        Edge Cases:\n            - If the test period does not match any records, the function should return an empty test set.\n            - It is assumed that the DataFrame's temporal column is pre-processed and valid.\n        \"\"\"\n        pass\n\n    def split_by_time_intervals(self, df: pd.DataFrame, interval_params: Dict[str, Any]) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Split the input DataFrame into training and testing sets based on specified time intervals.\n        \n        This method creates splits by dividing the dataset into segments defined by uniform or custom time intervals.\n        \n        Args:\n            df (pd.DataFrame): The input dataset containing time-related data.\n            interval_params (Dict[str, Any]): Parameters defining the time interval splits. For example, it may include\n                                              the interval duration, the start time, and the end time.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n            the resulting DataFrame splits based on time intervals.\n        \n        Constraints:\n            - It is assumed that the interval parameters are provided correctly.\n            - The DataFrame must contain a time-index or a dedicated time column.\n        \"\"\"\n        pass\n\n    def split_by_event_intervals(self, df: pd.DataFrame, event_intervals: Dict[str, Any]) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Split the input DataFrame into training and testing sets based on event-driven intervals.\n        \n        This method leverages events annotated in the DataFrame (or computed externally) to decide the split points,\n        where the events signify transitions between training and testing periods.\n        \n        Args:\n            df (pd.DataFrame): The input dataset that includes event-related data markers.\n            event_intervals (Dict[str, Any]): A dictionary containing parameters that define the event intervals. These\n                                              parameters might dictate how many events mark the split or conditions based\n                                              on event types.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' containing the respective splits determined \n            by event intervals.\n        \n        Edge Cases:\n            - The method should correctly handle cases where events are sparse or overly frequent.\n            - It assumes that the DataFrame includes valid and pre-processed event markers.\n        \"\"\"\n        pass",
                                                    "lineno": 4
                                                },
                                                {
                                                    "name": "split_by_test_period",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/train_test_temporal.py",
                                                    "parent": "TrainTestTemporalSplitter",
                                                    "extra": {},
                                                    "code": "def split_by_test_period(self, df: pd.DataFrame, test_period: Any) -> Dict[str, pd.DataFrame]:\n    \"\"\"\n        Split the input DataFrame into training and testing sets based on a specified test period.\n        \n        This method uses a user-defined test period (which could be a specific date or a time range) to\n        determine the boundary between training and testing data.\n\n        Args:\n            df (pd.DataFrame): The input dataset containing temporal data.\n            test_period (Any): The criterion defining the test period. This might be a date string, a tuple of dates,\n                               or any object representing the test period boundary.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n            the split DataFrames.\n\n        Edge Cases:\n            - If the test period does not match any records, the function should return an empty test set.\n            - It is assumed that the DataFrame's temporal column is pre-processed and valid.\n        \"\"\"\n    pass",
                                                    "lineno": 28
                                                },
                                                {
                                                    "name": "split_by_time_intervals",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/train_test_temporal.py",
                                                    "parent": "TrainTestTemporalSplitter",
                                                    "extra": {},
                                                    "code": "def split_by_time_intervals(self, df: pd.DataFrame, interval_params: Dict[str, Any]) -> Dict[str, pd.DataFrame]:\n    \"\"\"\n        Split the input DataFrame into training and testing sets based on specified time intervals.\n        \n        This method creates splits by dividing the dataset into segments defined by uniform or custom time intervals.\n        \n        Args:\n            df (pd.DataFrame): The input dataset containing time-related data.\n            interval_params (Dict[str, Any]): Parameters defining the time interval splits. For example, it may include\n                                              the interval duration, the start time, and the end time.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n            the resulting DataFrame splits based on time intervals.\n        \n        Constraints:\n            - It is assumed that the interval parameters are provided correctly.\n            - The DataFrame must contain a time-index or a dedicated time column.\n        \"\"\"\n    pass",
                                                    "lineno": 50
                                                },
                                                {
                                                    "name": "split_by_event_intervals",
                                                    "unit_type": "method",
                                                    "file_path": "./src/data_engineering/data_splitting/train_test_temporal.py",
                                                    "parent": "TrainTestTemporalSplitter",
                                                    "extra": {},
                                                    "code": "def split_by_event_intervals(self, df: pd.DataFrame, event_intervals: Dict[str, Any]) -> Dict[str, pd.DataFrame]:\n    \"\"\"\n        Split the input DataFrame into training and testing sets based on event-driven intervals.\n        \n        This method leverages events annotated in the DataFrame (or computed externally) to decide the split points,\n        where the events signify transitions between training and testing periods.\n        \n        Args:\n            df (pd.DataFrame): The input dataset that includes event-related data markers.\n            event_intervals (Dict[str, Any]): A dictionary containing parameters that define the event intervals. These\n                                              parameters might dictate how many events mark the split or conditions based\n                                              on event types.\n\n        Returns:\n            Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' containing the respective splits determined \n            by event intervals.\n        \n        Edge Cases:\n            - The method should correctly handle cases where events are sparse or overly frequent.\n            - It assumes that the DataFrame includes valid and pre-processed event markers.\n        \"\"\"\n    pass",
                                                    "lineno": 71
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "data_validation",
                                    "path": "./src/data_engineering/data_validation",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "cross_validation.py",
                                            "path": "./src/data_engineering/data_validation/cross_validation.py",
                                            "code": "import pandas as pd\nimport numpy as np\nfrom typing import List, Tuple, Optional\n\ndef stratified_cross_validation(X: pd.DataFrame, y: pd.Series, n_splits: int=5, random_state: Optional[int]=None) -> List[Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Perform stratified cross-validation splitting on the given dataset.\n    \n    This function splits the dataset into training and testing folds ensuring that each fold maintains approximately \n    the same percentage of samples from each target class. It is particularly useful in scenarios involving imbalanced\n    classification where preserving class distribution during evaluation is crucial.\n    \n    Args:\n        X (pd.DataFrame): The input features dataset.\n        y (pd.Series): The target labels corresponding to the data in X.\n        n_splits (int): The number of folds to split the data into. Must be at least 2. Default is 5.\n        random_state (Optional[int]): A seed for the random number generator to ensure reproducibility. Default is None.\n    \n    Returns:\n        List[Tuple[np.ndarray, np.ndarray]]: A list where each element is a tuple containing two numpy arrays:\n            - The first array represents the indices for the training set.\n            - The second array represents the indices for the testing set.\n    \n    Edge Cases and Constraints:\n        - n_splits must be at least 2, otherwise valid splits cannot be produced.\n        - If the distribution of classes in y is such that any class does not have enough samples to support the specified n_splits, \n          the function's behavior is undefined and should either raise an error or be handled by pre-validation.\n        - It is assumed that the order of samples in X and y is aligned.\n    \n    Assumptions:\n        - Input X and y are non-empty and correctly aligned.\n        - The function does not perform the actual splitting logic; it defines the interface to be implemented.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_validation/cross_validation.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "import numpy as np",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_validation/cross_validation.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import numpy as np",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "from typing import List, Tuple, Optional",
                                                    "unit_type": "import",
                                                    "file_path": "./src/data_engineering/data_validation/cross_validation.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import List, Tuple, Optional",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "stratified_cross_validation",
                                                    "unit_type": "function",
                                                    "file_path": "./src/data_engineering/data_validation/cross_validation.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def stratified_cross_validation(X: pd.DataFrame, y: pd.Series, n_splits: int=5, random_state: Optional[int]=None) -> List[Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Perform stratified cross-validation splitting on the given dataset.\n    \n    This function splits the dataset into training and testing folds ensuring that each fold maintains approximately \n    the same percentage of samples from each target class. It is particularly useful in scenarios involving imbalanced\n    classification where preserving class distribution during evaluation is crucial.\n    \n    Args:\n        X (pd.DataFrame): The input features dataset.\n        y (pd.Series): The target labels corresponding to the data in X.\n        n_splits (int): The number of folds to split the data into. Must be at least 2. Default is 5.\n        random_state (Optional[int]): A seed for the random number generator to ensure reproducibility. Default is None.\n    \n    Returns:\n        List[Tuple[np.ndarray, np.ndarray]]: A list where each element is a tuple containing two numpy arrays:\n            - The first array represents the indices for the training set.\n            - The second array represents the indices for the testing set.\n    \n    Edge Cases and Constraints:\n        - n_splits must be at least 2, otherwise valid splits cannot be produced.\n        - If the distribution of classes in y is such that any class does not have enough samples to support the specified n_splits, \n          the function's behavior is undefined and should either raise an error or be handled by pre-validation.\n        - It is assumed that the order of samples in X and y is aligned.\n    \n    Assumptions:\n        - Input X and y are non-empty and correctly aligned.\n        - The function does not perform the actual splitting logic; it defines the interface to be implemented.\n    \"\"\"\n    pass",
                                                    "lineno": 5
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "type": "directory",
                            "name": "algorithms",
                            "path": "./src/algorithms",
                            "children": [
                                {
                                    "type": "file",
                                    "name": "base_algorithm.py",
                                    "path": "./src/algorithms/base_algorithm.py",
                                    "code": "from abc import ABC, abstractmethod\nfrom general.base import BaseEstimator\n\nclass BaseAlgorithm(BaseEstimator, ABC):\n    \"\"\"\n    BaseAlgorithm serves as the foundational class for all algorithm implementations.\n    This includes supervised, unsupervised, and optimization-based methods. The class\n    reinforces the estimator interface and allows for additional algorithm-specific\n    methods if required.\n    \"\"\"\n    \n    @abstractmethod\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the algorithm to the data.\n\n        Args:\n            X (DataFrame or array-like): Training data.\n            y (Series or array-like, optional): Target values for applicable models.\n\n        Returns:\n            self: The fitted algorithm instance.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def predict(self, X):\n        \"\"\"\n        Generate predictions for the input data.\n\n        Args:\n            X (DataFrame or array-like): Data for which to generate predictions.\n\n        Returns:\n            Array-like: Predicted values.\n        \"\"\"\n        pass\n\n    def score(self, X, y):\n        \"\"\"\n        Provide a performance metric computation based on predictions and true values.\n\n        Args:\n            X (DataFrame or array-like): Data to predict on.\n            y (Series or array-like): True target values.\n\n        Returns:\n            float: Performance score.\n        \"\"\"\n        pass",
                                    "feature_paths": [],
                                    "units": [
                                        {
                                            "name": "from abc import ABC, abstractmethod",
                                            "unit_type": "import",
                                            "file_path": "./src/algorithms/base_algorithm.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "from abc import ABC, abstractmethod",
                                            "lineno": 1
                                        },
                                        {
                                            "name": "from general.base import BaseEstimator",
                                            "unit_type": "import",
                                            "file_path": "./src/algorithms/base_algorithm.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "from general.base import BaseEstimator",
                                            "lineno": 2
                                        },
                                        {
                                            "name": "BaseAlgorithm",
                                            "unit_type": "class",
                                            "file_path": "./src/algorithms/base_algorithm.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "class BaseAlgorithm(BaseEstimator, ABC):\n    \"\"\"\n    BaseAlgorithm serves as the foundational class for all algorithm implementations.\n    This includes supervised, unsupervised, and optimization-based methods. The class\n    reinforces the estimator interface and allows for additional algorithm-specific\n    methods if required.\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the algorithm to the data.\n\n        Args:\n            X (DataFrame or array-like): Training data.\n            y (Series or array-like, optional): Target values for applicable models.\n\n        Returns:\n            self: The fitted algorithm instance.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def predict(self, X):\n        \"\"\"\n        Generate predictions for the input data.\n\n        Args:\n            X (DataFrame or array-like): Data for which to generate predictions.\n\n        Returns:\n            Array-like: Predicted values.\n        \"\"\"\n        pass\n\n    def score(self, X, y):\n        \"\"\"\n        Provide a performance metric computation based on predictions and true values.\n\n        Args:\n            X (DataFrame or array-like): Data to predict on.\n            y (Series or array-like): True target values.\n\n        Returns:\n            float: Performance score.\n        \"\"\"\n        pass",
                                            "lineno": 4
                                        },
                                        {
                                            "name": "fit",
                                            "unit_type": "method",
                                            "file_path": "./src/algorithms/base_algorithm.py",
                                            "parent": "BaseAlgorithm",
                                            "extra": {},
                                            "code": "@abstractmethod\ndef fit(self, X, y=None):\n    \"\"\"\n        Fit the algorithm to the data.\n\n        Args:\n            X (DataFrame or array-like): Training data.\n            y (Series or array-like, optional): Target values for applicable models.\n\n        Returns:\n            self: The fitted algorithm instance.\n        \"\"\"\n    pass",
                                            "lineno": 13
                                        },
                                        {
                                            "name": "predict",
                                            "unit_type": "method",
                                            "file_path": "./src/algorithms/base_algorithm.py",
                                            "parent": "BaseAlgorithm",
                                            "extra": {},
                                            "code": "@abstractmethod\ndef predict(self, X):\n    \"\"\"\n        Generate predictions for the input data.\n\n        Args:\n            X (DataFrame or array-like): Data for which to generate predictions.\n\n        Returns:\n            Array-like: Predicted values.\n        \"\"\"\n    pass",
                                            "lineno": 27
                                        },
                                        {
                                            "name": "score",
                                            "unit_type": "method",
                                            "file_path": "./src/algorithms/base_algorithm.py",
                                            "parent": "BaseAlgorithm",
                                            "extra": {},
                                            "code": "def score(self, X, y):\n    \"\"\"\n        Provide a performance metric computation based on predictions and true values.\n\n        Args:\n            X (DataFrame or array-like): Data to predict on.\n            y (Series or array-like): True target values.\n\n        Returns:\n            float: Performance score.\n        \"\"\"\n    pass",
                                            "lineno": 39
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "supervised",
                                    "path": "./src/algorithms/supervised",
                                    "children": [
                                        {
                                            "type": "directory",
                                            "name": "regression",
                                            "path": "./src/algorithms/supervised/regression",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "linear_regression.py",
                                                    "path": "./src/algorithms/supervised/regression/linear_regression.py",
                                                    "code": "import pandas as pd\nfrom typing import Any\nfrom src.algorithms.base_algorithm import BaseAlgorithm\n\nclass LinearRegression(BaseAlgorithm):\n    \"\"\"\n    LinearRegression implements various linear regression techniques including:\n      - Simple linear regression: to model a relationship between one independent variable and the dependent variable.\n      - Multiple linear regression: to model the relationship between multiple independent variables and the dependent variable.\n      - Ordinary Least Squares (OLS): the standard method for estimating the parameters in linear regression.\n      - Generic linear regression behavior encapsulated as basic linear regression.\n    \n    This class leverages the ordinary least squares method to estimate coefficients. It can be configured to work for both simple and multiple regression scenarios.\n    \n    Attributes:\n        fit_intercept (bool): Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations.\n        coefficients (Any): Model coefficients computed during fitting.\n    \n    Methods:\n        fit(X: pd.DataFrame, y: pd.Series) -> 'LinearRegression':\n            Fit the linear regression model using the ordinary least squares method on the provided training data.\n            \n        predict(X: pd.DataFrame) -> pd.Series:\n            Predict the target values for the given input data based on the computed regression coefficients.\n    \n    Edge Cases and Assumptions:\n        - Assumes input features in X are numerical and properly preprocessed.\n        - The target y is expected to be continuous.\n        - May not handle singular matrices; preprocessing (e.g., feature selection) should be conducted to ensure numerical stability.\n    \"\"\"\n\n    def __init__(self, fit_intercept: bool=True) -> None:\n        \"\"\"\n        Initialize the LinearRegression model.\n\n        Args:\n            fit_intercept (bool): Flag indicating whether to calculate the intercept. Default is True.\n        \"\"\"\n        self.fit_intercept = fit_intercept\n        self.coefficients = None\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'LinearRegression':\n        \"\"\"\n        Fit the linear regression model using the ordinary least squares method.\n\n        Args:\n            X (pd.DataFrame): Training data with one or multiple features.\n            y (pd.Series): Continuous target values corresponding to X.\n\n        Returns:\n            LinearRegression: The fitted model instance with computed coefficients.\n        \n        Raises:\n            None: The method does not raise exceptions; input validation should be performed prior to calling.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict target values using the linear regression model.\n\n        Args:\n            X (pd.DataFrame): Data containing the same features as used during fitting.\n\n        Returns:\n            pd.Series: Predicted continuous target values.\n        \n        Raises:\n            None: It is assumed that the input data X is preprocessed similarly to the training data.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/regression/linear_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/regression/linear_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "from src.algorithms.base_algorithm import BaseAlgorithm",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/regression/linear_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from src.algorithms.base_algorithm import BaseAlgorithm",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "LinearRegression",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/regression/linear_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class LinearRegression(BaseAlgorithm):\n    \"\"\"\n    LinearRegression implements various linear regression techniques including:\n      - Simple linear regression: to model a relationship between one independent variable and the dependent variable.\n      - Multiple linear regression: to model the relationship between multiple independent variables and the dependent variable.\n      - Ordinary Least Squares (OLS): the standard method for estimating the parameters in linear regression.\n      - Generic linear regression behavior encapsulated as basic linear regression.\n    \n    This class leverages the ordinary least squares method to estimate coefficients. It can be configured to work for both simple and multiple regression scenarios.\n    \n    Attributes:\n        fit_intercept (bool): Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations.\n        coefficients (Any): Model coefficients computed during fitting.\n    \n    Methods:\n        fit(X: pd.DataFrame, y: pd.Series) -> 'LinearRegression':\n            Fit the linear regression model using the ordinary least squares method on the provided training data.\n            \n        predict(X: pd.DataFrame) -> pd.Series:\n            Predict the target values for the given input data based on the computed regression coefficients.\n    \n    Edge Cases and Assumptions:\n        - Assumes input features in X are numerical and properly preprocessed.\n        - The target y is expected to be continuous.\n        - May not handle singular matrices; preprocessing (e.g., feature selection) should be conducted to ensure numerical stability.\n    \"\"\"\n\n    def __init__(self, fit_intercept: bool=True) -> None:\n        \"\"\"\n        Initialize the LinearRegression model.\n\n        Args:\n            fit_intercept (bool): Flag indicating whether to calculate the intercept. Default is True.\n        \"\"\"\n        self.fit_intercept = fit_intercept\n        self.coefficients = None\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'LinearRegression':\n        \"\"\"\n        Fit the linear regression model using the ordinary least squares method.\n\n        Args:\n            X (pd.DataFrame): Training data with one or multiple features.\n            y (pd.Series): Continuous target values corresponding to X.\n\n        Returns:\n            LinearRegression: The fitted model instance with computed coefficients.\n        \n        Raises:\n            None: The method does not raise exceptions; input validation should be performed prior to calling.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict target values using the linear regression model.\n\n        Args:\n            X (pd.DataFrame): Data containing the same features as used during fitting.\n\n        Returns:\n            pd.Series: Predicted continuous target values.\n        \n        Raises:\n            None: It is assumed that the input data X is preprocessed similarly to the training data.\n        \"\"\"\n        pass",
                                                            "lineno": 5
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/regression/linear_regression.py",
                                                            "parent": "LinearRegression",
                                                            "extra": {},
                                                            "code": "def __init__(self, fit_intercept: bool=True) -> None:\n    \"\"\"\n        Initialize the LinearRegression model.\n\n        Args:\n            fit_intercept (bool): Flag indicating whether to calculate the intercept. Default is True.\n        \"\"\"\n    self.fit_intercept = fit_intercept\n    self.coefficients = None\n    pass",
                                                            "lineno": 32
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/regression/linear_regression.py",
                                                            "parent": "LinearRegression",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'LinearRegression':\n    \"\"\"\n        Fit the linear regression model using the ordinary least squares method.\n\n        Args:\n            X (pd.DataFrame): Training data with one or multiple features.\n            y (pd.Series): Continuous target values corresponding to X.\n\n        Returns:\n            LinearRegression: The fitted model instance with computed coefficients.\n        \n        Raises:\n            None: The method does not raise exceptions; input validation should be performed prior to calling.\n        \"\"\"\n    pass",
                                                            "lineno": 43
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/regression/linear_regression.py",
                                                            "parent": "LinearRegression",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    \"\"\"\n        Predict target values using the linear regression model.\n\n        Args:\n            X (pd.DataFrame): Data containing the same features as used during fitting.\n\n        Returns:\n            pd.Series: Predicted continuous target values.\n        \n        Raises:\n            None: It is assumed that the input data X is preprocessed similarly to the training data.\n        \"\"\"\n    pass",
                                                            "lineno": 59
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "regularized_regression.py",
                                                    "path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                    "code": "from algorithms.base_algorithm import BaseAlgorithm\nimport pandas as pd\nimport numpy as np\nfrom typing import List\n\nclass KernelRidgeRegression(BaseAlgorithm):\n    \"\"\"\n    Implements Kernel Ridge Regression for non-linear regression tasks.\n\n    This class integrates kernel methods with ridge regression to allow modeling\n    non-linear relationships. It inherently uses L2 regularization and supports the\n    use of various kernel functions for flexible modeling.\n\n    Args:\n        alpha (float): Regularization strength for L2 penalty.\n        kernel (str): Specifies the kernel type to be used (e.g., 'linear', 'rbf').\n        gamma (float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels.\n\n    Methods:\n        fit(X, y): Fit the Kernel Ridge Regression model on training data.\n        predict(X): Generate predictions using the fitted model.\n    \"\"\"\n\n    def __init__(self, alpha: float, kernel: str='rbf', gamma: float=1.0) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'KernelRidgeRegression':\n        \"\"\"\n        Fit the Kernel Ridge Regression model.\n\n        Args:\n            X (pd.DataFrame): Training data features.\n            y (pd.Series): Target values.\n\n        Returns:\n            KernelRidgeRegression: The fitted model instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict target values using the fitted Kernel Ridge Regression model.\n\n        Args:\n            X (pd.DataFrame): Data for which predictions are to be made.\n\n        Returns:\n            pd.Series: Predicted output values.\n        \"\"\"\n        pass\n\ndef logistic_regression_l1_penalty(coefficients: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the L1 regularization penalty for logistic regression.\n\n    This function computes the L1 penalty, defined as the sum of absolute\n    values of the coefficients scaled by the regularization parameter alpha.\n    It is specifically tailored for logistic regression models in a classification\n    setting.\n\n    Args:\n        coefficients (np.ndarray): Array of model coefficients.\n        alpha (float): Regularization strength parameter.\n\n    Returns:\n        float: The computed L1 penalty for logistic regression.\n\n    Edge Cases:\n        - Expects coefficients to be a non-empty numpy array.\n        - alpha should be a non-negative float.\n    \"\"\"\n    pass\n\ndef regularized_regression_feature_selection(X: pd.DataFrame, y: pd.Series, method: str='lasso') -> List[str]:\n    \"\"\"\n    Perform feature selection for regularized regression models.\n\n    This function applies a feature selection strategy, typically leveraging L1\n    regularization (e.g., LASSO), to identify a subset of important features\n    from the input dataset. The selected features aim to improve model performance\n    by reducing overfitting and enhancing interpretability.\n\n    Args:\n        X (pd.DataFrame): Input feature dataset.\n        y (pd.Series): Target variable.\n        method (str, optional): Feature selection method to apply. Defaults to \"lasso\".\n\n    Returns:\n        List[str]: A list of selected feature names.\n\n    Edge Cases:\n        - Assumes non-empty DataFrame and Series.\n        - The method parameter must be one of the supported feature selection strategies.\n    \"\"\"\n    pass\n\ndef l2_regularization_penalty(weights: np.ndarray, lambda_value: float) -> float:\n    \"\"\"\n    Compute the L2 regularization (ridge) penalty for a set of model weights.\n\n    This function calculates the L2 penalty as the sum of squared weights multiplied\n    by the regularization factor lambda_value. It is commonly used in regularized regression\n    to reduce overfitting by penalizing large coefficients.\n\n    Args:\n        weights (np.ndarray): Array of model weights.\n        lambda_value (float): Regularization factor (non-negative).\n\n    Returns:\n        float: The computed L2 regularization penalty.\n\n    Edge Cases:\n        - Assumes weights is a non-empty numpy array.\n        - lambda_value must be a non-negative float.\n    \"\"\"\n    pass\n\ndef regularized_regression_l1_penalty(weights: np.ndarray, lambda_value: float) -> float:\n    \"\"\"\n    Compute the L1 regularization penalty for regularized regression models.\n\n    This function computes the L1 penalty as the sum of the absolute values of the weights,\n    scaled by the regularization parameter lambda_value. This penalty is used to enforce sparsity\n    in regression models and is distinct from the logistic regression context.\n\n    Args:\n        weights (np.ndarray): Array of model weights.\n        lambda_value (float): Regularization parameter controlling the strength of the penalty.\n\n    Returns:\n        float: The calculated L1 regularization penalty.\n\n    Edge Cases:\n        - Expects weights to be a non-empty numpy array.\n        - lambda_value should be a non-negative float.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "import numpy as np",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import numpy as np",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "from typing import List",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import List",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "KernelRidgeRegression",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class KernelRidgeRegression(BaseAlgorithm):\n    \"\"\"\n    Implements Kernel Ridge Regression for non-linear regression tasks.\n\n    This class integrates kernel methods with ridge regression to allow modeling\n    non-linear relationships. It inherently uses L2 regularization and supports the\n    use of various kernel functions for flexible modeling.\n\n    Args:\n        alpha (float): Regularization strength for L2 penalty.\n        kernel (str): Specifies the kernel type to be used (e.g., 'linear', 'rbf').\n        gamma (float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels.\n\n    Methods:\n        fit(X, y): Fit the Kernel Ridge Regression model on training data.\n        predict(X): Generate predictions using the fitted model.\n    \"\"\"\n\n    def __init__(self, alpha: float, kernel: str='rbf', gamma: float=1.0) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'KernelRidgeRegression':\n        \"\"\"\n        Fit the Kernel Ridge Regression model.\n\n        Args:\n            X (pd.DataFrame): Training data features.\n            y (pd.Series): Target values.\n\n        Returns:\n            KernelRidgeRegression: The fitted model instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict target values using the fitted Kernel Ridge Regression model.\n\n        Args:\n            X (pd.DataFrame): Data for which predictions are to be made.\n\n        Returns:\n            pd.Series: Predicted output values.\n        \"\"\"\n        pass",
                                                            "lineno": 6
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": "KernelRidgeRegression",
                                                            "extra": {},
                                                            "code": "def __init__(self, alpha: float, kernel: str='rbf', gamma: float=1.0) -> None:\n    pass",
                                                            "lineno": 24
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": "KernelRidgeRegression",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'KernelRidgeRegression':\n    \"\"\"\n        Fit the Kernel Ridge Regression model.\n\n        Args:\n            X (pd.DataFrame): Training data features.\n            y (pd.Series): Target values.\n\n        Returns:\n            KernelRidgeRegression: The fitted model instance.\n        \"\"\"\n    pass",
                                                            "lineno": 27
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": "KernelRidgeRegression",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    \"\"\"\n        Predict target values using the fitted Kernel Ridge Regression model.\n\n        Args:\n            X (pd.DataFrame): Data for which predictions are to be made.\n\n        Returns:\n            pd.Series: Predicted output values.\n        \"\"\"\n    pass",
                                                            "lineno": 40
                                                        },
                                                        {
                                                            "name": "logistic_regression_l1_penalty",
                                                            "unit_type": "function",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def logistic_regression_l1_penalty(coefficients: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the L1 regularization penalty for logistic regression.\n\n    This function computes the L1 penalty, defined as the sum of absolute\n    values of the coefficients scaled by the regularization parameter alpha.\n    It is specifically tailored for logistic regression models in a classification\n    setting.\n\n    Args:\n        coefficients (np.ndarray): Array of model coefficients.\n        alpha (float): Regularization strength parameter.\n\n    Returns:\n        float: The computed L1 penalty for logistic regression.\n\n    Edge Cases:\n        - Expects coefficients to be a non-empty numpy array.\n        - alpha should be a non-negative float.\n    \"\"\"\n    pass",
                                                            "lineno": 52
                                                        },
                                                        {
                                                            "name": "regularized_regression_feature_selection",
                                                            "unit_type": "function",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def regularized_regression_feature_selection(X: pd.DataFrame, y: pd.Series, method: str='lasso') -> List[str]:\n    \"\"\"\n    Perform feature selection for regularized regression models.\n\n    This function applies a feature selection strategy, typically leveraging L1\n    regularization (e.g., LASSO), to identify a subset of important features\n    from the input dataset. The selected features aim to improve model performance\n    by reducing overfitting and enhancing interpretability.\n\n    Args:\n        X (pd.DataFrame): Input feature dataset.\n        y (pd.Series): Target variable.\n        method (str, optional): Feature selection method to apply. Defaults to \"lasso\".\n\n    Returns:\n        List[str]: A list of selected feature names.\n\n    Edge Cases:\n        - Assumes non-empty DataFrame and Series.\n        - The method parameter must be one of the supported feature selection strategies.\n    \"\"\"\n    pass",
                                                            "lineno": 74
                                                        },
                                                        {
                                                            "name": "l2_regularization_penalty",
                                                            "unit_type": "function",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def l2_regularization_penalty(weights: np.ndarray, lambda_value: float) -> float:\n    \"\"\"\n    Compute the L2 regularization (ridge) penalty for a set of model weights.\n\n    This function calculates the L2 penalty as the sum of squared weights multiplied\n    by the regularization factor lambda_value. It is commonly used in regularized regression\n    to reduce overfitting by penalizing large coefficients.\n\n    Args:\n        weights (np.ndarray): Array of model weights.\n        lambda_value (float): Regularization factor (non-negative).\n\n    Returns:\n        float: The computed L2 regularization penalty.\n\n    Edge Cases:\n        - Assumes weights is a non-empty numpy array.\n        - lambda_value must be a non-negative float.\n    \"\"\"\n    pass",
                                                            "lineno": 97
                                                        },
                                                        {
                                                            "name": "regularized_regression_l1_penalty",
                                                            "unit_type": "function",
                                                            "file_path": "./src/algorithms/supervised/regression/regularized_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def regularized_regression_l1_penalty(weights: np.ndarray, lambda_value: float) -> float:\n    \"\"\"\n    Compute the L1 regularization penalty for regularized regression models.\n\n    This function computes the L1 penalty as the sum of the absolute values of the weights,\n    scaled by the regularization parameter lambda_value. This penalty is used to enforce sparsity\n    in regression models and is distinct from the logistic regression context.\n\n    Args:\n        weights (np.ndarray): Array of model weights.\n        lambda_value (float): Regularization parameter controlling the strength of the penalty.\n\n    Returns:\n        float: The calculated L1 regularization penalty.\n\n    Edge Cases:\n        - Expects weights to be a non-empty numpy array.\n        - lambda_value should be a non-negative float.\n    \"\"\"\n    pass",
                                                            "lineno": 118
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "classification",
                                            "path": "./src/algorithms/supervised/classification",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "svm_classification.py",
                                                    "path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                    "code": "import pandas as pd\nfrom typing import Any\nfrom algorithms.base_algorithm import BaseAlgorithm\n\nclass KernelSVMClassifier(BaseAlgorithm):\n    \"\"\"\n    KernelSVMClassifier implements a support vector machine (SVM) classifier using kernel methods.\n    This classifier supports various kernel functions (e.g., linear, polynomial, rbf) to map input data\n    into higher-dimensional spaces for improved separation in cases where data is not linearly separable.\n\n    Attributes:\n        kernel (str): The kernel type to be used in the algorithm (e.g., 'linear', 'rbf', 'poly').\n        C (float): Regularization parameter. The strength of the regularization is inversely proportional to C.\n        gamma (str or float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. Can be 'scale', 'auto', or a numeric value.\n\n    Methods:\n        fit(X, y):\n            Fit the SVM classifier on the training data.\n        predict(X):\n            Predict the class labels for the provided data.\n    \"\"\"\n\n    def __init__(self, kernel: str='rbf', C: float=1.0, gamma: Any='scale') -> None:\n        \"\"\"\n        Initialize the KernelSVMClassifier with specified hyperparameters.\n\n        Args:\n            kernel (str): Specifies the kernel type to be used in the algorithm.\n            C (float): Regularization parameter. Lower values create a softer margin.\n            gamma (str or float): Kernel coefficient, controlling the influence of individual training examples.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'KernelSVMClassifier':\n        \"\"\"\n        Fit the SVM classifier on the provided training data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target class labels.\n\n        Returns:\n            KernelSVMClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for the input data using the fitted model.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n\n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n        pass\n\nclass HardMarginSVMClassifier(BaseAlgorithm):\n    \"\"\"\n    HardMarginSVMClassifier implements a variant of the support vector machine classifier\n    that assumes strict linear separability. This model enforces a hard margin, meaning no\n    misclassifications are allowed in the training data, resulting in no slack variables.\n\n    Attributes:\n        kernel (str): The kernel function used; typically 'linear' for hard-margin SVM.\n        C (float): Regularization parameter; for hard-margin, C is set to a very high value to enforce strict separation.\n    \n    Methods:\n        fit(X, y):\n            Train the hard-margin SVM classifier on provided training data.\n        predict(X):\n            Predict class labels for new data based on the hard-margin decision boundary.\n    \"\"\"\n\n    def __init__(self, kernel: str='linear', C: float=100000.0) -> None:\n        \"\"\"\n        Initialize the HardMarginSVMClassifier with strict margin parameters.\n        \n        Args:\n            kernel (str): Specifies the kernel type. The hard-margin version typically relies on a linear kernel.\n            C (float): A large regularization constant to enforce the hard margin constraint.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'HardMarginSVMClassifier':\n        \"\"\"\n        Fit the hard-margin SVM classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target class labels.\n\n        Returns:\n            HardMarginSVMClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for the input data using the hard-margin decision boundary.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n\n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "KernelSVMClassifier",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class KernelSVMClassifier(BaseAlgorithm):\n    \"\"\"\n    KernelSVMClassifier implements a support vector machine (SVM) classifier using kernel methods.\n    This classifier supports various kernel functions (e.g., linear, polynomial, rbf) to map input data\n    into higher-dimensional spaces for improved separation in cases where data is not linearly separable.\n\n    Attributes:\n        kernel (str): The kernel type to be used in the algorithm (e.g., 'linear', 'rbf', 'poly').\n        C (float): Regularization parameter. The strength of the regularization is inversely proportional to C.\n        gamma (str or float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. Can be 'scale', 'auto', or a numeric value.\n\n    Methods:\n        fit(X, y):\n            Fit the SVM classifier on the training data.\n        predict(X):\n            Predict the class labels for the provided data.\n    \"\"\"\n\n    def __init__(self, kernel: str='rbf', C: float=1.0, gamma: Any='scale') -> None:\n        \"\"\"\n        Initialize the KernelSVMClassifier with specified hyperparameters.\n\n        Args:\n            kernel (str): Specifies the kernel type to be used in the algorithm.\n            C (float): Regularization parameter. Lower values create a softer margin.\n            gamma (str or float): Kernel coefficient, controlling the influence of individual training examples.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'KernelSVMClassifier':\n        \"\"\"\n        Fit the SVM classifier on the provided training data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target class labels.\n\n        Returns:\n            KernelSVMClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for the input data using the fitted model.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n\n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n        pass",
                                                            "lineno": 5
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                            "parent": "KernelSVMClassifier",
                                                            "extra": {},
                                                            "code": "def __init__(self, kernel: str='rbf', C: float=1.0, gamma: Any='scale') -> None:\n    \"\"\"\n        Initialize the KernelSVMClassifier with specified hyperparameters.\n\n        Args:\n            kernel (str): Specifies the kernel type to be used in the algorithm.\n            C (float): Regularization parameter. Lower values create a softer margin.\n            gamma (str or float): Kernel coefficient, controlling the influence of individual training examples.\n        \"\"\"\n    pass",
                                                            "lineno": 23
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                            "parent": "KernelSVMClassifier",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'KernelSVMClassifier':\n    \"\"\"\n        Fit the SVM classifier on the provided training data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target class labels.\n\n        Returns:\n            KernelSVMClassifier: The fitted classifier instance.\n        \"\"\"\n    pass",
                                                            "lineno": 34
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                            "parent": "KernelSVMClassifier",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    \"\"\"\n        Predict class labels for the input data using the fitted model.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n\n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n    pass",
                                                            "lineno": 47
                                                        },
                                                        {
                                                            "name": "HardMarginSVMClassifier",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class HardMarginSVMClassifier(BaseAlgorithm):\n    \"\"\"\n    HardMarginSVMClassifier implements a variant of the support vector machine classifier\n    that assumes strict linear separability. This model enforces a hard margin, meaning no\n    misclassifications are allowed in the training data, resulting in no slack variables.\n\n    Attributes:\n        kernel (str): The kernel function used; typically 'linear' for hard-margin SVM.\n        C (float): Regularization parameter; for hard-margin, C is set to a very high value to enforce strict separation.\n    \n    Methods:\n        fit(X, y):\n            Train the hard-margin SVM classifier on provided training data.\n        predict(X):\n            Predict class labels for new data based on the hard-margin decision boundary.\n    \"\"\"\n\n    def __init__(self, kernel: str='linear', C: float=100000.0) -> None:\n        \"\"\"\n        Initialize the HardMarginSVMClassifier with strict margin parameters.\n        \n        Args:\n            kernel (str): Specifies the kernel type. The hard-margin version typically relies on a linear kernel.\n            C (float): A large regularization constant to enforce the hard margin constraint.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'HardMarginSVMClassifier':\n        \"\"\"\n        Fit the hard-margin SVM classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target class labels.\n\n        Returns:\n            HardMarginSVMClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for the input data using the hard-margin decision boundary.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n\n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n        pass",
                                                            "lineno": 59
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                            "parent": "HardMarginSVMClassifier",
                                                            "extra": {},
                                                            "code": "def __init__(self, kernel: str='linear', C: float=100000.0) -> None:\n    \"\"\"\n        Initialize the HardMarginSVMClassifier with strict margin parameters.\n        \n        Args:\n            kernel (str): Specifies the kernel type. The hard-margin version typically relies on a linear kernel.\n            C (float): A large regularization constant to enforce the hard margin constraint.\n        \"\"\"\n    pass",
                                                            "lineno": 76
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                            "parent": "HardMarginSVMClassifier",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'HardMarginSVMClassifier':\n    \"\"\"\n        Fit the hard-margin SVM classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target class labels.\n\n        Returns:\n            HardMarginSVMClassifier: The fitted classifier instance.\n        \"\"\"\n    pass",
                                                            "lineno": 86
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/svm_classification.py",
                                                            "parent": "HardMarginSVMClassifier",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    \"\"\"\n        Predict class labels for the input data using the hard-margin decision boundary.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n\n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n    pass",
                                                            "lineno": 99
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "decision_trees.py",
                                                    "path": "./src/algorithms/supervised/classification/decision_trees.py",
                                                    "code": "from algorithms.base_algorithm import BaseAlgorithm\nimport pandas as pd\nfrom typing import Any\n\nclass DecisionTreeClassifier(BaseAlgorithm):\n    \"\"\"\n    DecisionTreeClassifier implements a decision tree based classifier that partitions the feature space\n    and assigns class labels based on learned decision rules.\n\n    This classifier recursively splits the input feature space based on criteria that maximize the\n    separation between classes. It is designed to handle both categorical and numerical features.\n    \n    Args:\n        max_depth (int, optional): The maximum depth of the tree. If None, the tree is expanded until leaves are pure.\n        min_samples_split (int): The minimum number of samples required to split an internal node.\n\n    Methods:\n        fit(pd.DataFrame, pd.Series) -> DecisionTreeClassifier:\n            Trains the decision tree classifier using the provided training data and corresponding labels.\n        predict(pd.DataFrame) -> pd.Series:\n            Predicts class labels for the given input data based on the learned decision tree.\n    \"\"\"\n\n    def __init__(self, max_depth: int=None, min_samples_split: int=2) -> None:\n        \"\"\"\n        Initialize the DecisionTreeClassifier with specific hyperparameters.\n\n        Args:\n            max_depth (int, optional): Maximum depth that the tree can grow. Use None for unlimited depth.\n            min_samples_split (int): Minimum number of samples required to split an internal node.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'DecisionTreeClassifier':\n        \"\"\"\n        Build the decision tree classifier from the training set.\n\n        Args:\n            X (pd.DataFrame): The input training features.\n            y (pd.Series): The target class labels.\n        \n        Returns:\n            DecisionTreeClassifier: The trained decision tree classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for the provided input data using the trained decision tree.\n\n        Args:\n            X (pd.DataFrame): Input features for which to predict class labels.\n        \n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/decision_trees.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/decision_trees.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "from typing import Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/decision_trees.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "DecisionTreeClassifier",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/classification/decision_trees.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class DecisionTreeClassifier(BaseAlgorithm):\n    \"\"\"\n    DecisionTreeClassifier implements a decision tree based classifier that partitions the feature space\n    and assigns class labels based on learned decision rules.\n\n    This classifier recursively splits the input feature space based on criteria that maximize the\n    separation between classes. It is designed to handle both categorical and numerical features.\n    \n    Args:\n        max_depth (int, optional): The maximum depth of the tree. If None, the tree is expanded until leaves are pure.\n        min_samples_split (int): The minimum number of samples required to split an internal node.\n\n    Methods:\n        fit(pd.DataFrame, pd.Series) -> DecisionTreeClassifier:\n            Trains the decision tree classifier using the provided training data and corresponding labels.\n        predict(pd.DataFrame) -> pd.Series:\n            Predicts class labels for the given input data based on the learned decision tree.\n    \"\"\"\n\n    def __init__(self, max_depth: int=None, min_samples_split: int=2) -> None:\n        \"\"\"\n        Initialize the DecisionTreeClassifier with specific hyperparameters.\n\n        Args:\n            max_depth (int, optional): Maximum depth that the tree can grow. Use None for unlimited depth.\n            min_samples_split (int): Minimum number of samples required to split an internal node.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'DecisionTreeClassifier':\n        \"\"\"\n        Build the decision tree classifier from the training set.\n\n        Args:\n            X (pd.DataFrame): The input training features.\n            y (pd.Series): The target class labels.\n        \n        Returns:\n            DecisionTreeClassifier: The trained decision tree classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for the provided input data using the trained decision tree.\n\n        Args:\n            X (pd.DataFrame): Input features for which to predict class labels.\n        \n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n        pass",
                                                            "lineno": 5
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/decision_trees.py",
                                                            "parent": "DecisionTreeClassifier",
                                                            "extra": {},
                                                            "code": "def __init__(self, max_depth: int=None, min_samples_split: int=2) -> None:\n    \"\"\"\n        Initialize the DecisionTreeClassifier with specific hyperparameters.\n\n        Args:\n            max_depth (int, optional): Maximum depth that the tree can grow. Use None for unlimited depth.\n            min_samples_split (int): Minimum number of samples required to split an internal node.\n        \"\"\"\n    pass",
                                                            "lineno": 24
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/decision_trees.py",
                                                            "parent": "DecisionTreeClassifier",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'DecisionTreeClassifier':\n    \"\"\"\n        Build the decision tree classifier from the training set.\n\n        Args:\n            X (pd.DataFrame): The input training features.\n            y (pd.Series): The target class labels.\n        \n        Returns:\n            DecisionTreeClassifier: The trained decision tree classifier instance.\n        \"\"\"\n    pass",
                                                            "lineno": 34
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/decision_trees.py",
                                                            "parent": "DecisionTreeClassifier",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    \"\"\"\n        Predict class labels for the provided input data using the trained decision tree.\n\n        Args:\n            X (pd.DataFrame): Input features for which to predict class labels.\n        \n        Returns:\n            pd.Series: Predicted class labels.\n        \"\"\"\n    pass",
                                                            "lineno": 47
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "naive_bayes.py",
                                                    "path": "./src/algorithms/supervised/classification/naive_bayes.py",
                                                    "code": "import pandas as pd\nfrom typing import Any\nimport numpy as np\nfrom algorithms.base_algorithm import BaseAlgorithm\n\nclass NaiveBayesClassifier(BaseAlgorithm):\n    \"\"\"\n    NaiveBayesClassifier implements the Naive Bayes algorithm for classification tasks.\n    \n    This classifier uses Bayes' theorem with the assumption of strong (naive) independence between features.\n    It calculates prior probabilities and likelihoods based on the training data, and employs these to predict\n    the class labels for new data instances.\n\n    Attributes:\n        smoothing (float): The Laplace smoothing parameter to ensure robustness against zero-frequency issues \n                           in likelihood estimates.\n\n    Methods:\n        fit(X, y): Estimates the prior probabilities and feature likelihoods from the training data.\n        predict(X): Predicts class labels for new input data based on the computed probabilities.\n    \"\"\"\n\n    def __init__(self, smoothing: float=1.0) -> None:\n        \"\"\"\n        Initialize the NaiveBayesClassifier with a smoothing parameter.\n\n        Args:\n            smoothing (float, optional): Laplace smoothing constant to handle zero probabilities. Defaults to 1.0.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'NaiveBayesClassifier':\n        \"\"\"\n        Fit the Naive Bayes classifier by computing the class prior probabilities and feature likelihoods from training data.\n\n        Args:\n            X (pd.DataFrame): A DataFrame containing the training features.\n            y (pd.Series): A Series containing the training class labels.\n\n        Returns:\n            NaiveBayesClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for given input data using the Naive Bayes probabilistic model.\n\n        Args:\n            X (pd.DataFrame): A DataFrame containing the features for which to predict class labels.\n\n        Returns:\n            pd.Series: A Series containing the predicted class labels.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/naive_bayes.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/naive_bayes.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "import numpy as np",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/naive_bayes.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import numpy as np",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/naive_bayes.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "NaiveBayesClassifier",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/classification/naive_bayes.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class NaiveBayesClassifier(BaseAlgorithm):\n    \"\"\"\n    NaiveBayesClassifier implements the Naive Bayes algorithm for classification tasks.\n    \n    This classifier uses Bayes' theorem with the assumption of strong (naive) independence between features.\n    It calculates prior probabilities and likelihoods based on the training data, and employs these to predict\n    the class labels for new data instances.\n\n    Attributes:\n        smoothing (float): The Laplace smoothing parameter to ensure robustness against zero-frequency issues \n                           in likelihood estimates.\n\n    Methods:\n        fit(X, y): Estimates the prior probabilities and feature likelihoods from the training data.\n        predict(X): Predicts class labels for new input data based on the computed probabilities.\n    \"\"\"\n\n    def __init__(self, smoothing: float=1.0) -> None:\n        \"\"\"\n        Initialize the NaiveBayesClassifier with a smoothing parameter.\n\n        Args:\n            smoothing (float, optional): Laplace smoothing constant to handle zero probabilities. Defaults to 1.0.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'NaiveBayesClassifier':\n        \"\"\"\n        Fit the Naive Bayes classifier by computing the class prior probabilities and feature likelihoods from training data.\n\n        Args:\n            X (pd.DataFrame): A DataFrame containing the training features.\n            y (pd.Series): A Series containing the training class labels.\n\n        Returns:\n            NaiveBayesClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Predict class labels for given input data using the Naive Bayes probabilistic model.\n\n        Args:\n            X (pd.DataFrame): A DataFrame containing the features for which to predict class labels.\n\n        Returns:\n            pd.Series: A Series containing the predicted class labels.\n        \"\"\"\n        pass",
                                                            "lineno": 6
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/naive_bayes.py",
                                                            "parent": "NaiveBayesClassifier",
                                                            "extra": {},
                                                            "code": "def __init__(self, smoothing: float=1.0) -> None:\n    \"\"\"\n        Initialize the NaiveBayesClassifier with a smoothing parameter.\n\n        Args:\n            smoothing (float, optional): Laplace smoothing constant to handle zero probabilities. Defaults to 1.0.\n        \"\"\"\n    pass",
                                                            "lineno": 23
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/naive_bayes.py",
                                                            "parent": "NaiveBayesClassifier",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'NaiveBayesClassifier':\n    \"\"\"\n        Fit the Naive Bayes classifier by computing the class prior probabilities and feature likelihoods from training data.\n\n        Args:\n            X (pd.DataFrame): A DataFrame containing the training features.\n            y (pd.Series): A Series containing the training class labels.\n\n        Returns:\n            NaiveBayesClassifier: The fitted classifier instance.\n        \"\"\"\n    pass",
                                                            "lineno": 32
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/naive_bayes.py",
                                                            "parent": "NaiveBayesClassifier",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    \"\"\"\n        Predict class labels for given input data using the Naive Bayes probabilistic model.\n\n        Args:\n            X (pd.DataFrame): A DataFrame containing the features for which to predict class labels.\n\n        Returns:\n            pd.Series: A Series containing the predicted class labels.\n        \"\"\"\n    pass",
                                                            "lineno": 45
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "logistic_regression.py",
                                                    "path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                    "code": "from typing import Optional\nfrom algorithms.base_algorithm import BaseAlgorithm\nimport pandas as pd\nfrom typing import Any\nimport numpy as np\n\nclass LogisticRegressionClassifier(BaseAlgorithm):\n    \"\"\"\n    LogisticRegressionClassifier implements the basic logistic regression algorithm.\n    \n    This classifier provides methods to train a logistic regression model on provided data\n    and to generate predictions. It encapsulates the standard logistic regression functionality,\n    addressing binary classification tasks.\n\n    Methods:\n        __init__(self, penalty: str = 'l2', C: float = 1.0) -> None:\n            Initializes the classifier with regularization parameters.\n        \n        fit(self, X: pd.DataFrame, y: pd.Series) -> 'LogisticRegressionClassifier':\n            Fits the logistic regression model to the training data.\n        \n        predict(self, X: pd.DataFrame) -> pd.Series:\n            Predicts class labels for the input data.\n    \"\"\"\n\n    def __init__(self, penalty: str='l2', C: float=1.0) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'LogisticRegressionClassifier':\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        pass\n\nclass MultinomialLogisticRegressionClassifier(BaseAlgorithm):\n    \"\"\"\n    MultinomialLogisticRegressionClassifier extends logistic regression for multiclass problems.\n    \n    This classifier is designed to handle multinomial (multiclass) logistic regression tasks\n    where the response variable can take more than two classes. It incorporates methods to fit\n    the model and predict class probabilities.\n\n    Methods:\n        __init__(self, penalty: str = 'l2', C: float = 1.0) -> None:\n            Initializes the classifier with parameters suitable for multinomial regression.\n        \n        fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultinomialLogisticRegressionClassifier':\n            Trains the model using the provided training data.\n        \n        predict(self, X: pd.DataFrame) -> pd.Series:\n            Generates predictions for the input data.\n    \"\"\"\n\n    def __init__(self, penalty: str='l2', C: float=1.0) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultinomialLogisticRegressionClassifier':\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        pass\n\nclass ProbitRegressionClassifier(BaseAlgorithm):\n    \"\"\"\n    ProbitRegressionClassifier implements the probit regression model.\n    \n    This classifier uses the cumulative distribution function of the standard normal distribution\n    as the link function. It is useful in scenarios where the assumption of logistic regression may not hold.\n\n    Methods:\n        __init__(self) -> None:\n            Initializes the probit regression classifier.\n        \n        fit(self, X: pd.DataFrame, y: pd.Series) -> 'ProbitRegressionClassifier':\n            Fits the probit regression model to the training data.\n        \n        predict(self, X: pd.DataFrame) -> pd.Series:\n            Generates predictions based on the fitted model.\n    \"\"\"\n\n    def __init__(self) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'ProbitRegressionClassifier':\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        pass\n\ndef softmax(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the softmax function for each row of the input array.\n    \n    The softmax function is used to convert raw model output scores into probabilities that sum to 1.\n    This is particularly useful for multiclass classification problems.\n\n    Args:\n        z (np.ndarray): A 2D array of shape (n_samples, n_classes) containing raw scores.\n\n    Returns:\n        np.ndarray: A 2D array of the same shape as z, where each row represents a probability distribution\n                    over the classes.\n\n    Edge Cases:\n        - If the input is empty, returns an empty array.\n        - Numerical stability should be ensured by subtracting the max value in each row before exponentiation.\n    \"\"\"\n    pass\n\ndef compute_cross_entropy_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Computes the cross-entropy loss between true labels and predicted probabilities.\n    \n    Cross-entropy loss is commonly used as a cost function for classification models,\n    particularly in logistic and multinomial logistic regression.\n\n    Args:\n        y_true (np.ndarray): Array of true labels (one-hot encoded or as indices).\n        y_pred (np.ndarray): Array of predicted probabilities for each class.\n\n    Returns:\n        float: The computed cross-entropy loss value.\n\n    Edge Cases:\n        - Handles cases where predicted probabilities might be zero by clipping values.\n        - Assumes y_true and y_pred have compatible shapes.\n    \"\"\"\n    pass\n\ndef compute_log_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Computes the log loss (negative log-likelihood) for classification predictions.\n    \n    Log loss is equivalent to cross-entropy loss and is used to evaluate the performance of\n    classifiers built using probabilistic approaches like logistic regression.\n\n    Args:\n        y_true (np.ndarray): Array of true binary or categorical labels (appropriately encoded).\n        y_pred (np.ndarray): Array of predicted probabilities for the positive class or each class.\n\n    Returns:\n        float: The calculated log loss value.\n\n    Edge Cases:\n        - Applies clipping to y_pred to prevent taking log of zero.\n        - Assumes the input arrays are of compatible sizes.\n    \"\"\"\n    pass\n\ndef optimize_threshold(y_true: np.ndarray, y_scores: np.ndarray, metric: str='f1') -> float:\n    \"\"\"\n    Optimizes the classification threshold based on a given performance metric.\n    \n    This function iterates over possible threshold values to determine the threshold that maximizes \n    the specified evaluation metric (e.g., F1 score). It is particularly useful in logistic regression,\n    where adjusting the threshold can balance precision and recall.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels.\n        y_scores (np.ndarray): Array of predicted scores or probabilities.\n        metric (str, optional): The performance metric to optimize. Default is \"f1\".\n\n    Returns:\n        float: The optimal threshold value that maximizes the specified metric.\n\n    Edge Cases:\n        - If y_scores is empty, the function should handle the error gracefully.\n        - Assumes that y_true and y_scores are aligned and of equal length.\n    \"\"\"\n    pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "from typing import Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "import numpy as np",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import numpy as np",
                                                            "lineno": 5
                                                        },
                                                        {
                                                            "name": "LogisticRegressionClassifier",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class LogisticRegressionClassifier(BaseAlgorithm):\n    \"\"\"\n    LogisticRegressionClassifier implements the basic logistic regression algorithm.\n    \n    This classifier provides methods to train a logistic regression model on provided data\n    and to generate predictions. It encapsulates the standard logistic regression functionality,\n    addressing binary classification tasks.\n\n    Methods:\n        __init__(self, penalty: str = 'l2', C: float = 1.0) -> None:\n            Initializes the classifier with regularization parameters.\n        \n        fit(self, X: pd.DataFrame, y: pd.Series) -> 'LogisticRegressionClassifier':\n            Fits the logistic regression model to the training data.\n        \n        predict(self, X: pd.DataFrame) -> pd.Series:\n            Predicts class labels for the input data.\n    \"\"\"\n\n    def __init__(self, penalty: str='l2', C: float=1.0) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'LogisticRegressionClassifier':\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        pass",
                                                            "lineno": 7
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": "LogisticRegressionClassifier",
                                                            "extra": {},
                                                            "code": "def __init__(self, penalty: str='l2', C: float=1.0) -> None:\n    pass",
                                                            "lineno": 26
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": "LogisticRegressionClassifier",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'LogisticRegressionClassifier':\n    pass",
                                                            "lineno": 29
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": "LogisticRegressionClassifier",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    pass",
                                                            "lineno": 32
                                                        },
                                                        {
                                                            "name": "MultinomialLogisticRegressionClassifier",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class MultinomialLogisticRegressionClassifier(BaseAlgorithm):\n    \"\"\"\n    MultinomialLogisticRegressionClassifier extends logistic regression for multiclass problems.\n    \n    This classifier is designed to handle multinomial (multiclass) logistic regression tasks\n    where the response variable can take more than two classes. It incorporates methods to fit\n    the model and predict class probabilities.\n\n    Methods:\n        __init__(self, penalty: str = 'l2', C: float = 1.0) -> None:\n            Initializes the classifier with parameters suitable for multinomial regression.\n        \n        fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultinomialLogisticRegressionClassifier':\n            Trains the model using the provided training data.\n        \n        predict(self, X: pd.DataFrame) -> pd.Series:\n            Generates predictions for the input data.\n    \"\"\"\n\n    def __init__(self, penalty: str='l2', C: float=1.0) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultinomialLogisticRegressionClassifier':\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        pass",
                                                            "lineno": 35
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": "MultinomialLogisticRegressionClassifier",
                                                            "extra": {},
                                                            "code": "def __init__(self, penalty: str='l2', C: float=1.0) -> None:\n    pass",
                                                            "lineno": 54
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": "MultinomialLogisticRegressionClassifier",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultinomialLogisticRegressionClassifier':\n    pass",
                                                            "lineno": 57
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": "MultinomialLogisticRegressionClassifier",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    pass",
                                                            "lineno": 60
                                                        },
                                                        {
                                                            "name": "ProbitRegressionClassifier",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class ProbitRegressionClassifier(BaseAlgorithm):\n    \"\"\"\n    ProbitRegressionClassifier implements the probit regression model.\n    \n    This classifier uses the cumulative distribution function of the standard normal distribution\n    as the link function. It is useful in scenarios where the assumption of logistic regression may not hold.\n\n    Methods:\n        __init__(self) -> None:\n            Initializes the probit regression classifier.\n        \n        fit(self, X: pd.DataFrame, y: pd.Series) -> 'ProbitRegressionClassifier':\n            Fits the probit regression model to the training data.\n        \n        predict(self, X: pd.DataFrame) -> pd.Series:\n            Generates predictions based on the fitted model.\n    \"\"\"\n\n    def __init__(self) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'ProbitRegressionClassifier':\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        pass",
                                                            "lineno": 63
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": "ProbitRegressionClassifier",
                                                            "extra": {},
                                                            "code": "def __init__(self) -> None:\n    pass",
                                                            "lineno": 81
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": "ProbitRegressionClassifier",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'ProbitRegressionClassifier':\n    pass",
                                                            "lineno": 84
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": "ProbitRegressionClassifier",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    pass",
                                                            "lineno": 87
                                                        },
                                                        {
                                                            "name": "softmax",
                                                            "unit_type": "function",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def softmax(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the softmax function for each row of the input array.\n    \n    The softmax function is used to convert raw model output scores into probabilities that sum to 1.\n    This is particularly useful for multiclass classification problems.\n\n    Args:\n        z (np.ndarray): A 2D array of shape (n_samples, n_classes) containing raw scores.\n\n    Returns:\n        np.ndarray: A 2D array of the same shape as z, where each row represents a probability distribution\n                    over the classes.\n\n    Edge Cases:\n        - If the input is empty, returns an empty array.\n        - Numerical stability should be ensured by subtracting the max value in each row before exponentiation.\n    \"\"\"\n    pass",
                                                            "lineno": 90
                                                        },
                                                        {
                                                            "name": "compute_cross_entropy_loss",
                                                            "unit_type": "function",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def compute_cross_entropy_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Computes the cross-entropy loss between true labels and predicted probabilities.\n    \n    Cross-entropy loss is commonly used as a cost function for classification models,\n    particularly in logistic and multinomial logistic regression.\n\n    Args:\n        y_true (np.ndarray): Array of true labels (one-hot encoded or as indices).\n        y_pred (np.ndarray): Array of predicted probabilities for each class.\n\n    Returns:\n        float: The computed cross-entropy loss value.\n\n    Edge Cases:\n        - Handles cases where predicted probabilities might be zero by clipping values.\n        - Assumes y_true and y_pred have compatible shapes.\n    \"\"\"\n    pass",
                                                            "lineno": 110
                                                        },
                                                        {
                                                            "name": "compute_log_loss",
                                                            "unit_type": "function",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def compute_log_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Computes the log loss (negative log-likelihood) for classification predictions.\n    \n    Log loss is equivalent to cross-entropy loss and is used to evaluate the performance of\n    classifiers built using probabilistic approaches like logistic regression.\n\n    Args:\n        y_true (np.ndarray): Array of true binary or categorical labels (appropriately encoded).\n        y_pred (np.ndarray): Array of predicted probabilities for the positive class or each class.\n\n    Returns:\n        float: The calculated log loss value.\n\n    Edge Cases:\n        - Applies clipping to y_pred to prevent taking log of zero.\n        - Assumes the input arrays are of compatible sizes.\n    \"\"\"\n    pass",
                                                            "lineno": 130
                                                        },
                                                        {
                                                            "name": "optimize_threshold",
                                                            "unit_type": "function",
                                                            "file_path": "./src/algorithms/supervised/classification/logistic_regression.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "def optimize_threshold(y_true: np.ndarray, y_scores: np.ndarray, metric: str='f1') -> float:\n    \"\"\"\n    Optimizes the classification threshold based on a given performance metric.\n    \n    This function iterates over possible threshold values to determine the threshold that maximizes \n    the specified evaluation metric (e.g., F1 score). It is particularly useful in logistic regression,\n    where adjusting the threshold can balance precision and recall.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels.\n        y_scores (np.ndarray): Array of predicted scores or probabilities.\n        metric (str, optional): The performance metric to optimize. Default is \"f1\".\n\n    Returns:\n        float: The optimal threshold value that maximizes the specified metric.\n\n    Edge Cases:\n        - If y_scores is empty, the function should handle the error gracefully.\n        - Assumes that y_true and y_scores are aligned and of equal length.\n    \"\"\"\n    pass",
                                                            "lineno": 150
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "imbalanced_classification.py",
                                                    "path": "./src/algorithms/supervised/classification/imbalanced_classification.py",
                                                    "code": "from algorithms.base_algorithm import BaseAlgorithm\nimport pandas as pd\n\nclass ImbalancedClassifier(BaseAlgorithm):\n    \"\"\"\n    A classifier tailored to address imbalanced classification problems.\n\n    This classifier implements the core methods for fitting and predicting on datasets\n    where class distribution is skewed. It inherits from the BaseAlgorithm and integrates\n    seamlessly with the overall machine learning framework. The classifier allows for the\n    specification of a strategy to handle class imbalances (e.g., undersampling, oversampling,\n    or synthetic sample generation).\n\n    Args:\n        strategy (str): The strategy used to mitigate class imbalance. Common options include\n                        'undersample', 'oversample', or 'smote'. Defaults to 'undersample'.\n\n    Methods:\n        __init__(strategy: str = 'undersample') -> None:\n            Initializes the classifier with the given imbalance handling strategy.\n        \n        fit(X: pd.DataFrame, y: pd.Series) -> ImbalancedClassifier:\n            Trains the classifier on the provided training data and labels.\n        \n        predict(X: pd.DataFrame) -> pd.Series:\n            Generates predictions for the specified input features.\n\n    Returns:\n        An instance of the ImbalancedClassifier after fitting the model.\n\n    Assumptions and Edge Cases:\n        - Assumes preprocessed input where missing data and data scaling have been addressed.\n        - The actual internal logic for handling imbalanced data is not implemented here.\n    \"\"\"\n\n    def __init__(self, strategy: str='undersample') -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'ImbalancedClassifier':\n        \"\"\"\n        Train the imbalanced classifier on the provided dataset.\n\n        Args:\n            X (pd.DataFrame): The feature set used for training.\n            y (pd.Series): The target labels corresponding to the training data.\n\n        Returns:\n            ImbalancedClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions from the imbalanced classifier for the provided input data.\n\n        Args:\n            X (pd.DataFrame): The feature set for which predictions are to be made.\n\n        Returns:\n            pd.Series: The predicted class labels.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/imbalanced_classification.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/imbalanced_classification.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "ImbalancedClassifier",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/classification/imbalanced_classification.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class ImbalancedClassifier(BaseAlgorithm):\n    \"\"\"\n    A classifier tailored to address imbalanced classification problems.\n\n    This classifier implements the core methods for fitting and predicting on datasets\n    where class distribution is skewed. It inherits from the BaseAlgorithm and integrates\n    seamlessly with the overall machine learning framework. The classifier allows for the\n    specification of a strategy to handle class imbalances (e.g., undersampling, oversampling,\n    or synthetic sample generation).\n\n    Args:\n        strategy (str): The strategy used to mitigate class imbalance. Common options include\n                        'undersample', 'oversample', or 'smote'. Defaults to 'undersample'.\n\n    Methods:\n        __init__(strategy: str = 'undersample') -> None:\n            Initializes the classifier with the given imbalance handling strategy.\n        \n        fit(X: pd.DataFrame, y: pd.Series) -> ImbalancedClassifier:\n            Trains the classifier on the provided training data and labels.\n        \n        predict(X: pd.DataFrame) -> pd.Series:\n            Generates predictions for the specified input features.\n\n    Returns:\n        An instance of the ImbalancedClassifier after fitting the model.\n\n    Assumptions and Edge Cases:\n        - Assumes preprocessed input where missing data and data scaling have been addressed.\n        - The actual internal logic for handling imbalanced data is not implemented here.\n    \"\"\"\n\n    def __init__(self, strategy: str='undersample') -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'ImbalancedClassifier':\n        \"\"\"\n        Train the imbalanced classifier on the provided dataset.\n\n        Args:\n            X (pd.DataFrame): The feature set used for training.\n            y (pd.Series): The target labels corresponding to the training data.\n\n        Returns:\n            ImbalancedClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions from the imbalanced classifier for the provided input data.\n\n        Args:\n            X (pd.DataFrame): The feature set for which predictions are to be made.\n\n        Returns:\n            pd.Series: The predicted class labels.\n        \"\"\"\n        pass",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/imbalanced_classification.py",
                                                            "parent": "ImbalancedClassifier",
                                                            "extra": {},
                                                            "code": "def __init__(self, strategy: str='undersample') -> None:\n    pass",
                                                            "lineno": 36
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/imbalanced_classification.py",
                                                            "parent": "ImbalancedClassifier",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'ImbalancedClassifier':\n    \"\"\"\n        Train the imbalanced classifier on the provided dataset.\n\n        Args:\n            X (pd.DataFrame): The feature set used for training.\n            y (pd.Series): The target labels corresponding to the training data.\n\n        Returns:\n            ImbalancedClassifier: The fitted classifier instance.\n        \"\"\"\n    pass",
                                                            "lineno": 39
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/imbalanced_classification.py",
                                                            "parent": "ImbalancedClassifier",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    \"\"\"\n        Generate predictions from the imbalanced classifier for the provided input data.\n\n        Args:\n            X (pd.DataFrame): The feature set for which predictions are to be made.\n\n        Returns:\n            pd.Series: The predicted class labels.\n        \"\"\"\n    pass",
                                                            "lineno": 52
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "multi_label.py",
                                                    "path": "./src/algorithms/supervised/classification/multi_label.py",
                                                    "code": "import numpy as np\nfrom typing import Any, Optional\nfrom typing import Any, List\nfrom algorithms.base_algorithm import BaseAlgorithm\nimport pandas as pd\nfrom typing import Any\n\nclass MultiLabelKNNClassifier(BaseAlgorithm):\n    \"\"\"\n    MultiLabelKNNClassifier implements a k-nearest neighbors method tailored for multi-label classification.\n\n    This classifier applies the KNN algorithm in scenarios where each instance may be associated with multiple labels.\n    It computes distances between instances and aggregates the labels from the k nearest neighbors to derive multi-label predictions.\n\n    Args:\n        k (int): Number of nearest neighbors to consider.\n        metric (str): Distance metric to be used (e.g., 'euclidean', 'manhattan'). Defaults to 'euclidean'.\n\n    Methods:\n        fit(X, y): Fit the classifier using the training data.\n        predict(X): Predict multi-label targets for the provided input data.\n\n    Attributes:\n        trained (bool): Indicates whether the classifier has been fitted.\n    \"\"\"\n\n    def __init__(self, k: int=5, metric: str='euclidean') -> None:\n        self.k = k\n        self.metric = metric\n        self.trained = False\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'MultiLabelKNNClassifier':\n        \"\"\"\n        Fit the multi-label KNN classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe used for training.\n            y (pd.DataFrame): Dataframe containing multi-label targets.\n\n        Returns:\n            MultiLabelKNNClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Predict multi-label targets for new data instances.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for which predictions are to be made.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the predicted multi-label outputs.\n        \"\"\"\n        pass\n\nclass ClassifierChainsClassifier(BaseAlgorithm):\n    \"\"\"\n    ClassifierChainsClassifier implements the classifier chains approach for multi-label classification.\n\n    This technique builds a chain of binary classifiers where each classifier predicts one label while taking into\n    account the predictions of previous classifiers in the chain, enabling the capture of label dependencies.\n\n    Args:\n        base_estimator (Any): The base classifier to be used for each link in the chain.\n        order (List[int], optional): A list defining the order in which labels are predicted. If None, a default order is applied.\n\n    Methods:\n        fit(X, y): Train the chain of classifiers on the training data.\n        predict(X): Generate multi-label predictions by processing the input sequentially through the chain.\n\n    Attributes:\n        chain (List[Any]): List of individual classifiers fitted in the chain.\n    \"\"\"\n\n    def __init__(self, base_estimator: Any, order: List[int]=None) -> None:\n        self.base_estimator = base_estimator\n        self.order = order\n        self.chain = []\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'ClassifierChainsClassifier':\n        \"\"\"\n        Fit the classifier chain using the provided training data.\n\n        Args:\n            X (pd.DataFrame): Feature set for training.\n            y (pd.DataFrame): Dataframe of multi-label targets.\n\n        Returns:\n            ClassifierChainsClassifier: The fitted classifier chain instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Predict multi-label outputs by applying the classifier chain to the input data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for prediction.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the multi-label predictions.\n        \"\"\"\n        pass\n\nclass MultiLabelDecisionTreeClassifier(BaseAlgorithm):\n    \"\"\"\n    MultiLabelDecisionTreeClassifier implements a decision tree approach adapted for multi-label classification.\n\n    This classifier extends traditional decision tree methodologies to handle cases where instances may belong to multiple\n    classes. It adjusts split criteria and node decisions to accommodate the simultaneous prediction of multiple labels.\n\n    Args:\n        max_depth (Optional[int]): Maximum depth of the decision tree. If None, the tree is expanded until all leaves are pure.\n        min_samples_split (int): Minimum number of samples required to split an internal node. Defaults to 2.\n\n    Methods:\n        fit(X, y): Train the multi-label decision tree using the provided training data.\n        predict(X): Predict multi-label targets for new data instances.\n\n    Attributes:\n        tree_structure (Any): An internal representation of the constructed decision tree.\n    \"\"\"\n\n    def __init__(self, max_depth: Optional[int]=None, min_samples_split: int=2) -> None:\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.tree_structure = None\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'MultiLabelDecisionTreeClassifier':\n        \"\"\"\n        Fit the multi-label decision tree classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for training.\n            y (pd.DataFrame): Dataframe containing multi-label target values.\n\n        Returns:\n            MultiLabelDecisionTreeClassifier: The fitted decision tree classifier.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Predict multi-label outcomes for new data using the trained decision tree.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for which predictions are required.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the predicted multi-label outputs.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import numpy as np",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import numpy as np",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import Any, Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any, Optional",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "from typing import Any, List",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any, List",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from algorithms.base_algorithm import BaseAlgorithm",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 5
                                                        },
                                                        {
                                                            "name": "from typing import Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any",
                                                            "lineno": 6
                                                        },
                                                        {
                                                            "name": "MultiLabelKNNClassifier",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class MultiLabelKNNClassifier(BaseAlgorithm):\n    \"\"\"\n    MultiLabelKNNClassifier implements a k-nearest neighbors method tailored for multi-label classification.\n\n    This classifier applies the KNN algorithm in scenarios where each instance may be associated with multiple labels.\n    It computes distances between instances and aggregates the labels from the k nearest neighbors to derive multi-label predictions.\n\n    Args:\n        k (int): Number of nearest neighbors to consider.\n        metric (str): Distance metric to be used (e.g., 'euclidean', 'manhattan'). Defaults to 'euclidean'.\n\n    Methods:\n        fit(X, y): Fit the classifier using the training data.\n        predict(X): Predict multi-label targets for the provided input data.\n\n    Attributes:\n        trained (bool): Indicates whether the classifier has been fitted.\n    \"\"\"\n\n    def __init__(self, k: int=5, metric: str='euclidean') -> None:\n        self.k = k\n        self.metric = metric\n        self.trained = False\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'MultiLabelKNNClassifier':\n        \"\"\"\n        Fit the multi-label KNN classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe used for training.\n            y (pd.DataFrame): Dataframe containing multi-label targets.\n\n        Returns:\n            MultiLabelKNNClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Predict multi-label targets for new data instances.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for which predictions are to be made.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the predicted multi-label outputs.\n        \"\"\"\n        pass",
                                                            "lineno": 8
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": "MultiLabelKNNClassifier",
                                                            "extra": {},
                                                            "code": "def __init__(self, k: int=5, metric: str='euclidean') -> None:\n    self.k = k\n    self.metric = metric\n    self.trained = False\n    pass",
                                                            "lineno": 27
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": "MultiLabelKNNClassifier",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'MultiLabelKNNClassifier':\n    \"\"\"\n        Fit the multi-label KNN classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe used for training.\n            y (pd.DataFrame): Dataframe containing multi-label targets.\n\n        Returns:\n            MultiLabelKNNClassifier: The fitted classifier instance.\n        \"\"\"\n    pass",
                                                            "lineno": 33
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": "MultiLabelKNNClassifier",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Predict multi-label targets for new data instances.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for which predictions are to be made.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the predicted multi-label outputs.\n        \"\"\"\n    pass",
                                                            "lineno": 46
                                                        },
                                                        {
                                                            "name": "ClassifierChainsClassifier",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class ClassifierChainsClassifier(BaseAlgorithm):\n    \"\"\"\n    ClassifierChainsClassifier implements the classifier chains approach for multi-label classification.\n\n    This technique builds a chain of binary classifiers where each classifier predicts one label while taking into\n    account the predictions of previous classifiers in the chain, enabling the capture of label dependencies.\n\n    Args:\n        base_estimator (Any): The base classifier to be used for each link in the chain.\n        order (List[int], optional): A list defining the order in which labels are predicted. If None, a default order is applied.\n\n    Methods:\n        fit(X, y): Train the chain of classifiers on the training data.\n        predict(X): Generate multi-label predictions by processing the input sequentially through the chain.\n\n    Attributes:\n        chain (List[Any]): List of individual classifiers fitted in the chain.\n    \"\"\"\n\n    def __init__(self, base_estimator: Any, order: List[int]=None) -> None:\n        self.base_estimator = base_estimator\n        self.order = order\n        self.chain = []\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'ClassifierChainsClassifier':\n        \"\"\"\n        Fit the classifier chain using the provided training data.\n\n        Args:\n            X (pd.DataFrame): Feature set for training.\n            y (pd.DataFrame): Dataframe of multi-label targets.\n\n        Returns:\n            ClassifierChainsClassifier: The fitted classifier chain instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Predict multi-label outputs by applying the classifier chain to the input data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for prediction.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the multi-label predictions.\n        \"\"\"\n        pass",
                                                            "lineno": 58
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": "ClassifierChainsClassifier",
                                                            "extra": {},
                                                            "code": "def __init__(self, base_estimator: Any, order: List[int]=None) -> None:\n    self.base_estimator = base_estimator\n    self.order = order\n    self.chain = []\n    pass",
                                                            "lineno": 77
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": "ClassifierChainsClassifier",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'ClassifierChainsClassifier':\n    \"\"\"\n        Fit the classifier chain using the provided training data.\n\n        Args:\n            X (pd.DataFrame): Feature set for training.\n            y (pd.DataFrame): Dataframe of multi-label targets.\n\n        Returns:\n            ClassifierChainsClassifier: The fitted classifier chain instance.\n        \"\"\"\n    pass",
                                                            "lineno": 83
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": "ClassifierChainsClassifier",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Predict multi-label outputs by applying the classifier chain to the input data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for prediction.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the multi-label predictions.\n        \"\"\"\n    pass",
                                                            "lineno": 96
                                                        },
                                                        {
                                                            "name": "MultiLabelDecisionTreeClassifier",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class MultiLabelDecisionTreeClassifier(BaseAlgorithm):\n    \"\"\"\n    MultiLabelDecisionTreeClassifier implements a decision tree approach adapted for multi-label classification.\n\n    This classifier extends traditional decision tree methodologies to handle cases where instances may belong to multiple\n    classes. It adjusts split criteria and node decisions to accommodate the simultaneous prediction of multiple labels.\n\n    Args:\n        max_depth (Optional[int]): Maximum depth of the decision tree. If None, the tree is expanded until all leaves are pure.\n        min_samples_split (int): Minimum number of samples required to split an internal node. Defaults to 2.\n\n    Methods:\n        fit(X, y): Train the multi-label decision tree using the provided training data.\n        predict(X): Predict multi-label targets for new data instances.\n\n    Attributes:\n        tree_structure (Any): An internal representation of the constructed decision tree.\n    \"\"\"\n\n    def __init__(self, max_depth: Optional[int]=None, min_samples_split: int=2) -> None:\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.tree_structure = None\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'MultiLabelDecisionTreeClassifier':\n        \"\"\"\n        Fit the multi-label decision tree classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for training.\n            y (pd.DataFrame): Dataframe containing multi-label target values.\n\n        Returns:\n            MultiLabelDecisionTreeClassifier: The fitted decision tree classifier.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Predict multi-label outcomes for new data using the trained decision tree.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for which predictions are required.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the predicted multi-label outputs.\n        \"\"\"\n        pass",
                                                            "lineno": 108
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": "MultiLabelDecisionTreeClassifier",
                                                            "extra": {},
                                                            "code": "def __init__(self, max_depth: Optional[int]=None, min_samples_split: int=2) -> None:\n    self.max_depth = max_depth\n    self.min_samples_split = min_samples_split\n    self.tree_structure = None\n    pass",
                                                            "lineno": 127
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": "MultiLabelDecisionTreeClassifier",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> 'MultiLabelDecisionTreeClassifier':\n    \"\"\"\n        Fit the multi-label decision tree classifier on the training data.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for training.\n            y (pd.DataFrame): Dataframe containing multi-label target values.\n\n        Returns:\n            MultiLabelDecisionTreeClassifier: The fitted decision tree classifier.\n        \"\"\"\n    pass",
                                                            "lineno": 133
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/supervised/classification/multi_label.py",
                                                            "parent": "MultiLabelDecisionTreeClassifier",
                                                            "extra": {},
                                                            "code": "def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Predict multi-label outcomes for new data using the trained decision tree.\n\n        Args:\n            X (pd.DataFrame): Feature dataframe for which predictions are required.\n\n        Returns:\n            pd.DataFrame: A dataframe containing the predicted multi-label outputs.\n        \"\"\"\n    pass",
                                                            "lineno": 146
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "semi_supervised.py",
                                            "path": "./src/algorithms/supervised/semi_supervised.py",
                                            "code": "import pandas as pd\nfrom typing import Any\nfrom algorithms.base_algorithm import BaseAlgorithm\n\nclass CoTrainingClassifier(BaseAlgorithm):\n    \"\"\"\n    A classifier implementing the co-training semi-supervised learning technique.\n\n    This classifier trains two separate models on different views of the labeled data and \n    then iteratively refines each by pseudo-labeling the unlabeled data. The two models \n    are expected to complement each other, leveraging unlabeled samples to enhance learning.\n\n    Methods:\n        __init__(...): Initialize the classifier with hyperparameters and base learners.\n        fit(X, y): Fit the classifier on labeled (and optionally unlabeled) data.\n        predict(X): Generate predictions for the input data.\n\n    Args:\n        base_estimator1 (Any): The first base classifier instance.\n        base_estimator2 (Any): The second base classifier instance.\n        unlabeled_weight (float): Weighting factor for unlabeled data.\n        max_iter (int): Maximum number of iterations for the co-training process.\n\n    Returns:\n        CoTrainingClassifier: An instance of the co-training classifier after fitting.\n    \n    Edge Cases:\n        - Assumes that input data is a pandas DataFrame.\n        - The classifier expects valid and compatible base estimators.\n    \"\"\"\n\n    def __init__(self, base_estimator1: Any, base_estimator2: Any, unlabeled_weight: float=1.0, max_iter: int=10) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'CoTrainingClassifier':\n        \"\"\"\n        Fit the co-training classifier using labeled and unlabeled data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target labels for the labeled data.\n\n        Returns:\n            CoTrainingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions for the given input data.\n\n        Args:\n            X (pd.DataFrame): The data for which predictions are required.\n\n        Returns:\n            pd.Series: The predicted labels.\n        \"\"\"\n        pass\n\nclass SelfTrainingClassifier(BaseAlgorithm):\n    \"\"\"\n    A classifier implementing the self-training semi-supervised learning technique.\n\n    This classifier starts with a base estimator trained on labeled data and then iteratively \n    pseudo-labels the unlabeled data to expand the training set. The self-training mechanism \n    relies on the classifier's confidence to augment the labeled set over several iterations.\n\n    Methods:\n        __init__(...): Initialize the self-training classifier with a base estimator and parameters.\n        fit(X, y): Fit the classifier, leveraging a combination of labeled and pseudo-labeled data.\n        predict(X): Produce predictions using the trained classifier.\n\n    Args:\n        base_estimator (Any): An instance of a base classifier to be used for training.\n        confidence_threshold (float): The threshold above which predictions are considered reliable for pseudo-labeling.\n        max_iter (int): The maximum number of iterations for the self-training process.\n\n    Returns:\n        SelfTrainingClassifier: An instance of the self-training classifier after fitting.\n    \n    Edge Cases:\n        - Assumes input data is a pandas DataFrame.\n        - Relies on the base estimator having a predict_proba method for confidence estimation (if applicable).\n    \"\"\"\n\n    def __init__(self, base_estimator: Any, confidence_threshold: float=0.8, max_iter: int=10) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'SelfTrainingClassifier':\n        \"\"\"\n        Fit the self-training classifier using labeled and unlabeled data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target labels for the labeled portion of the data.\n\n        Returns:\n            SelfTrainingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions for the provided data.\n\n        Args:\n            X (pd.DataFrame): The data for which the predictions are required.\n\n        Returns:\n            pd.Series: The predicted labels.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/supervised/semi_supervised.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "from typing import Any",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/supervised/semi_supervised.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Any",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "from algorithms.base_algorithm import BaseAlgorithm",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/supervised/semi_supervised.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from algorithms.base_algorithm import BaseAlgorithm",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "CoTrainingClassifier",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/supervised/semi_supervised.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class CoTrainingClassifier(BaseAlgorithm):\n    \"\"\"\n    A classifier implementing the co-training semi-supervised learning technique.\n\n    This classifier trains two separate models on different views of the labeled data and \n    then iteratively refines each by pseudo-labeling the unlabeled data. The two models \n    are expected to complement each other, leveraging unlabeled samples to enhance learning.\n\n    Methods:\n        __init__(...): Initialize the classifier with hyperparameters and base learners.\n        fit(X, y): Fit the classifier on labeled (and optionally unlabeled) data.\n        predict(X): Generate predictions for the input data.\n\n    Args:\n        base_estimator1 (Any): The first base classifier instance.\n        base_estimator2 (Any): The second base classifier instance.\n        unlabeled_weight (float): Weighting factor for unlabeled data.\n        max_iter (int): Maximum number of iterations for the co-training process.\n\n    Returns:\n        CoTrainingClassifier: An instance of the co-training classifier after fitting.\n    \n    Edge Cases:\n        - Assumes that input data is a pandas DataFrame.\n        - The classifier expects valid and compatible base estimators.\n    \"\"\"\n\n    def __init__(self, base_estimator1: Any, base_estimator2: Any, unlabeled_weight: float=1.0, max_iter: int=10) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'CoTrainingClassifier':\n        \"\"\"\n        Fit the co-training classifier using labeled and unlabeled data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target labels for the labeled data.\n\n        Returns:\n            CoTrainingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions for the given input data.\n\n        Args:\n            X (pd.DataFrame): The data for which predictions are required.\n\n        Returns:\n            pd.Series: The predicted labels.\n        \"\"\"\n        pass",
                                                    "lineno": 5
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/supervised/semi_supervised.py",
                                                    "parent": "CoTrainingClassifier",
                                                    "extra": {},
                                                    "code": "def __init__(self, base_estimator1: Any, base_estimator2: Any, unlabeled_weight: float=1.0, max_iter: int=10) -> None:\n    pass",
                                                    "lineno": 32
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/supervised/semi_supervised.py",
                                                    "parent": "CoTrainingClassifier",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'CoTrainingClassifier':\n    \"\"\"\n        Fit the co-training classifier using labeled and unlabeled data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target labels for the labeled data.\n\n        Returns:\n            CoTrainingClassifier: The fitted classifier instance.\n        \"\"\"\n    pass",
                                                    "lineno": 35
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/supervised/semi_supervised.py",
                                                    "parent": "CoTrainingClassifier",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    \"\"\"\n        Generate predictions for the given input data.\n\n        Args:\n            X (pd.DataFrame): The data for which predictions are required.\n\n        Returns:\n            pd.Series: The predicted labels.\n        \"\"\"\n    pass",
                                                    "lineno": 48
                                                },
                                                {
                                                    "name": "SelfTrainingClassifier",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/supervised/semi_supervised.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class SelfTrainingClassifier(BaseAlgorithm):\n    \"\"\"\n    A classifier implementing the self-training semi-supervised learning technique.\n\n    This classifier starts with a base estimator trained on labeled data and then iteratively \n    pseudo-labels the unlabeled data to expand the training set. The self-training mechanism \n    relies on the classifier's confidence to augment the labeled set over several iterations.\n\n    Methods:\n        __init__(...): Initialize the self-training classifier with a base estimator and parameters.\n        fit(X, y): Fit the classifier, leveraging a combination of labeled and pseudo-labeled data.\n        predict(X): Produce predictions using the trained classifier.\n\n    Args:\n        base_estimator (Any): An instance of a base classifier to be used for training.\n        confidence_threshold (float): The threshold above which predictions are considered reliable for pseudo-labeling.\n        max_iter (int): The maximum number of iterations for the self-training process.\n\n    Returns:\n        SelfTrainingClassifier: An instance of the self-training classifier after fitting.\n    \n    Edge Cases:\n        - Assumes input data is a pandas DataFrame.\n        - Relies on the base estimator having a predict_proba method for confidence estimation (if applicable).\n    \"\"\"\n\n    def __init__(self, base_estimator: Any, confidence_threshold: float=0.8, max_iter: int=10) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'SelfTrainingClassifier':\n        \"\"\"\n        Fit the self-training classifier using labeled and unlabeled data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target labels for the labeled portion of the data.\n\n        Returns:\n            SelfTrainingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions for the provided data.\n\n        Args:\n            X (pd.DataFrame): The data for which the predictions are required.\n\n        Returns:\n            pd.Series: The predicted labels.\n        \"\"\"\n        pass",
                                                    "lineno": 60
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/supervised/semi_supervised.py",
                                                    "parent": "SelfTrainingClassifier",
                                                    "extra": {},
                                                    "code": "def __init__(self, base_estimator: Any, confidence_threshold: float=0.8, max_iter: int=10) -> None:\n    pass",
                                                    "lineno": 86
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/supervised/semi_supervised.py",
                                                    "parent": "SelfTrainingClassifier",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'SelfTrainingClassifier':\n    \"\"\"\n        Fit the self-training classifier using labeled and unlabeled data.\n\n        Args:\n            X (pd.DataFrame): The input feature data.\n            y (pd.Series): The target labels for the labeled portion of the data.\n\n        Returns:\n            SelfTrainingClassifier: The fitted classifier instance.\n        \"\"\"\n    pass",
                                                    "lineno": 89
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/supervised/semi_supervised.py",
                                                    "parent": "SelfTrainingClassifier",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    \"\"\"\n        Generate predictions for the provided data.\n\n        Args:\n            X (pd.DataFrame): The data for which the predictions are required.\n\n        Returns:\n            pd.Series: The predicted labels.\n        \"\"\"\n    pass",
                                                    "lineno": 102
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "unclassified_method.py",
                                            "path": "./src/algorithms/supervised/unclassified_method.py",
                                            "code": "import pandas as pd\nfrom src.algorithms.base_algorithm import BaseAlgorithm\n\nclass UnclassifiedMethod(BaseAlgorithm):\n    \"\"\"\n    A placeholder class for implementing a miscellaneous supervised learning method (feature '5').\n    \n    This class serves as an unclassified algorithm for supervised learning that does not\n    fit into traditional algorithm categories. It implements the basic estimator interface\n    from BaseAlgorithm, including methods for fitting to training data and making predictions.\n    The specific logic for this method should be provided by further implementation.\n    \n    Methods:\n        __init__(*args, **kwargs):\n            Initializes the algorithm with optional parameters.\n        \n        fit(X: pd.DataFrame, y: pd.Series) -> 'UnclassifiedMethod':\n            Trains the algorithm on the given training data and labels.\n        \n        predict(X: pd.DataFrame) -> pd.Series:\n            Predicts target values for the given input features.\n    \n    Args:\n        *args: Variable length argument list for initialization parameters.\n        **kwargs: Arbitrary keyword arguments for configuration.\n    \n    Returns:\n        UnclassifiedMethod: An instance of the fitted algorithm after calling fit.\n    \n    Raises:\n        NotImplementedError: The methods are not implemented and will raise an error if called.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the UnclassifiedMethod algorithm with optional parameters.\n        \n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments for algorithm configuration.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'UnclassifiedMethod':\n        \"\"\"\n        Fit the unclassified method on the training data.\n        \n        Args:\n            X (pd.DataFrame): Training data features.\n            y (pd.Series): True target values for training.\n        \n        Returns:\n            UnclassifiedMethod: The fitted model instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions from the fitted unclassified method.\n        \n        Args:\n            X (pd.DataFrame): Input data features for which predictions are to be made.\n        \n        Returns:\n            pd.Series: Predicted target values.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/supervised/unclassified_method.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "from src.algorithms.base_algorithm import BaseAlgorithm",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/supervised/unclassified_method.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from src.algorithms.base_algorithm import BaseAlgorithm",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "UnclassifiedMethod",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/supervised/unclassified_method.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class UnclassifiedMethod(BaseAlgorithm):\n    \"\"\"\n    A placeholder class for implementing a miscellaneous supervised learning method (feature '5').\n    \n    This class serves as an unclassified algorithm for supervised learning that does not\n    fit into traditional algorithm categories. It implements the basic estimator interface\n    from BaseAlgorithm, including methods for fitting to training data and making predictions.\n    The specific logic for this method should be provided by further implementation.\n    \n    Methods:\n        __init__(*args, **kwargs):\n            Initializes the algorithm with optional parameters.\n        \n        fit(X: pd.DataFrame, y: pd.Series) -> 'UnclassifiedMethod':\n            Trains the algorithm on the given training data and labels.\n        \n        predict(X: pd.DataFrame) -> pd.Series:\n            Predicts target values for the given input features.\n    \n    Args:\n        *args: Variable length argument list for initialization parameters.\n        **kwargs: Arbitrary keyword arguments for configuration.\n    \n    Returns:\n        UnclassifiedMethod: An instance of the fitted algorithm after calling fit.\n    \n    Raises:\n        NotImplementedError: The methods are not implemented and will raise an error if called.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the UnclassifiedMethod algorithm with optional parameters.\n        \n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments for algorithm configuration.\n        \"\"\"\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'UnclassifiedMethod':\n        \"\"\"\n        Fit the unclassified method on the training data.\n        \n        Args:\n            X (pd.DataFrame): Training data features.\n            y (pd.Series): True target values for training.\n        \n        Returns:\n            UnclassifiedMethod: The fitted model instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Generate predictions from the fitted unclassified method.\n        \n        Args:\n            X (pd.DataFrame): Input data features for which predictions are to be made.\n        \n        Returns:\n            pd.Series: Predicted target values.\n        \"\"\"\n        pass",
                                                    "lineno": 4
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/supervised/unclassified_method.py",
                                                    "parent": "UnclassifiedMethod",
                                                    "extra": {},
                                                    "code": "def __init__(self, *args, **kwargs):\n    \"\"\"\n        Initialize the UnclassifiedMethod algorithm with optional parameters.\n        \n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments for algorithm configuration.\n        \"\"\"\n    pass",
                                                    "lineno": 34
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/supervised/unclassified_method.py",
                                                    "parent": "UnclassifiedMethod",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'UnclassifiedMethod':\n    \"\"\"\n        Fit the unclassified method on the training data.\n        \n        Args:\n            X (pd.DataFrame): Training data features.\n            y (pd.Series): True target values for training.\n        \n        Returns:\n            UnclassifiedMethod: The fitted model instance.\n        \"\"\"\n    pass",
                                                    "lineno": 44
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/supervised/unclassified_method.py",
                                                    "parent": "UnclassifiedMethod",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> pd.Series:\n    \"\"\"\n        Generate predictions from the fitted unclassified method.\n        \n        Args:\n            X (pd.DataFrame): Input data features for which predictions are to be made.\n        \n        Returns:\n            pd.Series: Predicted target values.\n        \"\"\"\n    pass",
                                                    "lineno": 57
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "unsupervised",
                                    "path": "./src/algorithms/unsupervised",
                                    "children": [
                                        {
                                            "type": "directory",
                                            "name": "dimensionality_reduction",
                                            "path": "./src/algorithms/unsupervised/dimensionality_reduction",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "pca_methods.py",
                                                    "path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                    "code": "from typing import Optional\nfrom typing import Optional, Iterator\nimport pandas as pd\nfrom general.base import BaseTransformer\n\nclass KernelPCA(BaseTransformer):\n    \"\"\"\n    Perform Kernel Principal Component Analysis (Kernel PCA) for nonlinear dimensionality reduction.\n\n    This interface implements PCA using kernel methods to project data into a higher-dimensional feature space,\n    allowing for the extraction of nonlinear patterns. The transform method applies the learned projection\n    on new data.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'KernelPCA':\n            Fit the Kernel PCA model to the input data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Apply the kernel projection to reduce the dimensionality of X.\n\n    Args:\n        kernel (str): The kernel type to be used (e.g., 'rbf', 'poly', etc.).\n        n_components (int): Number of principal components to extract.\n        **kernel_params: Additional parameters for the kernel function.\n    \"\"\"\n\n    def __init__(self, kernel: str='rbf', n_components: int=2, **kernel_params):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'KernelPCA':\n        \"\"\"\n        Fit the Kernel PCA model using the given data.\n\n        Args:\n            X (pd.DataFrame): The input data for training.\n            y (Optional[pd.Series]): Not used, included for compatibility.\n\n        Returns:\n            KernelPCA: The fitted KernelPCA instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Transform the input data using the kernel PCA mapping.\n\n        Args:\n            X (pd.DataFrame): New input data to transform.\n\n        Returns:\n            pd.DataFrame: The data represented in the reduced dimensional space.\n        \"\"\"\n        pass\n\nclass StandardPCA(BaseTransformer):\n    \"\"\"\n    Perform standard Principal Component Analysis (PCA) using eigen or singular value decomposition.\n\n    This interface implements the classical PCA algorithm for dimensionality reduction. It computes the principal\n    components of the data via eigen decomposition or singular value decomposition, depending on the properties of\n    the input matrix. The resulting components can be used to reduce the dimensionality while preserving variance.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'StandardPCA':\n            Compute the principal components from the input data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Project new data onto the principal component space.\n\n    Args:\n        n_components (int): The number of principal components to compute.\n        method (str): The decomposition method ('eigen' or 'svd') to use; the selection may affect numerical stability.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, method: str='svd'):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'StandardPCA':\n        \"\"\"\n        Fit the PCA model to the training data using the specified decomposition method.\n\n        Args:\n            X (pd.DataFrame): Training data for PCA.\n            y (Optional[pd.Series]): Not used; included for API consistency.\n\n        Returns:\n            StandardPCA: The fitted PCA instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Apply the PCA transformation to reduce the dimensionality of the input data.\n\n        Args:\n            X (pd.DataFrame): Data to transform.\n\n        Returns:\n            pd.DataFrame: Data projected onto the principal components.\n        \"\"\"\n        pass\n\nclass SparsePCA(BaseTransformer):\n    \"\"\"\n    Perform Sparse Principal Component Analysis (Sparse PCA) for dimensionality reduction with sparsity constraints.\n\n    This interface implements Sparse PCA where the principal components are obtained under a sparsity constraint,\n    resulting in components with many zero loadings. This can enhance interpretability in high-dimensional data.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'SparsePCA':\n            Fit the Sparse PCA model using the input data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Transform the data into a sparse principal component space.\n\n    Args:\n        n_components (int): The number of sparse principal components to compute.\n        alpha (float): Sparsity controlling parameter, where a higher value leads to sparser components.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, alpha: float=1.0):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'SparsePCA':\n        \"\"\"\n        Fit the Sparse PCA model to the data.\n\n        Args:\n            X (pd.DataFrame): Input training data.\n            y (Optional[pd.Series]): Not used, for interface compatibility only.\n\n        Returns:\n            SparsePCA: The fitted SparsePCA instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Transform input data into the space spanned by the sparse principal components.\n\n        Args:\n            X (pd.DataFrame): New data to transform.\n\n        Returns:\n            pd.DataFrame: Data represented in the sparse principal component space.\n        \"\"\"\n        pass\n\nclass IncrementalPCA(BaseTransformer):\n    \"\"\"\n    Perform Incremental Principal Component Analysis (Incremental PCA) for large datasets that cannot be processed in memory.\n\n    This interface implements Incremental PCA which allows partial fitting of the model on mini-batches of data,\n    making it suitable for large datasets. The transformation yields principal components that approximate the standard PCA.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'IncrementalPCA':\n            Incrementally fit the PCA model on the input data.\n        partial_fit(X: pd.DataFrame) -> None:\n            Update the PCA model with a mini-batch of data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Transform new data using the incrementally learned principal components.\n\n    Args:\n        n_components (int): The number of principal components to extract incrementally.\n        batch_size (int): Size of the mini-batches to use during incremental fitting.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, batch_size: int=100):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'IncrementalPCA':\n        \"\"\"\n        Fit the Incremental PCA model using the data, processing it in mini-batches.\n\n        Args:\n            X (pd.DataFrame): Input data for incremental fitting.\n            y (Optional[pd.Series]): Not used; provided for compatibility.\n\n        Returns:\n            IncrementalPCA: The fitted IncrementalPCA model.\n        \"\"\"\n        pass\n\n    def partial_fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Update the PCA model with a mini-batch of data.\n\n        Args:\n            X (pd.DataFrame): A mini-batch of input data.\n\n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Transform the input data using the incrementally accumulated principal components.\n\n        Args:\n            X (pd.DataFrame): New input data to be transformed.\n\n        Returns:\n            pd.DataFrame: Data represented in the reduced dimensional space.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "from typing import Optional, Iterator",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Optional, Iterator",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "from general.base import BaseTransformer",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from general.base import BaseTransformer",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "KernelPCA",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class KernelPCA(BaseTransformer):\n    \"\"\"\n    Perform Kernel Principal Component Analysis (Kernel PCA) for nonlinear dimensionality reduction.\n\n    This interface implements PCA using kernel methods to project data into a higher-dimensional feature space,\n    allowing for the extraction of nonlinear patterns. The transform method applies the learned projection\n    on new data.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'KernelPCA':\n            Fit the Kernel PCA model to the input data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Apply the kernel projection to reduce the dimensionality of X.\n\n    Args:\n        kernel (str): The kernel type to be used (e.g., 'rbf', 'poly', etc.).\n        n_components (int): Number of principal components to extract.\n        **kernel_params: Additional parameters for the kernel function.\n    \"\"\"\n\n    def __init__(self, kernel: str='rbf', n_components: int=2, **kernel_params):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'KernelPCA':\n        \"\"\"\n        Fit the Kernel PCA model using the given data.\n\n        Args:\n            X (pd.DataFrame): The input data for training.\n            y (Optional[pd.Series]): Not used, included for compatibility.\n\n        Returns:\n            KernelPCA: The fitted KernelPCA instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Transform the input data using the kernel PCA mapping.\n\n        Args:\n            X (pd.DataFrame): New input data to transform.\n\n        Returns:\n            pd.DataFrame: The data represented in the reduced dimensional space.\n        \"\"\"\n        pass",
                                                            "lineno": 6
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "KernelPCA",
                                                            "extra": {},
                                                            "code": "def __init__(self, kernel: str='rbf', n_components: int=2, **kernel_params):\n    pass",
                                                            "lineno": 26
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "KernelPCA",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'KernelPCA':\n    \"\"\"\n        Fit the Kernel PCA model using the given data.\n\n        Args:\n            X (pd.DataFrame): The input data for training.\n            y (Optional[pd.Series]): Not used, included for compatibility.\n\n        Returns:\n            KernelPCA: The fitted KernelPCA instance.\n        \"\"\"\n    pass",
                                                            "lineno": 29
                                                        },
                                                        {
                                                            "name": "transform",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "KernelPCA",
                                                            "extra": {},
                                                            "code": "def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Transform the input data using the kernel PCA mapping.\n\n        Args:\n            X (pd.DataFrame): New input data to transform.\n\n        Returns:\n            pd.DataFrame: The data represented in the reduced dimensional space.\n        \"\"\"\n    pass",
                                                            "lineno": 42
                                                        },
                                                        {
                                                            "name": "StandardPCA",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class StandardPCA(BaseTransformer):\n    \"\"\"\n    Perform standard Principal Component Analysis (PCA) using eigen or singular value decomposition.\n\n    This interface implements the classical PCA algorithm for dimensionality reduction. It computes the principal\n    components of the data via eigen decomposition or singular value decomposition, depending on the properties of\n    the input matrix. The resulting components can be used to reduce the dimensionality while preserving variance.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'StandardPCA':\n            Compute the principal components from the input data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Project new data onto the principal component space.\n\n    Args:\n        n_components (int): The number of principal components to compute.\n        method (str): The decomposition method ('eigen' or 'svd') to use; the selection may affect numerical stability.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, method: str='svd'):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'StandardPCA':\n        \"\"\"\n        Fit the PCA model to the training data using the specified decomposition method.\n\n        Args:\n            X (pd.DataFrame): Training data for PCA.\n            y (Optional[pd.Series]): Not used; included for API consistency.\n\n        Returns:\n            StandardPCA: The fitted PCA instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Apply the PCA transformation to reduce the dimensionality of the input data.\n\n        Args:\n            X (pd.DataFrame): Data to transform.\n\n        Returns:\n            pd.DataFrame: Data projected onto the principal components.\n        \"\"\"\n        pass",
                                                            "lineno": 54
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "StandardPCA",
                                                            "extra": {},
                                                            "code": "def __init__(self, n_components: int=2, method: str='svd'):\n    pass",
                                                            "lineno": 73
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "StandardPCA",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'StandardPCA':\n    \"\"\"\n        Fit the PCA model to the training data using the specified decomposition method.\n\n        Args:\n            X (pd.DataFrame): Training data for PCA.\n            y (Optional[pd.Series]): Not used; included for API consistency.\n\n        Returns:\n            StandardPCA: The fitted PCA instance.\n        \"\"\"\n    pass",
                                                            "lineno": 76
                                                        },
                                                        {
                                                            "name": "transform",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "StandardPCA",
                                                            "extra": {},
                                                            "code": "def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Apply the PCA transformation to reduce the dimensionality of the input data.\n\n        Args:\n            X (pd.DataFrame): Data to transform.\n\n        Returns:\n            pd.DataFrame: Data projected onto the principal components.\n        \"\"\"\n    pass",
                                                            "lineno": 89
                                                        },
                                                        {
                                                            "name": "SparsePCA",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class SparsePCA(BaseTransformer):\n    \"\"\"\n    Perform Sparse Principal Component Analysis (Sparse PCA) for dimensionality reduction with sparsity constraints.\n\n    This interface implements Sparse PCA where the principal components are obtained under a sparsity constraint,\n    resulting in components with many zero loadings. This can enhance interpretability in high-dimensional data.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'SparsePCA':\n            Fit the Sparse PCA model using the input data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Transform the data into a sparse principal component space.\n\n    Args:\n        n_components (int): The number of sparse principal components to compute.\n        alpha (float): Sparsity controlling parameter, where a higher value leads to sparser components.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, alpha: float=1.0):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'SparsePCA':\n        \"\"\"\n        Fit the Sparse PCA model to the data.\n\n        Args:\n            X (pd.DataFrame): Input training data.\n            y (Optional[pd.Series]): Not used, for interface compatibility only.\n\n        Returns:\n            SparsePCA: The fitted SparsePCA instance.\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Transform input data into the space spanned by the sparse principal components.\n\n        Args:\n            X (pd.DataFrame): New data to transform.\n\n        Returns:\n            pd.DataFrame: Data represented in the sparse principal component space.\n        \"\"\"\n        pass",
                                                            "lineno": 101
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "SparsePCA",
                                                            "extra": {},
                                                            "code": "def __init__(self, n_components: int=2, alpha: float=1.0):\n    pass",
                                                            "lineno": 119
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "SparsePCA",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'SparsePCA':\n    \"\"\"\n        Fit the Sparse PCA model to the data.\n\n        Args:\n            X (pd.DataFrame): Input training data.\n            y (Optional[pd.Series]): Not used, for interface compatibility only.\n\n        Returns:\n            SparsePCA: The fitted SparsePCA instance.\n        \"\"\"\n    pass",
                                                            "lineno": 122
                                                        },
                                                        {
                                                            "name": "transform",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "SparsePCA",
                                                            "extra": {},
                                                            "code": "def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Transform input data into the space spanned by the sparse principal components.\n\n        Args:\n            X (pd.DataFrame): New data to transform.\n\n        Returns:\n            pd.DataFrame: Data represented in the sparse principal component space.\n        \"\"\"\n    pass",
                                                            "lineno": 135
                                                        },
                                                        {
                                                            "name": "IncrementalPCA",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class IncrementalPCA(BaseTransformer):\n    \"\"\"\n    Perform Incremental Principal Component Analysis (Incremental PCA) for large datasets that cannot be processed in memory.\n\n    This interface implements Incremental PCA which allows partial fitting of the model on mini-batches of data,\n    making it suitable for large datasets. The transformation yields principal components that approximate the standard PCA.\n\n    Methods:\n        fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'IncrementalPCA':\n            Incrementally fit the PCA model on the input data.\n        partial_fit(X: pd.DataFrame) -> None:\n            Update the PCA model with a mini-batch of data.\n        transform(X: pd.DataFrame) -> pd.DataFrame:\n            Transform new data using the incrementally learned principal components.\n\n    Args:\n        n_components (int): The number of principal components to extract incrementally.\n        batch_size (int): Size of the mini-batches to use during incremental fitting.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, batch_size: int=100):\n        pass\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'IncrementalPCA':\n        \"\"\"\n        Fit the Incremental PCA model using the data, processing it in mini-batches.\n\n        Args:\n            X (pd.DataFrame): Input data for incremental fitting.\n            y (Optional[pd.Series]): Not used; provided for compatibility.\n\n        Returns:\n            IncrementalPCA: The fitted IncrementalPCA model.\n        \"\"\"\n        pass\n\n    def partial_fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Update the PCA model with a mini-batch of data.\n\n        Args:\n            X (pd.DataFrame): A mini-batch of input data.\n\n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Transform the input data using the incrementally accumulated principal components.\n\n        Args:\n            X (pd.DataFrame): New input data to be transformed.\n\n        Returns:\n            pd.DataFrame: Data represented in the reduced dimensional space.\n        \"\"\"\n        pass",
                                                            "lineno": 147
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "IncrementalPCA",
                                                            "extra": {},
                                                            "code": "def __init__(self, n_components: int=2, batch_size: int=100):\n    pass",
                                                            "lineno": 167
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "IncrementalPCA",
                                                            "extra": {},
                                                            "code": "def fit(self, X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'IncrementalPCA':\n    \"\"\"\n        Fit the Incremental PCA model using the data, processing it in mini-batches.\n\n        Args:\n            X (pd.DataFrame): Input data for incremental fitting.\n            y (Optional[pd.Series]): Not used; provided for compatibility.\n\n        Returns:\n            IncrementalPCA: The fitted IncrementalPCA model.\n        \"\"\"\n    pass",
                                                            "lineno": 170
                                                        },
                                                        {
                                                            "name": "partial_fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "IncrementalPCA",
                                                            "extra": {},
                                                            "code": "def partial_fit(self, X: pd.DataFrame) -> None:\n    \"\"\"\n        Update the PCA model with a mini-batch of data.\n\n        Args:\n            X (pd.DataFrame): A mini-batch of input data.\n\n        Returns:\n            None\n        \"\"\"\n    pass",
                                                            "lineno": 183
                                                        },
                                                        {
                                                            "name": "transform",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py",
                                                            "parent": "IncrementalPCA",
                                                            "extra": {},
                                                            "code": "def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n        Transform the input data using the incrementally accumulated principal components.\n\n        Args:\n            X (pd.DataFrame): New input data to be transformed.\n\n        Returns:\n            pd.DataFrame: Data represented in the reduced dimensional space.\n        \"\"\"\n    pass",
                                                            "lineno": 195
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "other_methods.py",
                                                    "path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                    "code": "import pandas as pd\nimport numpy as np\nfrom typing import Union, Optional\n\nclass UMAPReducer:\n    \"\"\"\n    UMAPReducer performs dimensionality reduction using the UMAP algorithm.\n    \n    This class implements the UMAP algorithm with configurable minimum distance and number of neighbors.\n    It is used to project high-dimensional data into a lower-dimensional space while preserving the\n    underlying structure according to the parameters provided.\n\n    Attributes:\n        min_dist (float): The minimum distance between embedded points.\n        n_neighbors (int): The number of neighboring points used to balance local versus global structure.\n    \"\"\"\n\n    def __init__(self, min_dist: float=0.1, n_neighbors: int=15) -> None:\n        \"\"\"\n        Initialize the UMAP reducer with specified parameters.\n\n        Args:\n            min_dist (float): The minimum distance between points in the embedded space.\n            n_neighbors (int): The number of neighbors to consider for each point.\n        \"\"\"\n        self.min_dist = min_dist\n        self.n_neighbors = n_neighbors\n        pass\n\n    def reduce(self, data: Union[pd.DataFrame, np.ndarray], n_components: int=2) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Reduce the dimensionality of input data using UMAP.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): The high-dimensional input data.\n            n_components (int): The target number of dimensions. Defaults to 2.\n\n        Returns:\n            pd.DataFrame or np.ndarray: The dimensionally-reduced representation of the input data.\n        \"\"\"\n        pass\n\nclass TSNEReducer:\n    \"\"\"\n    TSNEReducer performs dimensionality reduction using t-SNE.\n\n    This class encapsulates the t-SNE algorithm for mapping high-dimensional data to a lower-dimensional space.\n    It helps in visualizing clusters and patterns by minimizing the divergence between the high-dimensional and\n    low-dimensional distributions.\n\n    Attributes:\n        n_components (int): The target number of dimensions.\n        perplexity (float): The perplexity parameter balances attention between local and global aspects.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, perplexity: float=30.0) -> None:\n        \"\"\"\n        Initialize the t-SNE reducer with provided parameters.\n\n        Args:\n            n_components (int): The number of dimensions for the output space.\n            perplexity (float): The perplexity parameter affecting the balance of local and global structure.\n        \"\"\"\n        self.n_components = n_components\n        self.perplexity = perplexity\n        pass\n\n    def reduce(self, data: Union[pd.DataFrame, np.ndarray]) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Reduce the dimensionality of input data using t-SNE.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): The high-dimensional data to be reduced.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Low-dimensional representation of the input data.\n        \"\"\"\n        pass\n\nclass LDATopicModeler:\n    \"\"\"\n    LDATopicModeler performs topic modeling using Latent Dirichlet Allocation (LDA).\n\n    This class applies LDA to extract latent topics from a corpus or a feature data matrix and\n    provides functionality to retrieve both the latent topics and the topic distribution across\n    documents/samples. It is primarily used for uncovering underlying topics in text or feature data.\n\n    Attributes:\n        n_topics (int): The number of latent topics to extract.\n        random_state (Optional[int]): Seed used by the random number generator.\n    \"\"\"\n\n    def __init__(self, n_topics: int=10, random_state: Optional[int]=None) -> None:\n        \"\"\"\n        Initialize the LDA modeler with specified parameters.\n\n        Args:\n            n_topics (int): The number of topics to extract.\n            random_state (Optional[int]): Seed for reproducibility.\n        \"\"\"\n        self.n_topics = n_topics\n        self.random_state = random_state\n        pass\n\n    def fit(self, data: Union[pd.DataFrame, np.ndarray]) -> None:\n        \"\"\"\n        Fit the LDA model on the given data.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): Data matrix where rows correspond to samples/documents\n              and columns correspond to features (e.g., word counts or TF-IDF values).\n        \"\"\"\n        pass\n\n    def get_latent_topics(self) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Retrieve the latent topic representation of the training data.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Matrix where each row corresponds to the topic distribution for a sample.\n        \"\"\"\n        pass\n\n    def get_topic_distribution(self) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Obtain the topic distribution over the vocabulary.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Matrix where each row contains the distribution of words for a topic.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import pandas as pd",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import pandas as pd",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import numpy as np",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import numpy as np",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "from typing import Union, Optional",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Union, Optional",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "UMAPReducer",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class UMAPReducer:\n    \"\"\"\n    UMAPReducer performs dimensionality reduction using the UMAP algorithm.\n    \n    This class implements the UMAP algorithm with configurable minimum distance and number of neighbors.\n    It is used to project high-dimensional data into a lower-dimensional space while preserving the\n    underlying structure according to the parameters provided.\n\n    Attributes:\n        min_dist (float): The minimum distance between embedded points.\n        n_neighbors (int): The number of neighboring points used to balance local versus global structure.\n    \"\"\"\n\n    def __init__(self, min_dist: float=0.1, n_neighbors: int=15) -> None:\n        \"\"\"\n        Initialize the UMAP reducer with specified parameters.\n\n        Args:\n            min_dist (float): The minimum distance between points in the embedded space.\n            n_neighbors (int): The number of neighbors to consider for each point.\n        \"\"\"\n        self.min_dist = min_dist\n        self.n_neighbors = n_neighbors\n        pass\n\n    def reduce(self, data: Union[pd.DataFrame, np.ndarray], n_components: int=2) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Reduce the dimensionality of input data using UMAP.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): The high-dimensional input data.\n            n_components (int): The target number of dimensions. Defaults to 2.\n\n        Returns:\n            pd.DataFrame or np.ndarray: The dimensionally-reduced representation of the input data.\n        \"\"\"\n        pass",
                                                            "lineno": 5
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": "UMAPReducer",
                                                            "extra": {},
                                                            "code": "def __init__(self, min_dist: float=0.1, n_neighbors: int=15) -> None:\n    \"\"\"\n        Initialize the UMAP reducer with specified parameters.\n\n        Args:\n            min_dist (float): The minimum distance between points in the embedded space.\n            n_neighbors (int): The number of neighbors to consider for each point.\n        \"\"\"\n    self.min_dist = min_dist\n    self.n_neighbors = n_neighbors\n    pass",
                                                            "lineno": 18
                                                        },
                                                        {
                                                            "name": "reduce",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": "UMAPReducer",
                                                            "extra": {},
                                                            "code": "def reduce(self, data: Union[pd.DataFrame, np.ndarray], n_components: int=2) -> Union[pd.DataFrame, np.ndarray]:\n    \"\"\"\n        Reduce the dimensionality of input data using UMAP.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): The high-dimensional input data.\n            n_components (int): The target number of dimensions. Defaults to 2.\n\n        Returns:\n            pd.DataFrame or np.ndarray: The dimensionally-reduced representation of the input data.\n        \"\"\"\n    pass",
                                                            "lineno": 30
                                                        },
                                                        {
                                                            "name": "TSNEReducer",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class TSNEReducer:\n    \"\"\"\n    TSNEReducer performs dimensionality reduction using t-SNE.\n\n    This class encapsulates the t-SNE algorithm for mapping high-dimensional data to a lower-dimensional space.\n    It helps in visualizing clusters and patterns by minimizing the divergence between the high-dimensional and\n    low-dimensional distributions.\n\n    Attributes:\n        n_components (int): The target number of dimensions.\n        perplexity (float): The perplexity parameter balances attention between local and global aspects.\n    \"\"\"\n\n    def __init__(self, n_components: int=2, perplexity: float=30.0) -> None:\n        \"\"\"\n        Initialize the t-SNE reducer with provided parameters.\n\n        Args:\n            n_components (int): The number of dimensions for the output space.\n            perplexity (float): The perplexity parameter affecting the balance of local and global structure.\n        \"\"\"\n        self.n_components = n_components\n        self.perplexity = perplexity\n        pass\n\n    def reduce(self, data: Union[pd.DataFrame, np.ndarray]) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Reduce the dimensionality of input data using t-SNE.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): The high-dimensional data to be reduced.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Low-dimensional representation of the input data.\n        \"\"\"\n        pass",
                                                            "lineno": 43
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": "TSNEReducer",
                                                            "extra": {},
                                                            "code": "def __init__(self, n_components: int=2, perplexity: float=30.0) -> None:\n    \"\"\"\n        Initialize the t-SNE reducer with provided parameters.\n\n        Args:\n            n_components (int): The number of dimensions for the output space.\n            perplexity (float): The perplexity parameter affecting the balance of local and global structure.\n        \"\"\"\n    self.n_components = n_components\n    self.perplexity = perplexity\n    pass",
                                                            "lineno": 56
                                                        },
                                                        {
                                                            "name": "reduce",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": "TSNEReducer",
                                                            "extra": {},
                                                            "code": "def reduce(self, data: Union[pd.DataFrame, np.ndarray]) -> Union[pd.DataFrame, np.ndarray]:\n    \"\"\"\n        Reduce the dimensionality of input data using t-SNE.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): The high-dimensional data to be reduced.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Low-dimensional representation of the input data.\n        \"\"\"\n    pass",
                                                            "lineno": 68
                                                        },
                                                        {
                                                            "name": "LDATopicModeler",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class LDATopicModeler:\n    \"\"\"\n    LDATopicModeler performs topic modeling using Latent Dirichlet Allocation (LDA).\n\n    This class applies LDA to extract latent topics from a corpus or a feature data matrix and\n    provides functionality to retrieve both the latent topics and the topic distribution across\n    documents/samples. It is primarily used for uncovering underlying topics in text or feature data.\n\n    Attributes:\n        n_topics (int): The number of latent topics to extract.\n        random_state (Optional[int]): Seed used by the random number generator.\n    \"\"\"\n\n    def __init__(self, n_topics: int=10, random_state: Optional[int]=None) -> None:\n        \"\"\"\n        Initialize the LDA modeler with specified parameters.\n\n        Args:\n            n_topics (int): The number of topics to extract.\n            random_state (Optional[int]): Seed for reproducibility.\n        \"\"\"\n        self.n_topics = n_topics\n        self.random_state = random_state\n        pass\n\n    def fit(self, data: Union[pd.DataFrame, np.ndarray]) -> None:\n        \"\"\"\n        Fit the LDA model on the given data.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): Data matrix where rows correspond to samples/documents\n              and columns correspond to features (e.g., word counts or TF-IDF values).\n        \"\"\"\n        pass\n\n    def get_latent_topics(self) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Retrieve the latent topic representation of the training data.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Matrix where each row corresponds to the topic distribution for a sample.\n        \"\"\"\n        pass\n\n    def get_topic_distribution(self) -> Union[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Obtain the topic distribution over the vocabulary.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Matrix where each row contains the distribution of words for a topic.\n        \"\"\"\n        pass",
                                                            "lineno": 80
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": "LDATopicModeler",
                                                            "extra": {},
                                                            "code": "def __init__(self, n_topics: int=10, random_state: Optional[int]=None) -> None:\n    \"\"\"\n        Initialize the LDA modeler with specified parameters.\n\n        Args:\n            n_topics (int): The number of topics to extract.\n            random_state (Optional[int]): Seed for reproducibility.\n        \"\"\"\n    self.n_topics = n_topics\n    self.random_state = random_state\n    pass",
                                                            "lineno": 93
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": "LDATopicModeler",
                                                            "extra": {},
                                                            "code": "def fit(self, data: Union[pd.DataFrame, np.ndarray]) -> None:\n    \"\"\"\n        Fit the LDA model on the given data.\n\n        Args:\n            data (pd.DataFrame or np.ndarray): Data matrix where rows correspond to samples/documents\n              and columns correspond to features (e.g., word counts or TF-IDF values).\n        \"\"\"\n    pass",
                                                            "lineno": 105
                                                        },
                                                        {
                                                            "name": "get_latent_topics",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": "LDATopicModeler",
                                                            "extra": {},
                                                            "code": "def get_latent_topics(self) -> Union[pd.DataFrame, np.ndarray]:\n    \"\"\"\n        Retrieve the latent topic representation of the training data.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Matrix where each row corresponds to the topic distribution for a sample.\n        \"\"\"\n    pass",
                                                            "lineno": 115
                                                        },
                                                        {
                                                            "name": "get_topic_distribution",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/dimensionality_reduction/other_methods.py",
                                                            "parent": "LDATopicModeler",
                                                            "extra": {},
                                                            "code": "def get_topic_distribution(self) -> Union[pd.DataFrame, np.ndarray]:\n    \"\"\"\n        Obtain the topic distribution over the vocabulary.\n\n        Returns:\n            pd.DataFrame or np.ndarray: Matrix where each row contains the distribution of words for a topic.\n        \"\"\"\n    pass",
                                                            "lineno": 124
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "type": "directory",
                                            "name": "clustering",
                                            "path": "./src/algorithms/unsupervised/clustering",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "k_means.py",
                                                    "path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                    "code": "from typing import Any, Dict\nimport numpy as np\nfrom typing import Any, List\nfrom typing import Any\n\nclass KMeansInitializer:\n    \"\"\"\n    Provides various centroid initialization methods for K-means clustering.\n    \n    This class encapsulates methods for initializing cluster centroids using different strategies including:\n    - Generic initialization methods\n    - Direct centroid initialization\n    - K-means++ initialization for improved convergence.\n    \n    Methods should be used to set up the initial state for clustering algorithms by selecting starting centroids.\n    \n    Usage:\n        initializer = KMeansInitializer()\n        centroids = initializer.initialize(data, method=\"k-means++\", num_clusters=3)\n    \n    Args:\n        None\n    \n    Returns:\n        Various initialization outputs as numpy.ndarray or list of centroids.\n    \n    Edge Cases:\n        Implementations should handle cases where the input data is insufficient or contains NaN values.\n    \"\"\"\n\n    def initialize(self, data: np.ndarray, method: str='default', num_clusters: int=3) -> Any:\n        \"\"\"\n        Initialize cluster centroids for k-means clustering.\n        \n        Args:\n            data (np.ndarray): The input data array for clustering.\n            method (str): The initialization method (\"default\", \"centroid\", \"k-means++\").\n            num_clusters (int): Number of clusters for which to initialize centroids.\n            \n        Returns:\n            Any: The initialized centroids (format can vary based on method).\n        \"\"\"\n        pass\n\nclass KMeansOptimizer:\n    \"\"\"\n    Provides utilities to assess convergence, evaluate the elbow method, and optimize the number of clusters \n    for k-means based clustering algorithms.\n    \n    This class offers methods to:\n    - Check convergence criteria during iterative updates.\n    - Compute the elbow metric to help in identifying the optimal number of clusters.\n    - Optimize the cluster count automatically based on provided metrics.\n    \n    Usage:\n        optimizer = KMeansOptimizer()\n        is_converged = optimizer.check_convergence(old_centroids, new_centroids, tol=0.001)\n        elbow_score = optimizer.compute_elbow_method(data, max_k=10)\n        optimal_k = optimizer.optimize_clusters(data, max_k=10)\n    \n    Args:\n        None\n    \n    Returns:\n        Convergence flags, elbow scores, or optimal cluster counts as integers or floats.\n    \n    Edge Cases:\n        Methods should effectively handle edge cases such as not reaching convergence within a fixed number of iterations.\n    \"\"\"\n\n    def check_convergence(self, old_centroids: np.ndarray, new_centroids: np.ndarray, tol: float=0.001) -> bool:\n        \"\"\"\n        Checks whether the centroids have converged within a specified tolerance.\n        \n        Args:\n            old_centroids (np.ndarray): Centroid positions from the previous iteration.\n            new_centroids (np.ndarray): Updated centroid positions.\n            tol (float): Tolerance threshold for convergence.\n            \n        Returns:\n            bool: True if the centroids have converged; False otherwise.\n        \"\"\"\n        pass\n\n    def compute_elbow_method(self, data: np.ndarray, max_k: int) -> Dict[int, float]:\n        \"\"\"\n        Computes the elbow metric for a range of clusters to aid in identifying the optimal number of clusters.\n        \n        Args:\n            data (np.ndarray): Input data for computing the metric.\n            max_k (int): Maximum number of clusters to consider.\n            \n        Returns:\n            Dict[int, float]: Mapping of number of clusters to computed elbow values.\n        \"\"\"\n        pass\n\n    def optimize_clusters(self, data: np.ndarray, max_k: int) -> int:\n        \"\"\"\n        Determines the optimal number of clusters based on optimization criteria.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            max_k (int): Maximum number of clusters to be evaluated.\n            \n        Returns:\n            int: The optimal number of clusters.\n        \"\"\"\n        pass\n\nclass KMeansClustering:\n    \"\"\"\n    Implements the standard K-means clustering algorithm.\n    \n    This class is responsible for performing the standard iterative clustering process, including:\n    - Assignment of data points to the nearest centroids.\n    - Updating centroid positions based on cluster membership.\n    \n    Usage:\n        kmeans = KMeansClustering(num_clusters=3, max_iter=300)\n        cluster_labels = kmeans.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters to form.\n        max_iter (int): Maximum iterations for the algorithm.\n    \n    Returns:\n        fit_predict returns cluster labels as a numpy.ndarray.\n    \n    Edge Cases:\n        The implementation should handle cases with empty clusters, convergence conditions, and data containing outliers.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the K-means clustering on the provided data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the cluster labels for the provided data.\n        \n        Args:\n            data (np.ndarray): Data for which to determine cluster membership.\n            \n        Returns:\n            np.ndarray: Array of predicted cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines fit and predict steps for convenience.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            np.ndarray: Cluster labels for the input data.\n        \"\"\"\n        pass\n\nclass MiniBatchKMeansClustering:\n    \"\"\"\n    Implements the Mini-Batch K-means clustering algorithm.\n    \n    This variant follows the standard k-means algorithm but uses mini-batches to reduce computation time.\n    \n    Usage:\n        mbkmeans = MiniBatchKMeansClustering(num_clusters=3, batch_size=100)\n        labels = mbkmeans.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters.\n        batch_size (int): Size of the mini-batch.\n        max_iter (int): Maximum number of iterations.\n    \n    Returns:\n        Cluster labels as a numpy.ndarray from fit_predict.\n    \n    Edge Cases:\n        The implementation should address cases where batch size exceeds dataset length and ensure consistent convergence.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, batch_size: int=100, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the Mini-Batch K-means model on provided data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Assigns data points to the nearest cluster centroid based on the fitted model.\n        \n        Args:\n            data (np.ndarray): Data for clustering prediction.\n        \n        Returns:\n            np.ndarray: Predicted cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines the fit and predict operations.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            \n        Returns:\n            np.ndarray: Cluster labels derived from the model.\n        \"\"\"\n        pass\n\nclass SpectralClustering:\n    \"\"\"\n    Implements the spectral clustering method in the context of k-means clustering based workflows.\n    \n    This algorithm uses eigen decomposition on a similarity matrix to reduce dimensionality before clustering.\n    \n    Usage:\n        spectral = SpectralClustering(num_clusters=3, affinity=\"rbf\")\n        labels = spectral.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Desired number of clusters.\n        affinity (str): Affinity type to construct the similarity matrix.\n        n_neighbors (int, optional): Number of neighbors for graph construction.\n    \n    Returns:\n        Cluster labels as a numpy.ndarray.\n    \n    Edge Cases:\n        The method should handle situations where the similarity matrix fails to capture structure in the data.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, affinity: str='rbf', n_neighbors: int=10) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the spectral clustering model on the input data.\n        \n        Args:\n            data (np.ndarray): Data for clustering.\n            \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Assigns cluster labels based on the spectral clustering.\n        \n        Args:\n            data (np.ndarray): Data for which clustering is to be predicted.\n            \n        Returns:\n            np.ndarray: Predicted cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines fitting and predicting into a single step.\n        \n        Args:\n            data (np.ndarray): Data to be clustered.\n            \n        Returns:\n            np.ndarray: Cluster labels for the input data.\n        \"\"\"\n        pass\n\nclass KMedoidsClustering:\n    \"\"\"\n    Implements the k-medoids clustering algorithm, an alternative to k-means that selects actual data points as centers.\n    \n    Usage:\n        kmedoids = KMedoidsClustering(num_clusters=3, max_iter=300)\n        labels = kmedoids.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters.\n        max_iter (int): Maximum number of iterations.\n    \n    Returns:\n        np.ndarray: Predicted cluster labels.\n    \n    Edge Cases:\n        Should handle data with outliers robustly by selecting medoids that minimize dissimilarity.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the k-medoids model to the input data.\n        \n        Args:\n            data (np.ndarray): Data used for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts cluster membership for input data based on medoid distances.\n        \n        Args:\n            data (np.ndarray): Input data.\n            \n        Returns:\n            np.ndarray: Cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Executes fitting and prediction in a single step.\n        \n        Args:\n            data (np.ndarray): Data for clustering.\n            \n        Returns:\n            np.ndarray: Cluster assignments.\n        \"\"\"\n        pass\n\nclass FuzzyKMeansClustering:\n    \"\"\"\n    Implements the fuzzy k-means (soft clustering) algorithm where each data point can belong\n    to multiple clusters with varying degrees of membership.\n    \n    Usage:\n        fuzzy_kmeans = FuzzyKMeansClustering(num_clusters=3, m=2.0, max_iter=300)\n        memberships = fuzzy_kmeans.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters.\n        m (float): Fuzziness parameter that controls the degree of membership sharing.\n        max_iter (int): Maximum number of iterations for convergence.\n    \n    Returns:\n        np.ndarray: Membership matrix indicating the degree of belonging to each cluster.\n    \n    Edge Cases:\n        Should handle cases where the fuzziness parameter may lead to degenerate membership assignments.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, m: float=2.0, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the fuzzy k-means model to the input data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the fuzzy cluster memberships for the provided data.\n        \n        Args:\n            data (np.ndarray): Data for which to compute cluster memberships.\n            \n        Returns:\n            np.ndarray: A membership matrix where each row sums to 1.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines fitting and predicting of fuzzy cluster memberships.\n        \n        Args:\n            data (np.ndarray): Input data for fuzzy clustering.\n            \n        Returns:\n            np.ndarray: The membership matrix.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import Any, Dict",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any, Dict",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import numpy as np",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import numpy as np",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "from typing import Any, List",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any, List",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "from typing import Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "KMeansInitializer",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class KMeansInitializer:\n    \"\"\"\n    Provides various centroid initialization methods for K-means clustering.\n    \n    This class encapsulates methods for initializing cluster centroids using different strategies including:\n    - Generic initialization methods\n    - Direct centroid initialization\n    - K-means++ initialization for improved convergence.\n    \n    Methods should be used to set up the initial state for clustering algorithms by selecting starting centroids.\n    \n    Usage:\n        initializer = KMeansInitializer()\n        centroids = initializer.initialize(data, method=\"k-means++\", num_clusters=3)\n    \n    Args:\n        None\n    \n    Returns:\n        Various initialization outputs as numpy.ndarray or list of centroids.\n    \n    Edge Cases:\n        Implementations should handle cases where the input data is insufficient or contains NaN values.\n    \"\"\"\n\n    def initialize(self, data: np.ndarray, method: str='default', num_clusters: int=3) -> Any:\n        \"\"\"\n        Initialize cluster centroids for k-means clustering.\n        \n        Args:\n            data (np.ndarray): The input data array for clustering.\n            method (str): The initialization method (\"default\", \"centroid\", \"k-means++\").\n            num_clusters (int): Number of clusters for which to initialize centroids.\n            \n        Returns:\n            Any: The initialized centroids (format can vary based on method).\n        \"\"\"\n        pass",
                                                            "lineno": 6
                                                        },
                                                        {
                                                            "name": "initialize",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMeansInitializer",
                                                            "extra": {},
                                                            "code": "def initialize(self, data: np.ndarray, method: str='default', num_clusters: int=3) -> Any:\n    \"\"\"\n        Initialize cluster centroids for k-means clustering.\n        \n        Args:\n            data (np.ndarray): The input data array for clustering.\n            method (str): The initialization method (\"default\", \"centroid\", \"k-means++\").\n            num_clusters (int): Number of clusters for which to initialize centroids.\n            \n        Returns:\n            Any: The initialized centroids (format can vary based on method).\n        \"\"\"\n    pass",
                                                            "lineno": 31
                                                        },
                                                        {
                                                            "name": "KMeansOptimizer",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class KMeansOptimizer:\n    \"\"\"\n    Provides utilities to assess convergence, evaluate the elbow method, and optimize the number of clusters \n    for k-means based clustering algorithms.\n    \n    This class offers methods to:\n    - Check convergence criteria during iterative updates.\n    - Compute the elbow metric to help in identifying the optimal number of clusters.\n    - Optimize the cluster count automatically based on provided metrics.\n    \n    Usage:\n        optimizer = KMeansOptimizer()\n        is_converged = optimizer.check_convergence(old_centroids, new_centroids, tol=0.001)\n        elbow_score = optimizer.compute_elbow_method(data, max_k=10)\n        optimal_k = optimizer.optimize_clusters(data, max_k=10)\n    \n    Args:\n        None\n    \n    Returns:\n        Convergence flags, elbow scores, or optimal cluster counts as integers or floats.\n    \n    Edge Cases:\n        Methods should effectively handle edge cases such as not reaching convergence within a fixed number of iterations.\n    \"\"\"\n\n    def check_convergence(self, old_centroids: np.ndarray, new_centroids: np.ndarray, tol: float=0.001) -> bool:\n        \"\"\"\n        Checks whether the centroids have converged within a specified tolerance.\n        \n        Args:\n            old_centroids (np.ndarray): Centroid positions from the previous iteration.\n            new_centroids (np.ndarray): Updated centroid positions.\n            tol (float): Tolerance threshold for convergence.\n            \n        Returns:\n            bool: True if the centroids have converged; False otherwise.\n        \"\"\"\n        pass\n\n    def compute_elbow_method(self, data: np.ndarray, max_k: int) -> Dict[int, float]:\n        \"\"\"\n        Computes the elbow metric for a range of clusters to aid in identifying the optimal number of clusters.\n        \n        Args:\n            data (np.ndarray): Input data for computing the metric.\n            max_k (int): Maximum number of clusters to consider.\n            \n        Returns:\n            Dict[int, float]: Mapping of number of clusters to computed elbow values.\n        \"\"\"\n        pass\n\n    def optimize_clusters(self, data: np.ndarray, max_k: int) -> int:\n        \"\"\"\n        Determines the optimal number of clusters based on optimization criteria.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            max_k (int): Maximum number of clusters to be evaluated.\n            \n        Returns:\n            int: The optimal number of clusters.\n        \"\"\"\n        pass",
                                                            "lineno": 45
                                                        },
                                                        {
                                                            "name": "check_convergence",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMeansOptimizer",
                                                            "extra": {},
                                                            "code": "def check_convergence(self, old_centroids: np.ndarray, new_centroids: np.ndarray, tol: float=0.001) -> bool:\n    \"\"\"\n        Checks whether the centroids have converged within a specified tolerance.\n        \n        Args:\n            old_centroids (np.ndarray): Centroid positions from the previous iteration.\n            new_centroids (np.ndarray): Updated centroid positions.\n            tol (float): Tolerance threshold for convergence.\n            \n        Returns:\n            bool: True if the centroids have converged; False otherwise.\n        \"\"\"\n    pass",
                                                            "lineno": 71
                                                        },
                                                        {
                                                            "name": "compute_elbow_method",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMeansOptimizer",
                                                            "extra": {},
                                                            "code": "def compute_elbow_method(self, data: np.ndarray, max_k: int) -> Dict[int, float]:\n    \"\"\"\n        Computes the elbow metric for a range of clusters to aid in identifying the optimal number of clusters.\n        \n        Args:\n            data (np.ndarray): Input data for computing the metric.\n            max_k (int): Maximum number of clusters to consider.\n            \n        Returns:\n            Dict[int, float]: Mapping of number of clusters to computed elbow values.\n        \"\"\"\n    pass",
                                                            "lineno": 85
                                                        },
                                                        {
                                                            "name": "optimize_clusters",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMeansOptimizer",
                                                            "extra": {},
                                                            "code": "def optimize_clusters(self, data: np.ndarray, max_k: int) -> int:\n    \"\"\"\n        Determines the optimal number of clusters based on optimization criteria.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            max_k (int): Maximum number of clusters to be evaluated.\n            \n        Returns:\n            int: The optimal number of clusters.\n        \"\"\"\n    pass",
                                                            "lineno": 98
                                                        },
                                                        {
                                                            "name": "KMeansClustering",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class KMeansClustering:\n    \"\"\"\n    Implements the standard K-means clustering algorithm.\n    \n    This class is responsible for performing the standard iterative clustering process, including:\n    - Assignment of data points to the nearest centroids.\n    - Updating centroid positions based on cluster membership.\n    \n    Usage:\n        kmeans = KMeansClustering(num_clusters=3, max_iter=300)\n        cluster_labels = kmeans.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters to form.\n        max_iter (int): Maximum iterations for the algorithm.\n    \n    Returns:\n        fit_predict returns cluster labels as a numpy.ndarray.\n    \n    Edge Cases:\n        The implementation should handle cases with empty clusters, convergence conditions, and data containing outliers.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the K-means clustering on the provided data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the cluster labels for the provided data.\n        \n        Args:\n            data (np.ndarray): Data for which to determine cluster membership.\n            \n        Returns:\n            np.ndarray: Array of predicted cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines fit and predict steps for convenience.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            np.ndarray: Cluster labels for the input data.\n        \"\"\"\n        pass",
                                                            "lineno": 111
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMeansClustering",
                                                            "extra": {},
                                                            "code": "def __init__(self, num_clusters: int=3, max_iter: int=300) -> None:\n    pass",
                                                            "lineno": 134
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMeansClustering",
                                                            "extra": {},
                                                            "code": "def fit(self, data: np.ndarray) -> None:\n    \"\"\"\n        Fits the K-means clustering on the provided data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n    pass",
                                                            "lineno": 137
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMeansClustering",
                                                            "extra": {},
                                                            "code": "def predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Predicts the cluster labels for the provided data.\n        \n        Args:\n            data (np.ndarray): Data for which to determine cluster membership.\n            \n        Returns:\n            np.ndarray: Array of predicted cluster labels.\n        \"\"\"\n    pass",
                                                            "lineno": 149
                                                        },
                                                        {
                                                            "name": "fit_predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMeansClustering",
                                                            "extra": {},
                                                            "code": "def fit_predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Combines fit and predict steps for convenience.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            np.ndarray: Cluster labels for the input data.\n        \"\"\"\n    pass",
                                                            "lineno": 161
                                                        },
                                                        {
                                                            "name": "MiniBatchKMeansClustering",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class MiniBatchKMeansClustering:\n    \"\"\"\n    Implements the Mini-Batch K-means clustering algorithm.\n    \n    This variant follows the standard k-means algorithm but uses mini-batches to reduce computation time.\n    \n    Usage:\n        mbkmeans = MiniBatchKMeansClustering(num_clusters=3, batch_size=100)\n        labels = mbkmeans.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters.\n        batch_size (int): Size of the mini-batch.\n        max_iter (int): Maximum number of iterations.\n    \n    Returns:\n        Cluster labels as a numpy.ndarray from fit_predict.\n    \n    Edge Cases:\n        The implementation should address cases where batch size exceeds dataset length and ensure consistent convergence.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, batch_size: int=100, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the Mini-Batch K-means model on provided data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Assigns data points to the nearest cluster centroid based on the fitted model.\n        \n        Args:\n            data (np.ndarray): Data for clustering prediction.\n        \n        Returns:\n            np.ndarray: Predicted cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines the fit and predict operations.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            \n        Returns:\n            np.ndarray: Cluster labels derived from the model.\n        \"\"\"\n        pass",
                                                            "lineno": 173
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "MiniBatchKMeansClustering",
                                                            "extra": {},
                                                            "code": "def __init__(self, num_clusters: int=3, batch_size: int=100, max_iter: int=300) -> None:\n    pass",
                                                            "lineno": 195
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "MiniBatchKMeansClustering",
                                                            "extra": {},
                                                            "code": "def fit(self, data: np.ndarray) -> None:\n    \"\"\"\n        Fits the Mini-Batch K-means model on provided data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n    pass",
                                                            "lineno": 198
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "MiniBatchKMeansClustering",
                                                            "extra": {},
                                                            "code": "def predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Assigns data points to the nearest cluster centroid based on the fitted model.\n        \n        Args:\n            data (np.ndarray): Data for clustering prediction.\n        \n        Returns:\n            np.ndarray: Predicted cluster labels.\n        \"\"\"\n    pass",
                                                            "lineno": 210
                                                        },
                                                        {
                                                            "name": "fit_predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "MiniBatchKMeansClustering",
                                                            "extra": {},
                                                            "code": "def fit_predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Combines the fit and predict operations.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            \n        Returns:\n            np.ndarray: Cluster labels derived from the model.\n        \"\"\"\n    pass",
                                                            "lineno": 222
                                                        },
                                                        {
                                                            "name": "SpectralClustering",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class SpectralClustering:\n    \"\"\"\n    Implements the spectral clustering method in the context of k-means clustering based workflows.\n    \n    This algorithm uses eigen decomposition on a similarity matrix to reduce dimensionality before clustering.\n    \n    Usage:\n        spectral = SpectralClustering(num_clusters=3, affinity=\"rbf\")\n        labels = spectral.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Desired number of clusters.\n        affinity (str): Affinity type to construct the similarity matrix.\n        n_neighbors (int, optional): Number of neighbors for graph construction.\n    \n    Returns:\n        Cluster labels as a numpy.ndarray.\n    \n    Edge Cases:\n        The method should handle situations where the similarity matrix fails to capture structure in the data.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, affinity: str='rbf', n_neighbors: int=10) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the spectral clustering model on the input data.\n        \n        Args:\n            data (np.ndarray): Data for clustering.\n            \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Assigns cluster labels based on the spectral clustering.\n        \n        Args:\n            data (np.ndarray): Data for which clustering is to be predicted.\n            \n        Returns:\n            np.ndarray: Predicted cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines fitting and predicting into a single step.\n        \n        Args:\n            data (np.ndarray): Data to be clustered.\n            \n        Returns:\n            np.ndarray: Cluster labels for the input data.\n        \"\"\"\n        pass",
                                                            "lineno": 234
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "SpectralClustering",
                                                            "extra": {},
                                                            "code": "def __init__(self, num_clusters: int=3, affinity: str='rbf', n_neighbors: int=10) -> None:\n    pass",
                                                            "lineno": 256
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "SpectralClustering",
                                                            "extra": {},
                                                            "code": "def fit(self, data: np.ndarray) -> None:\n    \"\"\"\n        Fits the spectral clustering model on the input data.\n        \n        Args:\n            data (np.ndarray): Data for clustering.\n            \n        Returns:\n            None\n        \"\"\"\n    pass",
                                                            "lineno": 259
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "SpectralClustering",
                                                            "extra": {},
                                                            "code": "def predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Assigns cluster labels based on the spectral clustering.\n        \n        Args:\n            data (np.ndarray): Data for which clustering is to be predicted.\n            \n        Returns:\n            np.ndarray: Predicted cluster labels.\n        \"\"\"\n    pass",
                                                            "lineno": 271
                                                        },
                                                        {
                                                            "name": "fit_predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "SpectralClustering",
                                                            "extra": {},
                                                            "code": "def fit_predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Combines fitting and predicting into a single step.\n        \n        Args:\n            data (np.ndarray): Data to be clustered.\n            \n        Returns:\n            np.ndarray: Cluster labels for the input data.\n        \"\"\"\n    pass",
                                                            "lineno": 283
                                                        },
                                                        {
                                                            "name": "KMedoidsClustering",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class KMedoidsClustering:\n    \"\"\"\n    Implements the k-medoids clustering algorithm, an alternative to k-means that selects actual data points as centers.\n    \n    Usage:\n        kmedoids = KMedoidsClustering(num_clusters=3, max_iter=300)\n        labels = kmedoids.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters.\n        max_iter (int): Maximum number of iterations.\n    \n    Returns:\n        np.ndarray: Predicted cluster labels.\n    \n    Edge Cases:\n        Should handle data with outliers robustly by selecting medoids that minimize dissimilarity.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the k-medoids model to the input data.\n        \n        Args:\n            data (np.ndarray): Data used for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts cluster membership for input data based on medoid distances.\n        \n        Args:\n            data (np.ndarray): Input data.\n            \n        Returns:\n            np.ndarray: Cluster labels.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Executes fitting and prediction in a single step.\n        \n        Args:\n            data (np.ndarray): Data for clustering.\n            \n        Returns:\n            np.ndarray: Cluster assignments.\n        \"\"\"\n        pass",
                                                            "lineno": 295
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMedoidsClustering",
                                                            "extra": {},
                                                            "code": "def __init__(self, num_clusters: int=3, max_iter: int=300) -> None:\n    pass",
                                                            "lineno": 314
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMedoidsClustering",
                                                            "extra": {},
                                                            "code": "def fit(self, data: np.ndarray) -> None:\n    \"\"\"\n        Fits the k-medoids model to the input data.\n        \n        Args:\n            data (np.ndarray): Data used for clustering.\n        \n        Returns:\n            None\n        \"\"\"\n    pass",
                                                            "lineno": 317
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMedoidsClustering",
                                                            "extra": {},
                                                            "code": "def predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Predicts cluster membership for input data based on medoid distances.\n        \n        Args:\n            data (np.ndarray): Input data.\n            \n        Returns:\n            np.ndarray: Cluster labels.\n        \"\"\"\n    pass",
                                                            "lineno": 329
                                                        },
                                                        {
                                                            "name": "fit_predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "KMedoidsClustering",
                                                            "extra": {},
                                                            "code": "def fit_predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Executes fitting and prediction in a single step.\n        \n        Args:\n            data (np.ndarray): Data for clustering.\n            \n        Returns:\n            np.ndarray: Cluster assignments.\n        \"\"\"\n    pass",
                                                            "lineno": 341
                                                        },
                                                        {
                                                            "name": "FuzzyKMeansClustering",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class FuzzyKMeansClustering:\n    \"\"\"\n    Implements the fuzzy k-means (soft clustering) algorithm where each data point can belong\n    to multiple clusters with varying degrees of membership.\n    \n    Usage:\n        fuzzy_kmeans = FuzzyKMeansClustering(num_clusters=3, m=2.0, max_iter=300)\n        memberships = fuzzy_kmeans.fit_predict(data)\n    \n    Args:\n        num_clusters (int): Number of clusters.\n        m (float): Fuzziness parameter that controls the degree of membership sharing.\n        max_iter (int): Maximum number of iterations for convergence.\n    \n    Returns:\n        np.ndarray: Membership matrix indicating the degree of belonging to each cluster.\n    \n    Edge Cases:\n        Should handle cases where the fuzziness parameter may lead to degenerate membership assignments.\n    \"\"\"\n\n    def __init__(self, num_clusters: int=3, m: float=2.0, max_iter: int=300) -> None:\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fits the fuzzy k-means model to the input data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predicts the fuzzy cluster memberships for the provided data.\n        \n        Args:\n            data (np.ndarray): Data for which to compute cluster memberships.\n            \n        Returns:\n            np.ndarray: A membership matrix where each row sums to 1.\n        \"\"\"\n        pass\n\n    def fit_predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Combines fitting and predicting of fuzzy cluster memberships.\n        \n        Args:\n            data (np.ndarray): Input data for fuzzy clustering.\n            \n        Returns:\n            np.ndarray: The membership matrix.\n        \"\"\"\n        pass",
                                                            "lineno": 353
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "FuzzyKMeansClustering",
                                                            "extra": {},
                                                            "code": "def __init__(self, num_clusters: int=3, m: float=2.0, max_iter: int=300) -> None:\n    pass",
                                                            "lineno": 374
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "FuzzyKMeansClustering",
                                                            "extra": {},
                                                            "code": "def fit(self, data: np.ndarray) -> None:\n    \"\"\"\n        Fits the fuzzy k-means model to the input data.\n        \n        Args:\n            data (np.ndarray): Input data for clustering.\n            \n        Returns:\n            None\n        \"\"\"\n    pass",
                                                            "lineno": 377
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "FuzzyKMeansClustering",
                                                            "extra": {},
                                                            "code": "def predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Predicts the fuzzy cluster memberships for the provided data.\n        \n        Args:\n            data (np.ndarray): Data for which to compute cluster memberships.\n            \n        Returns:\n            np.ndarray: A membership matrix where each row sums to 1.\n        \"\"\"\n    pass",
                                                            "lineno": 389
                                                        },
                                                        {
                                                            "name": "fit_predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/k_means.py",
                                                            "parent": "FuzzyKMeansClustering",
                                                            "extra": {},
                                                            "code": "def fit_predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Combines fitting and predicting of fuzzy cluster memberships.\n        \n        Args:\n            data (np.ndarray): Input data for fuzzy clustering.\n            \n        Returns:\n            np.ndarray: The membership matrix.\n        \"\"\"\n    pass",
                                                            "lineno": 401
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "hierarchical_density.py",
                                                    "path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                    "code": "from typing import Any\nimport numpy as np\n\nclass DensityBasedClustering:\n    \"\"\"\n    Interface for performing density-based clustering.\n\n    This class provides a framework to implement density-based clustering techniques,\n    such as DBSCAN or similar algorithms that group data based on the local density of points.\n    It encapsulates configuration parameters such as the neighborhood radius and the minimum number\n    of points required to form a cluster.\n\n    Args:\n        eps (float): The radius within which to search for neighboring points.\n        min_samples (int): The minimum number of points required to form a dense region.\n\n    Methods:\n        fit(data: np.ndarray) -> None:\n            Compute the clustering structure from the input data.\n        predict(data: np.ndarray) -> np.ndarray:\n            Assign cluster labels to new or existing data points based on the learned clustering structure.\n    \"\"\"\n\n    def __init__(self, eps: float=0.5, min_samples: int=5) -> None:\n        self.eps = eps\n        self.min_samples = min_samples\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fit the density-based clustering model on the provided data.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            The method should compute the clustering structure by analyzing point density.\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict cluster labels for the provided data with the density-based clustering model.\n\n        Args:\n            data (np.ndarray): New or training data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            np.ndarray: An array of cluster labels, where noise points may be labeled as -1.\n\n        Notes:\n            The output labels should be consistent with the clustering computed in the fit method.\n        \"\"\"\n        pass\n\nclass AgglomerativeClustering:\n    \"\"\"\n    Interface for performing agglomerative hierarchical clustering.\n\n    This class implements the agglomerative (bottom-up) clustering strategy by merging the\n    closest clusters iteratively based on a chosen linkage criterion. It supports extracting\n    the final cluster labels as well as generating a dendrogram representation of the hierarchical structure.\n\n    Args:\n        linkage (str): The linkage criterion to use (e.g., 'ward', 'complete', 'average').\n        distance_metric (str, optional): The metric to measure distance between data points. Defaults to 'euclidean'.\n\n    Methods:\n        fit(data: np.ndarray) -> None:\n            Perform the agglomerative clustering on the input data.\n        predict(data: np.ndarray) -> np.ndarray:\n            Return cluster labels for the input data based on the fitted model.\n        get_dendrogram() -> Any:\n            Retrieve a dendrogram structure representing the hierarchical cluster merge history.\n    \"\"\"\n\n    def __init__(self, linkage: str='ward', distance_metric: str='euclidean') -> None:\n        self.linkage = linkage\n        self.distance_metric = distance_metric\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fit the agglomerative clustering model on the provided dataset.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            This method should execute the bottom-up merging process based on the specified linkage.\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict cluster labels for the input data using the agglomerative clustering model.\n\n        Args:\n            data (np.ndarray): Data as a NumPy array for which cluster labels are required.\n\n        Returns:\n            np.ndarray: An array of cluster labels corresponding to each sample.\n        \"\"\"\n        pass\n\n    def get_dendrogram(self) -> Any:\n        \"\"\"\n        Retrieve the dendrogram structure representing the hierarchical clustering tree.\n\n        Returns:\n            Any: An object representing the dendrogram, which may be used for visualization or further analysis.\n        \"\"\"\n        pass\n\nclass DivisiveClustering:\n    \"\"\"\n    Interface for performing divisive hierarchical clustering.\n\n    This class implements the divisive (top-down) clustering strategy by recursively splitting\n    the dataset into clusters based on specified criteria. It provides methods to obtain cluster\n    labels and to generate a dendrogram that represents the recursive division of the data.\n\n    Args:\n        criterion (Any, optional): The criterion for splitting clusters (e.g., maximization of between-cluster variance).\n        distance_metric (str, optional): The metric used for evaluating splits. Defaults to 'euclidean'.\n\n    Methods:\n        fit(data: np.ndarray) -> None:\n            Perform the divisive clustering on the input data.\n        predict(data: np.ndarray) -> np.ndarray:\n            Return cluster labels for the input data based on the fitted model.\n        get_dendrogram() -> Any:\n            Retrieve a dendrogram that illustrates the hierarchical splits made by the model.\n    \"\"\"\n\n    def __init__(self, criterion: Any=None, distance_metric: str='euclidean') -> None:\n        self.criterion = criterion\n        self.distance_metric = distance_metric\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fit the divisive clustering model on the provided dataset by recursively splitting the data.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            This method should implement a top-down clustering approach, recursively dividing the data.\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict cluster labels for the given data using the divisive clustering model.\n\n        Args:\n            data (np.ndarray): Data as a NumPy array for which cluster labels are to be computed.\n\n        Returns:\n            np.ndarray: An array of cluster labels reflecting the outcome of the divisive clustering.\n        \"\"\"\n        pass\n\n    def get_dendrogram(self) -> Any:\n        \"\"\"\n        Retrieve the hierarchical dendrogram representing the splits generated by the divisive clustering process.\n\n        Returns:\n            Any: A dendrogram object that can be used to analyze or visualize the hierarchical structure.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import numpy as np",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import numpy as np",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "DensityBasedClustering",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class DensityBasedClustering:\n    \"\"\"\n    Interface for performing density-based clustering.\n\n    This class provides a framework to implement density-based clustering techniques,\n    such as DBSCAN or similar algorithms that group data based on the local density of points.\n    It encapsulates configuration parameters such as the neighborhood radius and the minimum number\n    of points required to form a cluster.\n\n    Args:\n        eps (float): The radius within which to search for neighboring points.\n        min_samples (int): The minimum number of points required to form a dense region.\n\n    Methods:\n        fit(data: np.ndarray) -> None:\n            Compute the clustering structure from the input data.\n        predict(data: np.ndarray) -> np.ndarray:\n            Assign cluster labels to new or existing data points based on the learned clustering structure.\n    \"\"\"\n\n    def __init__(self, eps: float=0.5, min_samples: int=5) -> None:\n        self.eps = eps\n        self.min_samples = min_samples\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fit the density-based clustering model on the provided data.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            The method should compute the clustering structure by analyzing point density.\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict cluster labels for the provided data with the density-based clustering model.\n\n        Args:\n            data (np.ndarray): New or training data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            np.ndarray: An array of cluster labels, where noise points may be labeled as -1.\n\n        Notes:\n            The output labels should be consistent with the clustering computed in the fit method.\n        \"\"\"\n        pass",
                                                            "lineno": 4
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": "DensityBasedClustering",
                                                            "extra": {},
                                                            "code": "def __init__(self, eps: float=0.5, min_samples: int=5) -> None:\n    self.eps = eps\n    self.min_samples = min_samples\n    pass",
                                                            "lineno": 24
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": "DensityBasedClustering",
                                                            "extra": {},
                                                            "code": "def fit(self, data: np.ndarray) -> None:\n    \"\"\"\n        Fit the density-based clustering model on the provided data.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            The method should compute the clustering structure by analyzing point density.\n        \"\"\"\n    pass",
                                                            "lineno": 29
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": "DensityBasedClustering",
                                                            "extra": {},
                                                            "code": "def predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Predict cluster labels for the provided data with the density-based clustering model.\n\n        Args:\n            data (np.ndarray): New or training data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            np.ndarray: An array of cluster labels, where noise points may be labeled as -1.\n\n        Notes:\n            The output labels should be consistent with the clustering computed in the fit method.\n        \"\"\"\n    pass",
                                                            "lineno": 44
                                                        },
                                                        {
                                                            "name": "AgglomerativeClustering",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class AgglomerativeClustering:\n    \"\"\"\n    Interface for performing agglomerative hierarchical clustering.\n\n    This class implements the agglomerative (bottom-up) clustering strategy by merging the\n    closest clusters iteratively based on a chosen linkage criterion. It supports extracting\n    the final cluster labels as well as generating a dendrogram representation of the hierarchical structure.\n\n    Args:\n        linkage (str): The linkage criterion to use (e.g., 'ward', 'complete', 'average').\n        distance_metric (str, optional): The metric to measure distance between data points. Defaults to 'euclidean'.\n\n    Methods:\n        fit(data: np.ndarray) -> None:\n            Perform the agglomerative clustering on the input data.\n        predict(data: np.ndarray) -> np.ndarray:\n            Return cluster labels for the input data based on the fitted model.\n        get_dendrogram() -> Any:\n            Retrieve a dendrogram structure representing the hierarchical cluster merge history.\n    \"\"\"\n\n    def __init__(self, linkage: str='ward', distance_metric: str='euclidean') -> None:\n        self.linkage = linkage\n        self.distance_metric = distance_metric\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fit the agglomerative clustering model on the provided dataset.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            This method should execute the bottom-up merging process based on the specified linkage.\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict cluster labels for the input data using the agglomerative clustering model.\n\n        Args:\n            data (np.ndarray): Data as a NumPy array for which cluster labels are required.\n\n        Returns:\n            np.ndarray: An array of cluster labels corresponding to each sample.\n        \"\"\"\n        pass\n\n    def get_dendrogram(self) -> Any:\n        \"\"\"\n        Retrieve the dendrogram structure representing the hierarchical clustering tree.\n\n        Returns:\n            Any: An object representing the dendrogram, which may be used for visualization or further analysis.\n        \"\"\"\n        pass",
                                                            "lineno": 59
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": "AgglomerativeClustering",
                                                            "extra": {},
                                                            "code": "def __init__(self, linkage: str='ward', distance_metric: str='euclidean') -> None:\n    self.linkage = linkage\n    self.distance_metric = distance_metric\n    pass",
                                                            "lineno": 80
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": "AgglomerativeClustering",
                                                            "extra": {},
                                                            "code": "def fit(self, data: np.ndarray) -> None:\n    \"\"\"\n        Fit the agglomerative clustering model on the provided dataset.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            This method should execute the bottom-up merging process based on the specified linkage.\n        \"\"\"\n    pass",
                                                            "lineno": 85
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": "AgglomerativeClustering",
                                                            "extra": {},
                                                            "code": "def predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Predict cluster labels for the input data using the agglomerative clustering model.\n\n        Args:\n            data (np.ndarray): Data as a NumPy array for which cluster labels are required.\n\n        Returns:\n            np.ndarray: An array of cluster labels corresponding to each sample.\n        \"\"\"\n    pass",
                                                            "lineno": 100
                                                        },
                                                        {
                                                            "name": "get_dendrogram",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": "AgglomerativeClustering",
                                                            "extra": {},
                                                            "code": "def get_dendrogram(self) -> Any:\n    \"\"\"\n        Retrieve the dendrogram structure representing the hierarchical clustering tree.\n\n        Returns:\n            Any: An object representing the dendrogram, which may be used for visualization or further analysis.\n        \"\"\"\n    pass",
                                                            "lineno": 112
                                                        },
                                                        {
                                                            "name": "DivisiveClustering",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class DivisiveClustering:\n    \"\"\"\n    Interface for performing divisive hierarchical clustering.\n\n    This class implements the divisive (top-down) clustering strategy by recursively splitting\n    the dataset into clusters based on specified criteria. It provides methods to obtain cluster\n    labels and to generate a dendrogram that represents the recursive division of the data.\n\n    Args:\n        criterion (Any, optional): The criterion for splitting clusters (e.g., maximization of between-cluster variance).\n        distance_metric (str, optional): The metric used for evaluating splits. Defaults to 'euclidean'.\n\n    Methods:\n        fit(data: np.ndarray) -> None:\n            Perform the divisive clustering on the input data.\n        predict(data: np.ndarray) -> np.ndarray:\n            Return cluster labels for the input data based on the fitted model.\n        get_dendrogram() -> Any:\n            Retrieve a dendrogram that illustrates the hierarchical splits made by the model.\n    \"\"\"\n\n    def __init__(self, criterion: Any=None, distance_metric: str='euclidean') -> None:\n        self.criterion = criterion\n        self.distance_metric = distance_metric\n        pass\n\n    def fit(self, data: np.ndarray) -> None:\n        \"\"\"\n        Fit the divisive clustering model on the provided dataset by recursively splitting the data.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            This method should implement a top-down clustering approach, recursively dividing the data.\n        \"\"\"\n        pass\n\n    def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict cluster labels for the given data using the divisive clustering model.\n\n        Args:\n            data (np.ndarray): Data as a NumPy array for which cluster labels are to be computed.\n\n        Returns:\n            np.ndarray: An array of cluster labels reflecting the outcome of the divisive clustering.\n        \"\"\"\n        pass\n\n    def get_dendrogram(self) -> Any:\n        \"\"\"\n        Retrieve the hierarchical dendrogram representing the splits generated by the divisive clustering process.\n\n        Returns:\n            Any: A dendrogram object that can be used to analyze or visualize the hierarchical structure.\n        \"\"\"\n        pass",
                                                            "lineno": 121
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": "DivisiveClustering",
                                                            "extra": {},
                                                            "code": "def __init__(self, criterion: Any=None, distance_metric: str='euclidean') -> None:\n    self.criterion = criterion\n    self.distance_metric = distance_metric\n    pass",
                                                            "lineno": 142
                                                        },
                                                        {
                                                            "name": "fit",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": "DivisiveClustering",
                                                            "extra": {},
                                                            "code": "def fit(self, data: np.ndarray) -> None:\n    \"\"\"\n        Fit the divisive clustering model on the provided dataset by recursively splitting the data.\n\n        Args:\n            data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\n        Returns:\n            None\n\n        Notes:\n            This method should implement a top-down clustering approach, recursively dividing the data.\n        \"\"\"\n    pass",
                                                            "lineno": 147
                                                        },
                                                        {
                                                            "name": "predict",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": "DivisiveClustering",
                                                            "extra": {},
                                                            "code": "def predict(self, data: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Predict cluster labels for the given data using the divisive clustering model.\n\n        Args:\n            data (np.ndarray): Data as a NumPy array for which cluster labels are to be computed.\n\n        Returns:\n            np.ndarray: An array of cluster labels reflecting the outcome of the divisive clustering.\n        \"\"\"\n    pass",
                                                            "lineno": 162
                                                        },
                                                        {
                                                            "name": "get_dendrogram",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/unsupervised/clustering/hierarchical_density.py",
                                                            "parent": "DivisiveClustering",
                                                            "extra": {},
                                                            "code": "def get_dendrogram(self) -> Any:\n    \"\"\"\n        Retrieve the hierarchical dendrogram representing the splits generated by the divisive clustering process.\n\n        Returns:\n            Any: A dendrogram object that can be used to analyze or visualize the hierarchical structure.\n        \"\"\"\n    pass",
                                                            "lineno": 174
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "anomaly_detection.py",
                                            "path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                            "code": "import pandas as pd\nimport numpy as np\n\nclass OneClassSVMAnomalyDetector:\n    \"\"\"\n    Anomaly detector using the One-Class SVM algorithm.\n    \n    This class implements the one-class SVM approach for detecting anomalies in an unsupervised\n    learning setting. It is designed to learn the boundary of normal data points during training\n    and subsequently predict whether a new observation is an anomaly.\n    \n    Methods:\n        __init__(kernel: str = 'rbf', nu: float = 0.5, gamma: Optional[float] = None)\n            Initializes the anomaly detector with parameters for the SVM kernel, the nu parameter,\n            and the gamma value.\n\n        fit(X: pd.DataFrame) -> None:\n            Train the anomaly detector on the provided dataset.\n\n        predict(X: pd.DataFrame) -> np.ndarray:\n            Predict if the samples in the dataset are anomalies. Returns an array where typically\n            -1 indicates an anomaly and 1 indicates a normal data point.\n    \n    Args:\n        kernel (str): Specifies the kernel type to be used in the algorithm (default is 'rbf').\n        nu (float): An upper bound on the fraction of training errors and a lower bound of the fraction\n                    of support vectors (default is 0.5).\n        gamma (Optional[float]): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. If None, it is set\n                                 to 'scale' by convention.\n    \"\"\"\n\n    def __init__(self, kernel: str='rbf', nu: float=0.5, gamma: Optional[float]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Fit the One-Class SVM model on the training data.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict anomalies in the provided data.\n        \n        Args:\n            X (pd.DataFrame): Data to be evaluated for anomalies.\n        \n        Returns:\n            np.ndarray: An array with predicted labels (e.g., 1 for normal, -1 for anomaly).\n        \"\"\"\n        pass\n\nclass IsolationForestAnomalyDetector:\n    \"\"\"\n    Anomaly detector using the Isolation Forest algorithm.\n    \n    This class implements the Isolation Forest approach for unsupervised anomaly detection.\n    The algorithm isolates anomalies instead of profiling normal data points, making it effective\n    for handling high-dimensional datasets.\n    \n    Methods:\n        __init__(n_estimators: int = 100, contamination: float = 0.1, max_samples: Optional[int] = None)\n            Initializes the Isolation Forest with the specified number of trees (estimators), expected\n            contamination (proportion of anomalies), and the number of samples to draw from X to train each tree.\n\n        fit(X: pd.DataFrame) -> None:\n            Build the isolation forest on the training data.\n\n        predict(X: pd.DataFrame) -> np.ndarray:\n            Predict whether the samples are anomalies, returning an array where anomalies are usually\n            marked with -1.\n    \n    Args:\n        n_estimators (int): The number of base estimators in the ensemble (default is 100).\n        contamination (float): The expected proportion of outliers in the data (default is 0.1).\n        max_samples (Optional[int]): The number of samples to draw to train each base estimator. If None,\n                                     the maximum available samples are used.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, contamination: float=0.1, max_samples: Optional[int]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Fit the Isolation Forest model on the training data.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict anomalies using the Isolation Forest model.\n        \n        Args:\n            X (pd.DataFrame): Data for which anomaly detection is to be performed.\n        \n        Returns:\n            np.ndarray: An array with predictions where typically -1 indicates an anomaly and 1 indicates normal.\n        \"\"\"\n        pass\n\nclass LocalOutlierFactorAnomalyDetector:\n    \"\"\"\n    Anomaly detector using the Local Outlier Factor (LOF) algorithm.\n    \n    This class implements the Local Outlier Factor method for detecting anomalies by measuring the\n    local deviation of density of a given data point with respect to its neighbors. A lower density\n    compared to its neighbors indicates a potential outlier.\n    \n    Methods:\n        __init__(n_neighbors: int = 20, contamination: float = 0.1)\n            Initializes the LOF detector with the number of neighbors to use and the expected\n            proportion of anomalies.\n\n        fit(X: pd.DataFrame) -> None:\n            Compute the LOF scores from the training data.\n\n        predict(X: pd.DataFrame) -> np.ndarray:\n            Predict whether data instances are anomalies based on their LOF scores, with an output\n            array where -1 indicates an anomaly.\n    \n    Args:\n        n_neighbors (int): The number of neighbors to use for computing the local density (default is 20).\n        contamination (float): The expected proportion of anomalies in the data (default is 0.1).\n    \"\"\"\n\n    def __init__(self, n_neighbors: int=20, contamination: float=0.1) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Fit the Local Outlier Factor model by computing neighbor densities.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict anomalies using the LOF model.\n        \n        Args:\n            X (pd.DataFrame): Data for which anomaly detection is performed.\n        \n        Returns:\n            np.ndarray: An array with predictions where -1 indicates an anomaly and 1 indicates a normal point.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "import numpy as np",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import numpy as np",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "OneClassSVMAnomalyDetector",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class OneClassSVMAnomalyDetector:\n    \"\"\"\n    Anomaly detector using the One-Class SVM algorithm.\n    \n    This class implements the one-class SVM approach for detecting anomalies in an unsupervised\n    learning setting. It is designed to learn the boundary of normal data points during training\n    and subsequently predict whether a new observation is an anomaly.\n    \n    Methods:\n        __init__(kernel: str = 'rbf', nu: float = 0.5, gamma: Optional[float] = None)\n            Initializes the anomaly detector with parameters for the SVM kernel, the nu parameter,\n            and the gamma value.\n\n        fit(X: pd.DataFrame) -> None:\n            Train the anomaly detector on the provided dataset.\n\n        predict(X: pd.DataFrame) -> np.ndarray:\n            Predict if the samples in the dataset are anomalies. Returns an array where typically\n            -1 indicates an anomaly and 1 indicates a normal data point.\n    \n    Args:\n        kernel (str): Specifies the kernel type to be used in the algorithm (default is 'rbf').\n        nu (float): An upper bound on the fraction of training errors and a lower bound of the fraction\n                    of support vectors (default is 0.5).\n        gamma (Optional[float]): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. If None, it is set\n                                 to 'scale' by convention.\n    \"\"\"\n\n    def __init__(self, kernel: str='rbf', nu: float=0.5, gamma: Optional[float]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Fit the One-Class SVM model on the training data.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict anomalies in the provided data.\n        \n        Args:\n            X (pd.DataFrame): Data to be evaluated for anomalies.\n        \n        Returns:\n            np.ndarray: An array with predicted labels (e.g., 1 for normal, -1 for anomaly).\n        \"\"\"\n        pass",
                                                    "lineno": 4
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": "OneClassSVMAnomalyDetector",
                                                    "extra": {},
                                                    "code": "def __init__(self, kernel: str='rbf', nu: float=0.5, gamma: Optional[float]=None) -> None:\n    pass",
                                                    "lineno": 32
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": "OneClassSVMAnomalyDetector",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame) -> None:\n    \"\"\"\n        Fit the One-Class SVM model on the training data.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n    pass",
                                                    "lineno": 35
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": "OneClassSVMAnomalyDetector",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n        Predict anomalies in the provided data.\n        \n        Args:\n            X (pd.DataFrame): Data to be evaluated for anomalies.\n        \n        Returns:\n            np.ndarray: An array with predicted labels (e.g., 1 for normal, -1 for anomaly).\n        \"\"\"\n    pass",
                                                    "lineno": 47
                                                },
                                                {
                                                    "name": "IsolationForestAnomalyDetector",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class IsolationForestAnomalyDetector:\n    \"\"\"\n    Anomaly detector using the Isolation Forest algorithm.\n    \n    This class implements the Isolation Forest approach for unsupervised anomaly detection.\n    The algorithm isolates anomalies instead of profiling normal data points, making it effective\n    for handling high-dimensional datasets.\n    \n    Methods:\n        __init__(n_estimators: int = 100, contamination: float = 0.1, max_samples: Optional[int] = None)\n            Initializes the Isolation Forest with the specified number of trees (estimators), expected\n            contamination (proportion of anomalies), and the number of samples to draw from X to train each tree.\n\n        fit(X: pd.DataFrame) -> None:\n            Build the isolation forest on the training data.\n\n        predict(X: pd.DataFrame) -> np.ndarray:\n            Predict whether the samples are anomalies, returning an array where anomalies are usually\n            marked with -1.\n    \n    Args:\n        n_estimators (int): The number of base estimators in the ensemble (default is 100).\n        contamination (float): The expected proportion of outliers in the data (default is 0.1).\n        max_samples (Optional[int]): The number of samples to draw to train each base estimator. If None,\n                                     the maximum available samples are used.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, contamination: float=0.1, max_samples: Optional[int]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Fit the Isolation Forest model on the training data.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict anomalies using the Isolation Forest model.\n        \n        Args:\n            X (pd.DataFrame): Data for which anomaly detection is to be performed.\n        \n        Returns:\n            np.ndarray: An array with predictions where typically -1 indicates an anomaly and 1 indicates normal.\n        \"\"\"\n        pass",
                                                    "lineno": 59
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": "IsolationForestAnomalyDetector",
                                                    "extra": {},
                                                    "code": "def __init__(self, n_estimators: int=100, contamination: float=0.1, max_samples: Optional[int]=None) -> None:\n    pass",
                                                    "lineno": 86
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": "IsolationForestAnomalyDetector",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame) -> None:\n    \"\"\"\n        Fit the Isolation Forest model on the training data.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n    pass",
                                                    "lineno": 89
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": "IsolationForestAnomalyDetector",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n        Predict anomalies using the Isolation Forest model.\n        \n        Args:\n            X (pd.DataFrame): Data for which anomaly detection is to be performed.\n        \n        Returns:\n            np.ndarray: An array with predictions where typically -1 indicates an anomaly and 1 indicates normal.\n        \"\"\"\n    pass",
                                                    "lineno": 101
                                                },
                                                {
                                                    "name": "LocalOutlierFactorAnomalyDetector",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class LocalOutlierFactorAnomalyDetector:\n    \"\"\"\n    Anomaly detector using the Local Outlier Factor (LOF) algorithm.\n    \n    This class implements the Local Outlier Factor method for detecting anomalies by measuring the\n    local deviation of density of a given data point with respect to its neighbors. A lower density\n    compared to its neighbors indicates a potential outlier.\n    \n    Methods:\n        __init__(n_neighbors: int = 20, contamination: float = 0.1)\n            Initializes the LOF detector with the number of neighbors to use and the expected\n            proportion of anomalies.\n\n        fit(X: pd.DataFrame) -> None:\n            Compute the LOF scores from the training data.\n\n        predict(X: pd.DataFrame) -> np.ndarray:\n            Predict whether data instances are anomalies based on their LOF scores, with an output\n            array where -1 indicates an anomaly.\n    \n    Args:\n        n_neighbors (int): The number of neighbors to use for computing the local density (default is 20).\n        contamination (float): The expected proportion of anomalies in the data (default is 0.1).\n    \"\"\"\n\n    def __init__(self, n_neighbors: int=20, contamination: float=0.1) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame) -> None:\n        \"\"\"\n        Fit the Local Outlier Factor model by computing neighbor densities.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict anomalies using the LOF model.\n        \n        Args:\n            X (pd.DataFrame): Data for which anomaly detection is performed.\n        \n        Returns:\n            np.ndarray: An array with predictions where -1 indicates an anomaly and 1 indicates a normal point.\n        \"\"\"\n        pass",
                                                    "lineno": 113
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": "LocalOutlierFactorAnomalyDetector",
                                                    "extra": {},
                                                    "code": "def __init__(self, n_neighbors: int=20, contamination: float=0.1) -> None:\n    pass",
                                                    "lineno": 138
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": "LocalOutlierFactorAnomalyDetector",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame) -> None:\n    \"\"\"\n        Fit the Local Outlier Factor model by computing neighbor densities.\n        \n        Args:\n            X (pd.DataFrame): The input training data.\n        \n        Returns:\n            None\n        \"\"\"\n    pass",
                                                    "lineno": 141
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/anomaly_detection.py",
                                                    "parent": "LocalOutlierFactorAnomalyDetector",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n        Predict anomalies using the LOF model.\n        \n        Args:\n            X (pd.DataFrame): Data for which anomaly detection is performed.\n        \n        Returns:\n            np.ndarray: An array with predictions where -1 indicates an anomaly and 1 indicates a normal point.\n        \"\"\"\n    pass",
                                                    "lineno": 153
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "self_organizing_maps.py",
                                            "path": "./src/algorithms/unsupervised/self_organizing_maps.py",
                                            "code": "from typing import Optional\nimport numpy as np\n\nclass SelfOrganizingMap:\n    \"\"\"\n    Represents a Self-Organizing Map (SOM) with adjustable learning rate and topology preservation mechanisms.\n\n    This class encapsulates the functionality required to train a Self-Organizing Map. It allows the\n    specification of an initial learning rate, which controls the magnitude of weight updates during training, \n    and includes functionality to maintain and assess topology preservation, ensuring that the spatial relationships \n    in the input data are maintained in the resulting map.\n\n    Attributes:\n        learning_rate (float): The initial learning rate to be used during training.\n        topology_preservation (bool): A flag indicating whether topology preservation mechanisms should be applied.\n        map_dimensions (tuple): The dimensions of the SOM grid (e.g., number of rows and columns).\n    \"\"\"\n\n    def __init__(self, learning_rate: float, topology_preservation: bool, map_dimensions: Optional[tuple]=(10, 10)) -> None:\n        \"\"\"\n        Initialize the Self-Organizing Map with specified learning rate and topology preservation settings.\n\n        Args:\n            learning_rate (float): The initial learning rate for the training process. Must be a positive float.\n            topology_preservation (bool): Determines if topology preservation is enforced during training.\n            map_dimensions (tuple, optional): Dimensions of the SOM grid as (rows, columns). Defaults to (10, 10).\n\n        Raises:\n            ValueError: If learning_rate is not positive or if map_dimensions is not a valid tuple.\n        \"\"\"\n        pass\n\n    def fit(self, data: np.ndarray, num_iterations: int) -> None:\n        \"\"\"\n        Train the Self-Organizing Map on the provided dataset.\n\n        The fit method updates the internal map (i.e., weights) based on the input data over a number of iterations.\n        The learning rate may be adjusted during the training, and topology preservation is maintained through\n        appropriate update rules.\n\n        Args:\n            data (np.ndarray): A 2D numpy array representing the training input features.\n            num_iterations (int): The total number of iterations for training the SOM.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the input data is not in the expected format or if num_iterations is not a positive integer.\n        \"\"\"\n        pass\n\n    def get_topology_preservation_metric(self) -> float:\n        \"\"\"\n        Compute and return a metric measuring the quality of topology preservation in the SOM.\n\n        This method evaluates how well the SOM maintains the relative spatial relationships of the input data.\n        A higher value typically indicates better preservation of the topological structure.\n\n        Returns:\n            float: A numerical value representing the quality of topology preservation.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "from typing import Optional",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/unsupervised/self_organizing_maps.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Optional",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "import numpy as np",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/unsupervised/self_organizing_maps.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import numpy as np",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "SelfOrganizingMap",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/unsupervised/self_organizing_maps.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class SelfOrganizingMap:\n    \"\"\"\n    Represents a Self-Organizing Map (SOM) with adjustable learning rate and topology preservation mechanisms.\n\n    This class encapsulates the functionality required to train a Self-Organizing Map. It allows the\n    specification of an initial learning rate, which controls the magnitude of weight updates during training, \n    and includes functionality to maintain and assess topology preservation, ensuring that the spatial relationships \n    in the input data are maintained in the resulting map.\n\n    Attributes:\n        learning_rate (float): The initial learning rate to be used during training.\n        topology_preservation (bool): A flag indicating whether topology preservation mechanisms should be applied.\n        map_dimensions (tuple): The dimensions of the SOM grid (e.g., number of rows and columns).\n    \"\"\"\n\n    def __init__(self, learning_rate: float, topology_preservation: bool, map_dimensions: Optional[tuple]=(10, 10)) -> None:\n        \"\"\"\n        Initialize the Self-Organizing Map with specified learning rate and topology preservation settings.\n\n        Args:\n            learning_rate (float): The initial learning rate for the training process. Must be a positive float.\n            topology_preservation (bool): Determines if topology preservation is enforced during training.\n            map_dimensions (tuple, optional): Dimensions of the SOM grid as (rows, columns). Defaults to (10, 10).\n\n        Raises:\n            ValueError: If learning_rate is not positive or if map_dimensions is not a valid tuple.\n        \"\"\"\n        pass\n\n    def fit(self, data: np.ndarray, num_iterations: int) -> None:\n        \"\"\"\n        Train the Self-Organizing Map on the provided dataset.\n\n        The fit method updates the internal map (i.e., weights) based on the input data over a number of iterations.\n        The learning rate may be adjusted during the training, and topology preservation is maintained through\n        appropriate update rules.\n\n        Args:\n            data (np.ndarray): A 2D numpy array representing the training input features.\n            num_iterations (int): The total number of iterations for training the SOM.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the input data is not in the expected format or if num_iterations is not a positive integer.\n        \"\"\"\n        pass\n\n    def get_topology_preservation_metric(self) -> float:\n        \"\"\"\n        Compute and return a metric measuring the quality of topology preservation in the SOM.\n\n        This method evaluates how well the SOM maintains the relative spatial relationships of the input data.\n        A higher value typically indicates better preservation of the topological structure.\n\n        Returns:\n            float: A numerical value representing the quality of topology preservation.\n        \"\"\"\n        pass",
                                                    "lineno": 4
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/self_organizing_maps.py",
                                                    "parent": "SelfOrganizingMap",
                                                    "extra": {},
                                                    "code": "def __init__(self, learning_rate: float, topology_preservation: bool, map_dimensions: Optional[tuple]=(10, 10)) -> None:\n    \"\"\"\n        Initialize the Self-Organizing Map with specified learning rate and topology preservation settings.\n\n        Args:\n            learning_rate (float): The initial learning rate for the training process. Must be a positive float.\n            topology_preservation (bool): Determines if topology preservation is enforced during training.\n            map_dimensions (tuple, optional): Dimensions of the SOM grid as (rows, columns). Defaults to (10, 10).\n\n        Raises:\n            ValueError: If learning_rate is not positive or if map_dimensions is not a valid tuple.\n        \"\"\"\n    pass",
                                                    "lineno": 19
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/self_organizing_maps.py",
                                                    "parent": "SelfOrganizingMap",
                                                    "extra": {},
                                                    "code": "def fit(self, data: np.ndarray, num_iterations: int) -> None:\n    \"\"\"\n        Train the Self-Organizing Map on the provided dataset.\n\n        The fit method updates the internal map (i.e., weights) based on the input data over a number of iterations.\n        The learning rate may be adjusted during the training, and topology preservation is maintained through\n        appropriate update rules.\n\n        Args:\n            data (np.ndarray): A 2D numpy array representing the training input features.\n            num_iterations (int): The total number of iterations for training the SOM.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the input data is not in the expected format or if num_iterations is not a positive integer.\n        \"\"\"\n    pass",
                                                    "lineno": 33
                                                },
                                                {
                                                    "name": "get_topology_preservation_metric",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/unsupervised/self_organizing_maps.py",
                                                    "parent": "SelfOrganizingMap",
                                                    "extra": {},
                                                    "code": "def get_topology_preservation_metric(self) -> float:\n    \"\"\"\n        Compute and return a metric measuring the quality of topology preservation in the SOM.\n\n        This method evaluates how well the SOM maintains the relative spatial relationships of the input data.\n        A higher value typically indicates better preservation of the topological structure.\n\n        Returns:\n            float: A numerical value representing the quality of topology preservation.\n        \"\"\"\n    pass",
                                                    "lineno": 53
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "optimization",
                                    "path": "./src/algorithms/optimization",
                                    "children": [
                                        {
                                            "type": "directory",
                                            "name": "gradient_descent",
                                            "path": "./src/algorithms/optimization/gradient_descent",
                                            "children": [
                                                {
                                                    "type": "file",
                                                    "name": "adaptive_batch.py",
                                                    "path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                    "code": "from typing import Any\nimport numpy as np\nfrom typing import Any, Callable\n\nclass AdagradOptimizer:\n    \"\"\"\n    Optimizer implementing the Adagrad algorithm which adapts learning rate for each parameter.\n    \n    This optimizer adjusts the learning rate for each parameter based on the history of gradients.\n    It is typically used when dealing with sparse data or scenarios where different parameters\n    require different updates.\n\n    Args:\n        learning_rate (float): The initial step size for gradient updates.\n        epsilon (float): A small constant to prevent division by zero, typically 1e-8.\n\n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Updates the parameters using the Adagrad algorithm.\n            \n            Args:\n                params (np.ndarray): Current parameters to be updated.\n                grads (np.ndarray): Gradients computed from the loss function.\n            \n            Returns:\n                np.ndarray: Updated parameters.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, epsilon: float=1e-08) -> None:\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon\n        self.grad_squared_accum = None\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass\n\nclass NesterovAcceleratedGradient:\n    \"\"\"\n    Optimizer implementing Nesterov Accelerated Gradient (NAG) method.\n\n    This optimizer improves upon the standard momentum method by computing the gradient \n    at the approximate future position of the parameters, leading to faster convergence.\n\n    Args:\n        learning_rate (float): The step size for updating parameters.\n        momentum (float): The momentum factor (typically between 0 and 1).\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Perform a parameter update using the Nesterov accelerated gradient method.\n            \n            Args:\n                params (np.ndarray): The current model parameters.\n                grads (np.ndarray): The computed gradients based on the lookahead position.\n            \n            Returns:\n                np.ndarray: Updated parameters after applying the NAG update.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, momentum: float) -> None:\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.velocity = None\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass\n\nclass RMSPropOptimizer:\n    \"\"\"\n    Optimizer implementing the RMSProp algorithm.\n\n    RMSProp adapts the learning rate for each parameter by dividing the learning rate\n    by a running average of the magnitudes of recent gradients, which helps with faster and \n    more stable convergence.\n\n    Args:\n        learning_rate (float): Initial learning rate for the optimizer.\n        decay_rate (float): Decay rate for the moving average of squared gradients.\n        epsilon (float): A small constant to prevent division by zero, typically set to 1e-8.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Updates parameters using RMSProp optimization.\n            \n            Args:\n                params (np.ndarray): Array of current parameters.\n                grads (np.ndarray): Array of gradients computed from the loss.\n            \n            Returns:\n                np.ndarray: Updated parameters after applying the RMSProp step.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, decay_rate: float, epsilon: float=1e-08) -> None:\n        self.learning_rate = learning_rate\n        self.decay_rate = decay_rate\n        self.epsilon = epsilon\n        self.squared_avg = None\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass\n\nclass GDWithLineSearch:\n    \"\"\"\n    Optimizer that uses gradient descent augmented with line search.\n\n    This method performs standard gradient descent updates while dynamically determining\n    an optimal learning rate through a line search strategy. The line search procedure iteratively\n    selects a step size that sufficiently decreases the loss function.\n\n    Args:\n        initial_learning_rate (float): The starting step size before line search adjustment.\n        line_search_fn (Callable[[np.ndarray, np.ndarray], float]): A callable that computes the optimal \n            step size given the current parameters and gradient. This function should accept the\n            current parameter vector and gradient and return a suitable learning rate.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray, loss_fn: Callable[[np.ndarray], float]) -> np.ndarray:\n            Updates the parameters using gradient descent with a dynamically determined step size.\n            \n            Args:\n                params (np.ndarray): Current model parameters.\n                grads (np.ndarray): Gradients computed for the current parameters.\n                loss_fn (Callable[[np.ndarray], float]): The loss function to evaluate parameter updates.\n            \n            Returns:\n                np.ndarray: Updated parameters after applying the line search-based gradient descent step.\n    \"\"\"\n\n    def __init__(self, initial_learning_rate: float, line_search_fn: Callable[[np.ndarray, np.ndarray], float]) -> None:\n        self.initial_learning_rate = initial_learning_rate\n        self.line_search_fn = line_search_fn\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray, loss_fn: Callable[[np.ndarray], float]) -> np.ndarray:\n        pass\n\nclass AdamOptimizer:\n    \"\"\"\n    Optimizer implementing the Adam algorithm, which combines adaptive learning rates and momentum.\n\n    Adam maintains exponential moving averages of both the gradient and its square, and it\n    includes bias-correction terms. It is widely used for training deep learning models due\n    to its computational efficiency and robust performance on large datasets.\n\n    Args:\n        learning_rate (float): The initial step size for parameter updates.\n        beta1 (float): Exponential decay rate for the first moment estimates.\n        beta2 (float): Exponential decay rate for the second moment estimates.\n        epsilon (float): A small constant to prevent division by zero (usually around 1e-8).\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Performs an update on the parameters using the Adam optimization algorithm.\n            \n            Args:\n                params (np.ndarray): Current parameters as a numpy array.\n                grads (np.ndarray): Gradients calculated from the loss function.\n            \n            Returns:\n                np.ndarray: Updated parameters after the Adam step.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, beta1: float=0.9, beta2: float=0.999, epsilon: float=1e-08) -> None:\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.m = None\n        self.v = None\n        self.t = 0\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "from typing import Any",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "import numpy as np",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import numpy as np",
                                                            "lineno": 2
                                                        },
                                                        {
                                                            "name": "from typing import Any, Callable",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "from typing import Any, Callable",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "AdagradOptimizer",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class AdagradOptimizer:\n    \"\"\"\n    Optimizer implementing the Adagrad algorithm which adapts learning rate for each parameter.\n    \n    This optimizer adjusts the learning rate for each parameter based on the history of gradients.\n    It is typically used when dealing with sparse data or scenarios where different parameters\n    require different updates.\n\n    Args:\n        learning_rate (float): The initial step size for gradient updates.\n        epsilon (float): A small constant to prevent division by zero, typically 1e-8.\n\n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Updates the parameters using the Adagrad algorithm.\n            \n            Args:\n                params (np.ndarray): Current parameters to be updated.\n                grads (np.ndarray): Gradients computed from the loss function.\n            \n            Returns:\n                np.ndarray: Updated parameters.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, epsilon: float=1e-08) -> None:\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon\n        self.grad_squared_accum = None\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass",
                                                            "lineno": 5
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": "AdagradOptimizer",
                                                            "extra": {},
                                                            "code": "def __init__(self, learning_rate: float, epsilon: float=1e-08) -> None:\n    self.learning_rate = learning_rate\n    self.epsilon = epsilon\n    self.grad_squared_accum = None\n    pass",
                                                            "lineno": 29
                                                        },
                                                        {
                                                            "name": "step",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": "AdagradOptimizer",
                                                            "extra": {},
                                                            "code": "def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n    pass",
                                                            "lineno": 35
                                                        },
                                                        {
                                                            "name": "NesterovAcceleratedGradient",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class NesterovAcceleratedGradient:\n    \"\"\"\n    Optimizer implementing Nesterov Accelerated Gradient (NAG) method.\n\n    This optimizer improves upon the standard momentum method by computing the gradient \n    at the approximate future position of the parameters, leading to faster convergence.\n\n    Args:\n        learning_rate (float): The step size for updating parameters.\n        momentum (float): The momentum factor (typically between 0 and 1).\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Perform a parameter update using the Nesterov accelerated gradient method.\n            \n            Args:\n                params (np.ndarray): The current model parameters.\n                grads (np.ndarray): The computed gradients based on the lookahead position.\n            \n            Returns:\n                np.ndarray: Updated parameters after applying the NAG update.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, momentum: float) -> None:\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.velocity = None\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass",
                                                            "lineno": 38
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": "NesterovAcceleratedGradient",
                                                            "extra": {},
                                                            "code": "def __init__(self, learning_rate: float, momentum: float) -> None:\n    self.learning_rate = learning_rate\n    self.momentum = momentum\n    self.velocity = None\n    pass",
                                                            "lineno": 61
                                                        },
                                                        {
                                                            "name": "step",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": "NesterovAcceleratedGradient",
                                                            "extra": {},
                                                            "code": "def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n    pass",
                                                            "lineno": 67
                                                        },
                                                        {
                                                            "name": "RMSPropOptimizer",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class RMSPropOptimizer:\n    \"\"\"\n    Optimizer implementing the RMSProp algorithm.\n\n    RMSProp adapts the learning rate for each parameter by dividing the learning rate\n    by a running average of the magnitudes of recent gradients, which helps with faster and \n    more stable convergence.\n\n    Args:\n        learning_rate (float): Initial learning rate for the optimizer.\n        decay_rate (float): Decay rate for the moving average of squared gradients.\n        epsilon (float): A small constant to prevent division by zero, typically set to 1e-8.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Updates parameters using RMSProp optimization.\n            \n            Args:\n                params (np.ndarray): Array of current parameters.\n                grads (np.ndarray): Array of gradients computed from the loss.\n            \n            Returns:\n                np.ndarray: Updated parameters after applying the RMSProp step.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, decay_rate: float, epsilon: float=1e-08) -> None:\n        self.learning_rate = learning_rate\n        self.decay_rate = decay_rate\n        self.epsilon = epsilon\n        self.squared_avg = None\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass",
                                                            "lineno": 70
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": "RMSPropOptimizer",
                                                            "extra": {},
                                                            "code": "def __init__(self, learning_rate: float, decay_rate: float, epsilon: float=1e-08) -> None:\n    self.learning_rate = learning_rate\n    self.decay_rate = decay_rate\n    self.epsilon = epsilon\n    self.squared_avg = None\n    pass",
                                                            "lineno": 95
                                                        },
                                                        {
                                                            "name": "step",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": "RMSPropOptimizer",
                                                            "extra": {},
                                                            "code": "def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n    pass",
                                                            "lineno": 102
                                                        },
                                                        {
                                                            "name": "GDWithLineSearch",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class GDWithLineSearch:\n    \"\"\"\n    Optimizer that uses gradient descent augmented with line search.\n\n    This method performs standard gradient descent updates while dynamically determining\n    an optimal learning rate through a line search strategy. The line search procedure iteratively\n    selects a step size that sufficiently decreases the loss function.\n\n    Args:\n        initial_learning_rate (float): The starting step size before line search adjustment.\n        line_search_fn (Callable[[np.ndarray, np.ndarray], float]): A callable that computes the optimal \n            step size given the current parameters and gradient. This function should accept the\n            current parameter vector and gradient and return a suitable learning rate.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray, loss_fn: Callable[[np.ndarray], float]) -> np.ndarray:\n            Updates the parameters using gradient descent with a dynamically determined step size.\n            \n            Args:\n                params (np.ndarray): Current model parameters.\n                grads (np.ndarray): Gradients computed for the current parameters.\n                loss_fn (Callable[[np.ndarray], float]): The loss function to evaluate parameter updates.\n            \n            Returns:\n                np.ndarray: Updated parameters after applying the line search-based gradient descent step.\n    \"\"\"\n\n    def __init__(self, initial_learning_rate: float, line_search_fn: Callable[[np.ndarray, np.ndarray], float]) -> None:\n        self.initial_learning_rate = initial_learning_rate\n        self.line_search_fn = line_search_fn\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray, loss_fn: Callable[[np.ndarray], float]) -> np.ndarray:\n        pass",
                                                            "lineno": 105
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": "GDWithLineSearch",
                                                            "extra": {},
                                                            "code": "def __init__(self, initial_learning_rate: float, line_search_fn: Callable[[np.ndarray, np.ndarray], float]) -> None:\n    self.initial_learning_rate = initial_learning_rate\n    self.line_search_fn = line_search_fn\n    pass",
                                                            "lineno": 132
                                                        },
                                                        {
                                                            "name": "step",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": "GDWithLineSearch",
                                                            "extra": {},
                                                            "code": "def step(self, params: np.ndarray, grads: np.ndarray, loss_fn: Callable[[np.ndarray], float]) -> np.ndarray:\n    pass",
                                                            "lineno": 137
                                                        },
                                                        {
                                                            "name": "AdamOptimizer",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class AdamOptimizer:\n    \"\"\"\n    Optimizer implementing the Adam algorithm, which combines adaptive learning rates and momentum.\n\n    Adam maintains exponential moving averages of both the gradient and its square, and it\n    includes bias-correction terms. It is widely used for training deep learning models due\n    to its computational efficiency and robust performance on large datasets.\n\n    Args:\n        learning_rate (float): The initial step size for parameter updates.\n        beta1 (float): Exponential decay rate for the first moment estimates.\n        beta2 (float): Exponential decay rate for the second moment estimates.\n        epsilon (float): A small constant to prevent division by zero (usually around 1e-8).\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Performs an update on the parameters using the Adam optimization algorithm.\n            \n            Args:\n                params (np.ndarray): Current parameters as a numpy array.\n                grads (np.ndarray): Gradients calculated from the loss function.\n            \n            Returns:\n                np.ndarray: Updated parameters after the Adam step.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, beta1: float=0.9, beta2: float=0.999, epsilon: float=1e-08) -> None:\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.m = None\n        self.v = None\n        self.t = 0\n        pass\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        pass",
                                                            "lineno": 140
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": "AdamOptimizer",
                                                            "extra": {},
                                                            "code": "def __init__(self, learning_rate: float, beta1: float=0.9, beta2: float=0.999, epsilon: float=1e-08) -> None:\n    self.learning_rate = learning_rate\n    self.beta1 = beta1\n    self.beta2 = beta2\n    self.epsilon = epsilon\n    self.m = None\n    self.v = None\n    self.t = 0\n    pass",
                                                            "lineno": 166
                                                        },
                                                        {
                                                            "name": "step",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/adaptive_batch.py",
                                                            "parent": "AdamOptimizer",
                                                            "extra": {},
                                                            "code": "def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n    pass",
                                                            "lineno": 176
                                                        }
                                                    ]
                                                },
                                                {
                                                    "type": "file",
                                                    "name": "stochastic.py",
                                                    "path": "./src/algorithms/optimization/gradient_descent/stochastic.py",
                                                    "code": "import numpy as np\n\nclass SGDWithMomentum:\n    \"\"\"\n    Implements Stochastic Gradient Descent with Momentum.\n    \n    This optimizer updates model parameters using a stochastic gradient descent approach\n    enhanced with momentum to accelerate convergence and dampen oscillations.\n    \n    Attributes:\n        learning_rate (float): The step size used for each parameter update.\n        momentum (float): The momentum factor, typically between 0 and 1, that determines\n                          the contribution of past gradients.\n        velocity (np.ndarray): The accumulated velocity vector, initialized during the first call to step.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Computes the update for parameters based on gradients and momentum.\n    \n    Args:\n        learning_rate (float): The learning rate for parameter updates.\n        momentum (float): Momentum coefficient for smoothing updates.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, momentum: float) -> None:\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.velocity = None\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Perform a single optimization step using SGD with momentum.\n        \n        Args:\n            params (np.ndarray): The current model parameters.\n            grads (np.ndarray): The gradients computed for the parameters.\n            \n        Returns:\n            np.ndarray: Updated model parameters after applying momentum-based SGD step.\n            \n        Edge Cases:\n            - If velocity is uninitialized, it should be set to zeros with the same shape as params.\n            - The method assumes that params and grads have identical shapes.\n        \"\"\"\n        pass\n\nclass MiniBatchSGD:\n    \"\"\"\n    Implements Mini-Batch Stochastic Gradient Descent.\n    \n    This optimizer updates model parameters using mini-batch gradient descent, which combines \n    the advantages of both stochastic and batch gradient descent. It processes a subset of the \n    training data (mini-batch) to compute the gradients for each update.\n    \n    Attributes:\n        learning_rate (float): The step size used for each parameter update.\n        batch_size (int): The number of training examples used to compute a single gradient update.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Performs an optimization step using the computed mini-batch gradients.\n    \n    Args:\n        learning_rate (float): The learning rate for parameter updates.\n        batch_size (int): The number of samples in each mini-batch.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, batch_size: int) -> None:\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Perform a single optimization step using mini-batch SGD.\n        \n        Args:\n            params (np.ndarray): The current model parameters.\n            grads (np.ndarray): The gradients computed from a mini-batch.\n        \n        Returns:\n            np.ndarray: Updated model parameters after applying the mini-batch SGD step.\n        \n        Assumptions:\n            - The gradients are computed from a mini-batch of the dataset.\n            - The shapes of params and grads are identical.\n        \"\"\"\n        pass\n",
                                                    "feature_paths": [],
                                                    "units": [
                                                        {
                                                            "name": "import numpy as np",
                                                            "unit_type": "import",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/stochastic.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "import numpy as np",
                                                            "lineno": 1
                                                        },
                                                        {
                                                            "name": "SGDWithMomentum",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/stochastic.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class SGDWithMomentum:\n    \"\"\"\n    Implements Stochastic Gradient Descent with Momentum.\n    \n    This optimizer updates model parameters using a stochastic gradient descent approach\n    enhanced with momentum to accelerate convergence and dampen oscillations.\n    \n    Attributes:\n        learning_rate (float): The step size used for each parameter update.\n        momentum (float): The momentum factor, typically between 0 and 1, that determines\n                          the contribution of past gradients.\n        velocity (np.ndarray): The accumulated velocity vector, initialized during the first call to step.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Computes the update for parameters based on gradients and momentum.\n    \n    Args:\n        learning_rate (float): The learning rate for parameter updates.\n        momentum (float): Momentum coefficient for smoothing updates.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, momentum: float) -> None:\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.velocity = None\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Perform a single optimization step using SGD with momentum.\n        \n        Args:\n            params (np.ndarray): The current model parameters.\n            grads (np.ndarray): The gradients computed for the parameters.\n            \n        Returns:\n            np.ndarray: Updated model parameters after applying momentum-based SGD step.\n            \n        Edge Cases:\n            - If velocity is uninitialized, it should be set to zeros with the same shape as params.\n            - The method assumes that params and grads have identical shapes.\n        \"\"\"\n        pass",
                                                            "lineno": 3
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/stochastic.py",
                                                            "parent": "SGDWithMomentum",
                                                            "extra": {},
                                                            "code": "def __init__(self, learning_rate: float, momentum: float) -> None:\n    self.learning_rate = learning_rate\n    self.momentum = momentum\n    self.velocity = None",
                                                            "lineno": 25
                                                        },
                                                        {
                                                            "name": "step",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/stochastic.py",
                                                            "parent": "SGDWithMomentum",
                                                            "extra": {},
                                                            "code": "def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Perform a single optimization step using SGD with momentum.\n        \n        Args:\n            params (np.ndarray): The current model parameters.\n            grads (np.ndarray): The gradients computed for the parameters.\n            \n        Returns:\n            np.ndarray: Updated model parameters after applying momentum-based SGD step.\n            \n        Edge Cases:\n            - If velocity is uninitialized, it should be set to zeros with the same shape as params.\n            - The method assumes that params and grads have identical shapes.\n        \"\"\"\n    pass",
                                                            "lineno": 30
                                                        },
                                                        {
                                                            "name": "MiniBatchSGD",
                                                            "unit_type": "class",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/stochastic.py",
                                                            "parent": null,
                                                            "extra": {},
                                                            "code": "class MiniBatchSGD:\n    \"\"\"\n    Implements Mini-Batch Stochastic Gradient Descent.\n    \n    This optimizer updates model parameters using mini-batch gradient descent, which combines \n    the advantages of both stochastic and batch gradient descent. It processes a subset of the \n    training data (mini-batch) to compute the gradients for each update.\n    \n    Attributes:\n        learning_rate (float): The step size used for each parameter update.\n        batch_size (int): The number of training examples used to compute a single gradient update.\n    \n    Methods:\n        step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n            Performs an optimization step using the computed mini-batch gradients.\n    \n    Args:\n        learning_rate (float): The learning rate for parameter updates.\n        batch_size (int): The number of samples in each mini-batch.\n    \"\"\"\n\n    def __init__(self, learning_rate: float, batch_size: int) -> None:\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n\n    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Perform a single optimization step using mini-batch SGD.\n        \n        Args:\n            params (np.ndarray): The current model parameters.\n            grads (np.ndarray): The gradients computed from a mini-batch.\n        \n        Returns:\n            np.ndarray: Updated model parameters after applying the mini-batch SGD step.\n        \n        Assumptions:\n            - The gradients are computed from a mini-batch of the dataset.\n            - The shapes of params and grads are identical.\n        \"\"\"\n        pass",
                                                            "lineno": 47
                                                        },
                                                        {
                                                            "name": "__init__",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/stochastic.py",
                                                            "parent": "MiniBatchSGD",
                                                            "extra": {},
                                                            "code": "def __init__(self, learning_rate: float, batch_size: int) -> None:\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size",
                                                            "lineno": 68
                                                        },
                                                        {
                                                            "name": "step",
                                                            "unit_type": "method",
                                                            "file_path": "./src/algorithms/optimization/gradient_descent/stochastic.py",
                                                            "parent": "MiniBatchSGD",
                                                            "extra": {},
                                                            "code": "def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Perform a single optimization step using mini-batch SGD.\n        \n        Args:\n            params (np.ndarray): The current model parameters.\n            grads (np.ndarray): The gradients computed from a mini-batch.\n        \n        Returns:\n            np.ndarray: Updated model parameters after applying the mini-batch SGD step.\n        \n        Assumptions:\n            - The gradients are computed from a mini-batch of the dataset.\n            - The shapes of params and grads are identical.\n        \"\"\"\n    pass",
                                                            "lineno": 72
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "svm_optimization.py",
                                            "path": "./src/algorithms/optimization/svm_optimization.py",
                                            "code": "import numpy as np\nfrom typing import Any, Optional\n\nclass SVMStochasticGradientDescentOptimizer:\n    \"\"\"\n    Optimizer for SVM classification using stochastic gradient descent.\n    \n    This class implements the optimization of the SVM objective function using\n    stochastic gradient descent (SGD). It is designed to handle large datasets by\n    iteratively updating the model weights based on mini-batches or single samples.\n    \n    Attributes:\n        learning_rate (float): The step size used for each update.\n        max_iter (int): The maximum number of iterations to run the optimizer.\n        tolerance (float): The threshold for stopping criterion based on weight change.\n    \n    Methods:\n        optimize(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n            Performs the optimization to determine the optimal parameters (weights)\n            for the SVM model given the training data and labels.\n    \n    Args:\n        learning_rate (float): Learning rate for the SGD updates.\n        max_iter (int): Maximum number of iterations to perform.\n        tolerance (float): Tolerance for convergence; if updates fall below this value,\n            optimization stops.\n    \"\"\"\n\n    def __init__(self, learning_rate: float=0.01, max_iter: int=1000, tolerance: float=0.0001) -> None:\n        self.learning_rate = learning_rate\n        self.max_iter = max_iter\n        self.tolerance = tolerance\n\n    def optimize(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Optimize the SVM objective function using stochastic gradient descent.\n        \n        Processes the feature matrix X and associated labels y, iteratively updating\n        the model's weights to minimize the SVM loss function.\n        \n        Args:\n            X (np.ndarray): A 2D array representing the input features.\n            y (np.ndarray): A 1D array of target labels corresponding to X.\n        \n        Returns:\n            np.ndarray: The optimized weight vector after convergence or after\n            reaching the maximum number of iterations.\n        \"\"\"\n        pass\n\nclass SMOSVMOptimizer:\n    \"\"\"\n    Optimizer for SVM classification using the Sequential Minimal Optimization (SMO) algorithm.\n    \n    This class provides an interface for optimizing SVM quadratic programming problems\n    by applying the SMO algorithm. It incrementally adjusts the Lagrange multipliers to\n    find the optimal decision boundary. The implementation is designed to ensure efficient\n    handling of constraints and calculation of the optimum parameters.\n    \n    Attributes:\n        C (float): Regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n        tolerance (float): Tolerance for the optimization to determine convergence.\n        max_iter (int): Maximum number of iterations to attempt optimization.\n    \n    Methods:\n        optimize(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n            Executes the SMO algorithm on the provided training data to compute the optimal\n            support vectors and corresponding model parameters.\n    \n    Args:\n        C (float): The regularization parameter for SVM.\n        tolerance (float): A small value to detect convergence of the algorithm.\n        max_iter (int): The limit on the number of iterations for the optimization process.\n    \"\"\"\n\n    def __init__(self, C: float=1.0, tolerance: float=0.001, max_iter: int=1000) -> None:\n        self.C = C\n        self.tolerance = tolerance\n        self.max_iter = max_iter\n\n    def optimize(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Optimize the SVM model parameters using the SMO algorithm.\n\n        Args:\n            X (np.ndarray): A 2D numpy array containing the input features.\n            y (np.ndarray): A 1D numpy array of target labels corresponding to X.\n\n        Returns:\n            np.ndarray: The vector of optimized parameters (e.g., Lagrange multipliers or weight vector)\n            resulting from the SMO optimization process.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import numpy as np",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/optimization/svm_optimization.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import numpy as np",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "from typing import Any, Optional",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/optimization/svm_optimization.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Any, Optional",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "SVMStochasticGradientDescentOptimizer",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/optimization/svm_optimization.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class SVMStochasticGradientDescentOptimizer:\n    \"\"\"\n    Optimizer for SVM classification using stochastic gradient descent.\n    \n    This class implements the optimization of the SVM objective function using\n    stochastic gradient descent (SGD). It is designed to handle large datasets by\n    iteratively updating the model weights based on mini-batches or single samples.\n    \n    Attributes:\n        learning_rate (float): The step size used for each update.\n        max_iter (int): The maximum number of iterations to run the optimizer.\n        tolerance (float): The threshold for stopping criterion based on weight change.\n    \n    Methods:\n        optimize(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n            Performs the optimization to determine the optimal parameters (weights)\n            for the SVM model given the training data and labels.\n    \n    Args:\n        learning_rate (float): Learning rate for the SGD updates.\n        max_iter (int): Maximum number of iterations to perform.\n        tolerance (float): Tolerance for convergence; if updates fall below this value,\n            optimization stops.\n    \"\"\"\n\n    def __init__(self, learning_rate: float=0.01, max_iter: int=1000, tolerance: float=0.0001) -> None:\n        self.learning_rate = learning_rate\n        self.max_iter = max_iter\n        self.tolerance = tolerance\n\n    def optimize(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Optimize the SVM objective function using stochastic gradient descent.\n        \n        Processes the feature matrix X and associated labels y, iteratively updating\n        the model's weights to minimize the SVM loss function.\n        \n        Args:\n            X (np.ndarray): A 2D array representing the input features.\n            y (np.ndarray): A 1D array of target labels corresponding to X.\n        \n        Returns:\n            np.ndarray: The optimized weight vector after convergence or after\n            reaching the maximum number of iterations.\n        \"\"\"\n        pass",
                                                    "lineno": 4
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/optimization/svm_optimization.py",
                                                    "parent": "SVMStochasticGradientDescentOptimizer",
                                                    "extra": {},
                                                    "code": "def __init__(self, learning_rate: float=0.01, max_iter: int=1000, tolerance: float=0.0001) -> None:\n    self.learning_rate = learning_rate\n    self.max_iter = max_iter\n    self.tolerance = tolerance",
                                                    "lineno": 29
                                                },
                                                {
                                                    "name": "optimize",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/optimization/svm_optimization.py",
                                                    "parent": "SVMStochasticGradientDescentOptimizer",
                                                    "extra": {},
                                                    "code": "def optimize(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Optimize the SVM objective function using stochastic gradient descent.\n        \n        Processes the feature matrix X and associated labels y, iteratively updating\n        the model's weights to minimize the SVM loss function.\n        \n        Args:\n            X (np.ndarray): A 2D array representing the input features.\n            y (np.ndarray): A 1D array of target labels corresponding to X.\n        \n        Returns:\n            np.ndarray: The optimized weight vector after convergence or after\n            reaching the maximum number of iterations.\n        \"\"\"\n    pass",
                                                    "lineno": 34
                                                },
                                                {
                                                    "name": "SMOSVMOptimizer",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/optimization/svm_optimization.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class SMOSVMOptimizer:\n    \"\"\"\n    Optimizer for SVM classification using the Sequential Minimal Optimization (SMO) algorithm.\n    \n    This class provides an interface for optimizing SVM quadratic programming problems\n    by applying the SMO algorithm. It incrementally adjusts the Lagrange multipliers to\n    find the optimal decision boundary. The implementation is designed to ensure efficient\n    handling of constraints and calculation of the optimum parameters.\n    \n    Attributes:\n        C (float): Regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n        tolerance (float): Tolerance for the optimization to determine convergence.\n        max_iter (int): Maximum number of iterations to attempt optimization.\n    \n    Methods:\n        optimize(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n            Executes the SMO algorithm on the provided training data to compute the optimal\n            support vectors and corresponding model parameters.\n    \n    Args:\n        C (float): The regularization parameter for SVM.\n        tolerance (float): A small value to detect convergence of the algorithm.\n        max_iter (int): The limit on the number of iterations for the optimization process.\n    \"\"\"\n\n    def __init__(self, C: float=1.0, tolerance: float=0.001, max_iter: int=1000) -> None:\n        self.C = C\n        self.tolerance = tolerance\n        self.max_iter = max_iter\n\n    def optimize(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Optimize the SVM model parameters using the SMO algorithm.\n\n        Args:\n            X (np.ndarray): A 2D numpy array containing the input features.\n            y (np.ndarray): A 1D numpy array of target labels corresponding to X.\n\n        Returns:\n            np.ndarray: The vector of optimized parameters (e.g., Lagrange multipliers or weight vector)\n            resulting from the SMO optimization process.\n        \"\"\"\n        pass",
                                                    "lineno": 51
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/optimization/svm_optimization.py",
                                                    "parent": "SMOSVMOptimizer",
                                                    "extra": {},
                                                    "code": "def __init__(self, C: float=1.0, tolerance: float=0.001, max_iter: int=1000) -> None:\n    self.C = C\n    self.tolerance = tolerance\n    self.max_iter = max_iter",
                                                    "lineno": 76
                                                },
                                                {
                                                    "name": "optimize",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/optimization/svm_optimization.py",
                                                    "parent": "SMOSVMOptimizer",
                                                    "extra": {},
                                                    "code": "def optimize(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Optimize the SVM model parameters using the SMO algorithm.\n\n        Args:\n            X (np.ndarray): A 2D numpy array containing the input features.\n            y (np.ndarray): A 1D numpy array of target labels corresponding to X.\n\n        Returns:\n            np.ndarray: The vector of optimized parameters (e.g., Lagrange multipliers or weight vector)\n            resulting from the SMO optimization process.\n        \"\"\"\n    pass",
                                                    "lineno": 81
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "tree_ensemble",
                                    "path": "./src/algorithms/tree_ensemble",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "decision_tree_methods.py",
                                            "path": "./src/algorithms/tree_ensemble/decision_tree_methods.py",
                                            "code": "import pandas as pd\nfrom typing import Any, Optional, Union\nfrom typing import Any\n\ndef apply_reduced_error_pruning(tree_model: Any, X_val: pd.DataFrame, y_val: pd.Series, tolerance: Optional[float]=None) -> Any:\n    \"\"\"\n    Apply reduced error pruning to a decision tree model using a validation dataset.\n\n    This function prunes the given decision tree model by iteratively removing nodes\n    and evaluating the impact on validation accuracy. The pruning stops when the pruning\n    does not yield an improvement greater than a specified tolerance.\n\n    Args:\n        tree_model (Any): The decision tree model to be pruned.\n        X_val (pd.DataFrame): The validation features used to evaluate pruning impact.\n        y_val (pd.Series): The validation labels corresponding to X_val.\n        tolerance (Optional[float]): The minimum improvement threshold required to continue pruning.\n                                     If None, a default threshold may be used by the underlying algorithm.\n\n    Returns:\n        Any: The pruned decision tree model.\n    \"\"\"\n    pass\n\ndef apply_node_limited_pre_pruning(tree_model: Any, max_nodes: int) -> Any:\n    \"\"\"\n    Perform node-limited pre-pruning on a decision tree model by restricting the total number of nodes.\n\n    This function applies a pre-pruning strategy where the tree growth is halted once the number\n    of nodes reaches a specified maximum. This helps in controlling model complexity and overfitting.\n\n    Args:\n        tree_model (Any): The decision tree model to be pruned.\n        max_nodes (int): The maximum number of nodes allowed in the tree.\n\n    Returns:\n        Any: The pruned decision tree model with node limitation.\n    \"\"\"\n    pass\n\ndef apply_early_stopping(tree_model: Any, X_val: pd.DataFrame, y_val: pd.Series, patience: int=10) -> Any:\n    \"\"\"\n    Apply early stopping as a pruning strategy during decision tree training.\n\n    This function monitors the performance of a decision tree model on a validation dataset\n    and stops further growth when the performance fails to improve for a defined number of iterations\n    (patience). This prevents the model from overfitting.\n\n    Args:\n        tree_model (Any): The decision tree model under training or intermediate stage.\n        X_val (pd.DataFrame): The validation features used to monitor performance improvements.\n        y_val (pd.Series): The validation labels corresponding to X_val.\n        patience (int): The number of iterations with no improvement after which training is stopped.\n\n    Returns:\n        Any: The decision tree model that has been halted early to avoid overfitting.\n    \"\"\"\n    pass\n\ndef apply_cost_complexity_pruning(tree_model: Any, ccp_alpha: float) -> Any:\n    \"\"\"\n    Apply cost complexity pruning to a decision tree model to balance complexity versus predictive accuracy.\n\n    This function prunes the decision tree based on a complexity parameter (ccp_alpha). The method\n    computes a trade-off between the model complexity and its performance, removing branches that incur\n    a cost higher than the improvement in error reduction.\n\n    Args:\n        tree_model (Any): The decision tree model to be pruned.\n        ccp_alpha (float): The complexity parameter used to control the trade-off between tree size and accuracy.\n                           A larger value leads to more pruning.\n\n    Returns:\n        Any: The pruned decision tree model after applying cost complexity adjustments.\n    \"\"\"\n    pass\n\ndef apply_depth_limitation(tree_model: Any, max_depth: int) -> Any:\n    \"\"\"\n    Limit the depth of a decision tree model to prevent overfitting and excessive complexity.\n\n    This function enforces a maximum tree depth by pruning nodes that exceed the specified depth limit,\n    ensuring that the final tree maintains a controlled level of granularity.\n\n    Args:\n        tree_model (Any): The decision tree model to which the depth limitation is applied.\n        max_depth (int): The maximum allowed depth for the tree. Nodes beyond this depth will be pruned.\n\n    Returns:\n        Any: The decision tree model with enforced depth limitation.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/tree_ensemble/decision_tree_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "from typing import Any, Optional, Union",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/tree_ensemble/decision_tree_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Any, Optional, Union",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "from typing import Any",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/tree_ensemble/decision_tree_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Any",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "apply_reduced_error_pruning",
                                                    "unit_type": "function",
                                                    "file_path": "./src/algorithms/tree_ensemble/decision_tree_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def apply_reduced_error_pruning(tree_model: Any, X_val: pd.DataFrame, y_val: pd.Series, tolerance: Optional[float]=None) -> Any:\n    \"\"\"\n    Apply reduced error pruning to a decision tree model using a validation dataset.\n\n    This function prunes the given decision tree model by iteratively removing nodes\n    and evaluating the impact on validation accuracy. The pruning stops when the pruning\n    does not yield an improvement greater than a specified tolerance.\n\n    Args:\n        tree_model (Any): The decision tree model to be pruned.\n        X_val (pd.DataFrame): The validation features used to evaluate pruning impact.\n        y_val (pd.Series): The validation labels corresponding to X_val.\n        tolerance (Optional[float]): The minimum improvement threshold required to continue pruning.\n                                     If None, a default threshold may be used by the underlying algorithm.\n\n    Returns:\n        Any: The pruned decision tree model.\n    \"\"\"\n    pass",
                                                    "lineno": 5
                                                },
                                                {
                                                    "name": "apply_node_limited_pre_pruning",
                                                    "unit_type": "function",
                                                    "file_path": "./src/algorithms/tree_ensemble/decision_tree_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def apply_node_limited_pre_pruning(tree_model: Any, max_nodes: int) -> Any:\n    \"\"\"\n    Perform node-limited pre-pruning on a decision tree model by restricting the total number of nodes.\n\n    This function applies a pre-pruning strategy where the tree growth is halted once the number\n    of nodes reaches a specified maximum. This helps in controlling model complexity and overfitting.\n\n    Args:\n        tree_model (Any): The decision tree model to be pruned.\n        max_nodes (int): The maximum number of nodes allowed in the tree.\n\n    Returns:\n        Any: The pruned decision tree model with node limitation.\n    \"\"\"\n    pass",
                                                    "lineno": 25
                                                },
                                                {
                                                    "name": "apply_early_stopping",
                                                    "unit_type": "function",
                                                    "file_path": "./src/algorithms/tree_ensemble/decision_tree_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def apply_early_stopping(tree_model: Any, X_val: pd.DataFrame, y_val: pd.Series, patience: int=10) -> Any:\n    \"\"\"\n    Apply early stopping as a pruning strategy during decision tree training.\n\n    This function monitors the performance of a decision tree model on a validation dataset\n    and stops further growth when the performance fails to improve for a defined number of iterations\n    (patience). This prevents the model from overfitting.\n\n    Args:\n        tree_model (Any): The decision tree model under training or intermediate stage.\n        X_val (pd.DataFrame): The validation features used to monitor performance improvements.\n        y_val (pd.Series): The validation labels corresponding to X_val.\n        patience (int): The number of iterations with no improvement after which training is stopped.\n\n    Returns:\n        Any: The decision tree model that has been halted early to avoid overfitting.\n    \"\"\"\n    pass",
                                                    "lineno": 41
                                                },
                                                {
                                                    "name": "apply_cost_complexity_pruning",
                                                    "unit_type": "function",
                                                    "file_path": "./src/algorithms/tree_ensemble/decision_tree_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def apply_cost_complexity_pruning(tree_model: Any, ccp_alpha: float) -> Any:\n    \"\"\"\n    Apply cost complexity pruning to a decision tree model to balance complexity versus predictive accuracy.\n\n    This function prunes the decision tree based on a complexity parameter (ccp_alpha). The method\n    computes a trade-off between the model complexity and its performance, removing branches that incur\n    a cost higher than the improvement in error reduction.\n\n    Args:\n        tree_model (Any): The decision tree model to be pruned.\n        ccp_alpha (float): The complexity parameter used to control the trade-off between tree size and accuracy.\n                           A larger value leads to more pruning.\n\n    Returns:\n        Any: The pruned decision tree model after applying cost complexity adjustments.\n    \"\"\"\n    pass",
                                                    "lineno": 60
                                                },
                                                {
                                                    "name": "apply_depth_limitation",
                                                    "unit_type": "function",
                                                    "file_path": "./src/algorithms/tree_ensemble/decision_tree_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def apply_depth_limitation(tree_model: Any, max_depth: int) -> Any:\n    \"\"\"\n    Limit the depth of a decision tree model to prevent overfitting and excessive complexity.\n\n    This function enforces a maximum tree depth by pruning nodes that exceed the specified depth limit,\n    ensuring that the final tree maintains a controlled level of granularity.\n\n    Args:\n        tree_model (Any): The decision tree model to which the depth limitation is applied.\n        max_depth (int): The maximum allowed depth for the tree. Nodes beyond this depth will be pruned.\n\n    Returns:\n        Any: The decision tree model with enforced depth limitation.\n    \"\"\"\n    pass",
                                                    "lineno": 78
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "forest_voting_methods.py",
                                            "path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                            "code": "from typing import Any, List\nfrom algorithms.base_algorithm import BaseAlgorithm\nimport numpy as np\nfrom typing import Any, List, Optional\nfrom typing import Any, Optional\nimport pandas as pd\n\nclass VotingEnsemble(BaseAlgorithm):\n    \"\"\"\n    VotingEnsemble provides an ensemble method that aggregates predictions from multiple models\n    using a voting mechanism. It supports both hard voting, where predictions are determined by\n    majority rule, and soft voting, where averaged probabilities determine the final outcome.\n\n    Attributes:\n        estimators (List[Any]): A list of fitted estimators whose predictions will be aggregated.\n        voting (str): The voting type, either 'hard' for majority vote or 'soft' for probability-weighted vote.\n        weights (Optional[List[float]]): Optional list of weights corresponding to each estimator.\n    \"\"\"\n\n    def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'VotingEnsemble':\n        \"\"\"\n        Fit the ensemble using the provided training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): True labels for training.\n\n        Returns:\n            VotingEnsemble: The fitted ensemble instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict the class labels for the given input data using the voting mechanism.\n\n        Args:\n            X (pd.DataFrame): Input data on which to predict.\n\n        Returns:\n            np.ndarray: Predicted class labels.\n        \"\"\"\n        pass\n\nclass GradientBoostingEnsemble(BaseAlgorithm):\n    \"\"\"\n    GradientBoostingEnsemble implements a gradient boosting ensemble method where\n    models are trained sequentially to correct the errors of prior models. This interface\n    is designed to support boosting with decision trees for improved predictive performance.\n\n    Attributes:\n        n_estimators (int): The number of boosting stages to perform.\n        learning_rate (float): The contribution weight of each model.\n        max_depth (int): The maximum depth of individual regression estimators.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, learning_rate: float=0.1, max_depth: int=3) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'GradientBoostingEnsemble':\n        \"\"\"\n        Fit the gradient boosting ensemble to the training data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Target values.\n\n        Returns:\n            GradientBoostingEnsemble: The fitted boosting ensemble instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Generate predictions for input data using the ensemble of boosted models.\n\n        Args:\n            X (pd.DataFrame): Data for prediction.\n\n        Returns:\n            np.ndarray: Predicted values or class labels.\n        \"\"\"\n        pass\n\nclass MultiClassVotingClassifier(BaseAlgorithm):\n    \"\"\"\n    MultiClassVotingClassifier is designed to perform ensemble classification specifically\n    for multi-class problems using voting mechanisms. It aggregates predictions from several\n    base classifiers to determine the most likely class among multiple possible outcomes.\n\n    Attributes:\n        estimators (List[Any]): A list of classifier estimators.\n        voting (str): Type of voting to use ('hard' or 'soft').\n        weights (Optional[List[float]]): Optional weightings for each estimator.\n    \"\"\"\n\n    def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultiClassVotingClassifier':\n        \"\"\"\n        Fit the multi-class voting classifier using training data.\n\n        Args:\n            X (pd.DataFrame): Input feature data.\n            y (pd.Series): Multi-class target labels.\n\n        Returns:\n            MultiClassVotingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict class labels for multi-class classification using a voting scheme.\n\n        Args:\n            X (pd.DataFrame): Data for which to predict class labels.\n\n        Returns:\n            np.ndarray: Predicted class labels.\n        \"\"\"\n        pass\n\nclass RandomForestEnsemble(BaseAlgorithm):\n    \"\"\"\n    RandomForestEnsemble implements the random forest algorithm, an ensemble method that\n    builds a multitude of decision trees at training time and outputs the mode of the classes\n    for classification or mean prediction for regression tasks.\n\n    Attributes:\n        n_estimators (int): Number of trees in the forest.\n        max_features (Optional[int]): The number of features to consider when looking for the best split.\n        bootstrap (bool): Whether bootstrap samples are used when building trees.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, max_features: Optional[int]=None, bootstrap: bool=True) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'RandomForestEnsemble':\n        \"\"\"\n        Fit the random forest ensemble on the training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): Training target labels.\n\n        Returns:\n            RandomForestEnsemble: The fitted ensemble model.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict class labels or regression outputs using the random forest ensemble.\n\n        Args:\n            X (pd.DataFrame): Input data for prediction.\n\n        Returns:\n            np.ndarray: Predicted outcomes.\n        \"\"\"\n        pass\n\nclass BaggingEnsemble(BaseAlgorithm):\n    \"\"\"\n    BaggingEnsemble implements the bootstrap aggregating (bagging) method for ensemble learning.\n    This technique trains multiple instances of a base estimator on random subsets of the training data\n    and aggregates their predictions to enhance stability and accuracy.\n\n    Attributes:\n        base_estimator (Any): The base model used for bagging.\n        n_estimators (int): The number of base estimators to train.\n        max_samples (Optional[int]): The number of samples drawn from the training set for each estimator.\n    \"\"\"\n\n    def __init__(self, base_estimator: Any, n_estimators: int=10, max_samples: Optional[int]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'BaggingEnsemble':\n        \"\"\"\n        Fit the bagging ensemble by training multiple instances of the base estimator on subsets of the data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Training target labels.\n\n        Returns:\n            BaggingEnsemble: The fitted bagging ensemble model.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict outcomes by aggregating the predictions of individual base estimators.\n\n        Args:\n            X (pd.DataFrame): Data for which predictions are required.\n\n        Returns:\n            np.ndarray: Aggregated predictions.\n        \"\"\"\n        pass\n\nclass BinaryVotingClassifier(BaseAlgorithm):\n    \"\"\"\n    BinaryVotingClassifier specializes in ensemble classification for binary outcomes using a voting mechanism.\n    It aggregates predictions from multiple binary classifiers to determine the final binary label.\n\n    Attributes:\n        estimators (List[Any]): A list of binary classifier estimators.\n        voting (str): The voting strategy ('hard' or 'soft').\n        weights (Optional[List[float]]): Optional weights for each classifier.\n    \"\"\"\n\n    def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'BinaryVotingClassifier':\n        \"\"\"\n        Fit the binary voting classifier with the training data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Binary target labels.\n\n        Returns:\n            BinaryVotingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict binary class labels for the given input data using a voting mechanism.\n\n        Args:\n            X (pd.DataFrame): Data for which to predict binary labels.\n\n        Returns:\n            np.ndarray: Predicted binary class labels.\n        \"\"\"\n        pass\n\nclass QuantileRegressionForest(BaseAlgorithm):\n    \"\"\"\n    QuantileRegressionForest implements an ensemble method for quantile regression using a collection \n    of decision trees. This model is designed to estimate conditional quantiles, providing insight into \n    the distribution of the target variable rather than just the mean.\n\n    Attributes:\n        n_estimators (int): The number of trees in the forest.\n        quantiles (List[float]): The list of quantiles to estimate (e.g., [0.1, 0.5, 0.9]).\n        max_depth (int): The maximum depth of each tree.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, quantiles: List[float]=[0.1, 0.5, 0.9], max_depth: int=5) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'QuantileRegressionForest':\n        \"\"\"\n        Fit the quantile regression forest model on the training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): Target values.\n\n        Returns:\n            QuantileRegressionForest: The fitted quantile regression model.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> dict:\n        \"\"\"\n        Predict quantiles for the given input data. Returns a dictionary mapping each quantile to an array\n        of predicted values.\n\n        Args:\n            X (pd.DataFrame): Data for prediction.\n\n        Returns:\n            dict: A dictionary where keys are quantile levels and values are arrays of predictions.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "from typing import Any, List",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Any, List",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "from algorithms.base_algorithm import BaseAlgorithm",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from algorithms.base_algorithm import BaseAlgorithm",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "import numpy as np",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import numpy as np",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "from typing import Any, List, Optional",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Any, List, Optional",
                                                    "lineno": 4
                                                },
                                                {
                                                    "name": "from typing import Any, Optional",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Any, Optional",
                                                    "lineno": 5
                                                },
                                                {
                                                    "name": "import pandas as pd",
                                                    "unit_type": "import",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import pandas as pd",
                                                    "lineno": 6
                                                },
                                                {
                                                    "name": "VotingEnsemble",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class VotingEnsemble(BaseAlgorithm):\n    \"\"\"\n    VotingEnsemble provides an ensemble method that aggregates predictions from multiple models\n    using a voting mechanism. It supports both hard voting, where predictions are determined by\n    majority rule, and soft voting, where averaged probabilities determine the final outcome.\n\n    Attributes:\n        estimators (List[Any]): A list of fitted estimators whose predictions will be aggregated.\n        voting (str): The voting type, either 'hard' for majority vote or 'soft' for probability-weighted vote.\n        weights (Optional[List[float]]): Optional list of weights corresponding to each estimator.\n    \"\"\"\n\n    def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'VotingEnsemble':\n        \"\"\"\n        Fit the ensemble using the provided training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): True labels for training.\n\n        Returns:\n            VotingEnsemble: The fitted ensemble instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict the class labels for the given input data using the voting mechanism.\n\n        Args:\n            X (pd.DataFrame): Input data on which to predict.\n\n        Returns:\n            np.ndarray: Predicted class labels.\n        \"\"\"\n        pass",
                                                    "lineno": 8
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "VotingEnsemble",
                                                    "extra": {},
                                                    "code": "def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n    pass",
                                                    "lineno": 20
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "VotingEnsemble",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'VotingEnsemble':\n    \"\"\"\n        Fit the ensemble using the provided training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): True labels for training.\n\n        Returns:\n            VotingEnsemble: The fitted ensemble instance.\n        \"\"\"\n    pass",
                                                    "lineno": 23
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "VotingEnsemble",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n        Predict the class labels for the given input data using the voting mechanism.\n\n        Args:\n            X (pd.DataFrame): Input data on which to predict.\n\n        Returns:\n            np.ndarray: Predicted class labels.\n        \"\"\"\n    pass",
                                                    "lineno": 36
                                                },
                                                {
                                                    "name": "GradientBoostingEnsemble",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class GradientBoostingEnsemble(BaseAlgorithm):\n    \"\"\"\n    GradientBoostingEnsemble implements a gradient boosting ensemble method where\n    models are trained sequentially to correct the errors of prior models. This interface\n    is designed to support boosting with decision trees for improved predictive performance.\n\n    Attributes:\n        n_estimators (int): The number of boosting stages to perform.\n        learning_rate (float): The contribution weight of each model.\n        max_depth (int): The maximum depth of individual regression estimators.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, learning_rate: float=0.1, max_depth: int=3) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'GradientBoostingEnsemble':\n        \"\"\"\n        Fit the gradient boosting ensemble to the training data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Target values.\n\n        Returns:\n            GradientBoostingEnsemble: The fitted boosting ensemble instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Generate predictions for input data using the ensemble of boosted models.\n\n        Args:\n            X (pd.DataFrame): Data for prediction.\n\n        Returns:\n            np.ndarray: Predicted values or class labels.\n        \"\"\"\n        pass",
                                                    "lineno": 48
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "GradientBoostingEnsemble",
                                                    "extra": {},
                                                    "code": "def __init__(self, n_estimators: int=100, learning_rate: float=0.1, max_depth: int=3) -> None:\n    pass",
                                                    "lineno": 60
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "GradientBoostingEnsemble",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'GradientBoostingEnsemble':\n    \"\"\"\n        Fit the gradient boosting ensemble to the training data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Target values.\n\n        Returns:\n            GradientBoostingEnsemble: The fitted boosting ensemble instance.\n        \"\"\"\n    pass",
                                                    "lineno": 63
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "GradientBoostingEnsemble",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n        Generate predictions for input data using the ensemble of boosted models.\n\n        Args:\n            X (pd.DataFrame): Data for prediction.\n\n        Returns:\n            np.ndarray: Predicted values or class labels.\n        \"\"\"\n    pass",
                                                    "lineno": 76
                                                },
                                                {
                                                    "name": "MultiClassVotingClassifier",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class MultiClassVotingClassifier(BaseAlgorithm):\n    \"\"\"\n    MultiClassVotingClassifier is designed to perform ensemble classification specifically\n    for multi-class problems using voting mechanisms. It aggregates predictions from several\n    base classifiers to determine the most likely class among multiple possible outcomes.\n\n    Attributes:\n        estimators (List[Any]): A list of classifier estimators.\n        voting (str): Type of voting to use ('hard' or 'soft').\n        weights (Optional[List[float]]): Optional weightings for each estimator.\n    \"\"\"\n\n    def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultiClassVotingClassifier':\n        \"\"\"\n        Fit the multi-class voting classifier using training data.\n\n        Args:\n            X (pd.DataFrame): Input feature data.\n            y (pd.Series): Multi-class target labels.\n\n        Returns:\n            MultiClassVotingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict class labels for multi-class classification using a voting scheme.\n\n        Args:\n            X (pd.DataFrame): Data for which to predict class labels.\n\n        Returns:\n            np.ndarray: Predicted class labels.\n        \"\"\"\n        pass",
                                                    "lineno": 88
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "MultiClassVotingClassifier",
                                                    "extra": {},
                                                    "code": "def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n    pass",
                                                    "lineno": 100
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "MultiClassVotingClassifier",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultiClassVotingClassifier':\n    \"\"\"\n        Fit the multi-class voting classifier using training data.\n\n        Args:\n            X (pd.DataFrame): Input feature data.\n            y (pd.Series): Multi-class target labels.\n\n        Returns:\n            MultiClassVotingClassifier: The fitted classifier instance.\n        \"\"\"\n    pass",
                                                    "lineno": 103
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "MultiClassVotingClassifier",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n        Predict class labels for multi-class classification using a voting scheme.\n\n        Args:\n            X (pd.DataFrame): Data for which to predict class labels.\n\n        Returns:\n            np.ndarray: Predicted class labels.\n        \"\"\"\n    pass",
                                                    "lineno": 116
                                                },
                                                {
                                                    "name": "RandomForestEnsemble",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class RandomForestEnsemble(BaseAlgorithm):\n    \"\"\"\n    RandomForestEnsemble implements the random forest algorithm, an ensemble method that\n    builds a multitude of decision trees at training time and outputs the mode of the classes\n    for classification or mean prediction for regression tasks.\n\n    Attributes:\n        n_estimators (int): Number of trees in the forest.\n        max_features (Optional[int]): The number of features to consider when looking for the best split.\n        bootstrap (bool): Whether bootstrap samples are used when building trees.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, max_features: Optional[int]=None, bootstrap: bool=True) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'RandomForestEnsemble':\n        \"\"\"\n        Fit the random forest ensemble on the training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): Training target labels.\n\n        Returns:\n            RandomForestEnsemble: The fitted ensemble model.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict class labels or regression outputs using the random forest ensemble.\n\n        Args:\n            X (pd.DataFrame): Input data for prediction.\n\n        Returns:\n            np.ndarray: Predicted outcomes.\n        \"\"\"\n        pass",
                                                    "lineno": 128
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "RandomForestEnsemble",
                                                    "extra": {},
                                                    "code": "def __init__(self, n_estimators: int=100, max_features: Optional[int]=None, bootstrap: bool=True) -> None:\n    pass",
                                                    "lineno": 140
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "RandomForestEnsemble",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'RandomForestEnsemble':\n    \"\"\"\n        Fit the random forest ensemble on the training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): Training target labels.\n\n        Returns:\n            RandomForestEnsemble: The fitted ensemble model.\n        \"\"\"\n    pass",
                                                    "lineno": 143
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "RandomForestEnsemble",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n        Predict class labels or regression outputs using the random forest ensemble.\n\n        Args:\n            X (pd.DataFrame): Input data for prediction.\n\n        Returns:\n            np.ndarray: Predicted outcomes.\n        \"\"\"\n    pass",
                                                    "lineno": 156
                                                },
                                                {
                                                    "name": "BaggingEnsemble",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class BaggingEnsemble(BaseAlgorithm):\n    \"\"\"\n    BaggingEnsemble implements the bootstrap aggregating (bagging) method for ensemble learning.\n    This technique trains multiple instances of a base estimator on random subsets of the training data\n    and aggregates their predictions to enhance stability and accuracy.\n\n    Attributes:\n        base_estimator (Any): The base model used for bagging.\n        n_estimators (int): The number of base estimators to train.\n        max_samples (Optional[int]): The number of samples drawn from the training set for each estimator.\n    \"\"\"\n\n    def __init__(self, base_estimator: Any, n_estimators: int=10, max_samples: Optional[int]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'BaggingEnsemble':\n        \"\"\"\n        Fit the bagging ensemble by training multiple instances of the base estimator on subsets of the data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Training target labels.\n\n        Returns:\n            BaggingEnsemble: The fitted bagging ensemble model.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict outcomes by aggregating the predictions of individual base estimators.\n\n        Args:\n            X (pd.DataFrame): Data for which predictions are required.\n\n        Returns:\n            np.ndarray: Aggregated predictions.\n        \"\"\"\n        pass",
                                                    "lineno": 168
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "BaggingEnsemble",
                                                    "extra": {},
                                                    "code": "def __init__(self, base_estimator: Any, n_estimators: int=10, max_samples: Optional[int]=None) -> None:\n    pass",
                                                    "lineno": 180
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "BaggingEnsemble",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'BaggingEnsemble':\n    \"\"\"\n        Fit the bagging ensemble by training multiple instances of the base estimator on subsets of the data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Training target labels.\n\n        Returns:\n            BaggingEnsemble: The fitted bagging ensemble model.\n        \"\"\"\n    pass",
                                                    "lineno": 183
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "BaggingEnsemble",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n        Predict outcomes by aggregating the predictions of individual base estimators.\n\n        Args:\n            X (pd.DataFrame): Data for which predictions are required.\n\n        Returns:\n            np.ndarray: Aggregated predictions.\n        \"\"\"\n    pass",
                                                    "lineno": 196
                                                },
                                                {
                                                    "name": "BinaryVotingClassifier",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class BinaryVotingClassifier(BaseAlgorithm):\n    \"\"\"\n    BinaryVotingClassifier specializes in ensemble classification for binary outcomes using a voting mechanism.\n    It aggregates predictions from multiple binary classifiers to determine the final binary label.\n\n    Attributes:\n        estimators (List[Any]): A list of binary classifier estimators.\n        voting (str): The voting strategy ('hard' or 'soft').\n        weights (Optional[List[float]]): Optional weights for each classifier.\n    \"\"\"\n\n    def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'BinaryVotingClassifier':\n        \"\"\"\n        Fit the binary voting classifier with the training data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Binary target labels.\n\n        Returns:\n            BinaryVotingClassifier: The fitted classifier instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Predict binary class labels for the given input data using a voting mechanism.\n\n        Args:\n            X (pd.DataFrame): Data for which to predict binary labels.\n\n        Returns:\n            np.ndarray: Predicted binary class labels.\n        \"\"\"\n        pass",
                                                    "lineno": 208
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "BinaryVotingClassifier",
                                                    "extra": {},
                                                    "code": "def __init__(self, estimators: List[Any], voting: str='hard', weights: Optional[List[float]]=None) -> None:\n    pass",
                                                    "lineno": 219
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "BinaryVotingClassifier",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'BinaryVotingClassifier':\n    \"\"\"\n        Fit the binary voting classifier with the training data.\n\n        Args:\n            X (pd.DataFrame): Training features.\n            y (pd.Series): Binary target labels.\n\n        Returns:\n            BinaryVotingClassifier: The fitted classifier instance.\n        \"\"\"\n    pass",
                                                    "lineno": 222
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "BinaryVotingClassifier",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n        Predict binary class labels for the given input data using a voting mechanism.\n\n        Args:\n            X (pd.DataFrame): Data for which to predict binary labels.\n\n        Returns:\n            np.ndarray: Predicted binary class labels.\n        \"\"\"\n    pass",
                                                    "lineno": 235
                                                },
                                                {
                                                    "name": "QuantileRegressionForest",
                                                    "unit_type": "class",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class QuantileRegressionForest(BaseAlgorithm):\n    \"\"\"\n    QuantileRegressionForest implements an ensemble method for quantile regression using a collection \n    of decision trees. This model is designed to estimate conditional quantiles, providing insight into \n    the distribution of the target variable rather than just the mean.\n\n    Attributes:\n        n_estimators (int): The number of trees in the forest.\n        quantiles (List[float]): The list of quantiles to estimate (e.g., [0.1, 0.5, 0.9]).\n        max_depth (int): The maximum depth of each tree.\n    \"\"\"\n\n    def __init__(self, n_estimators: int=100, quantiles: List[float]=[0.1, 0.5, 0.9], max_depth: int=5) -> None:\n        pass\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'QuantileRegressionForest':\n        \"\"\"\n        Fit the quantile regression forest model on the training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): Target values.\n\n        Returns:\n            QuantileRegressionForest: The fitted quantile regression model.\n        \"\"\"\n        pass\n\n    def predict(self, X: pd.DataFrame) -> dict:\n        \"\"\"\n        Predict quantiles for the given input data. Returns a dictionary mapping each quantile to an array\n        of predicted values.\n\n        Args:\n            X (pd.DataFrame): Data for prediction.\n\n        Returns:\n            dict: A dictionary where keys are quantile levels and values are arrays of predictions.\n        \"\"\"\n        pass",
                                                    "lineno": 247
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "QuantileRegressionForest",
                                                    "extra": {},
                                                    "code": "def __init__(self, n_estimators: int=100, quantiles: List[float]=[0.1, 0.5, 0.9], max_depth: int=5) -> None:\n    pass",
                                                    "lineno": 259
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "QuantileRegressionForest",
                                                    "extra": {},
                                                    "code": "def fit(self, X: pd.DataFrame, y: pd.Series) -> 'QuantileRegressionForest':\n    \"\"\"\n        Fit the quantile regression forest model on the training data.\n\n        Args:\n            X (pd.DataFrame): Training feature data.\n            y (pd.Series): Target values.\n\n        Returns:\n            QuantileRegressionForest: The fitted quantile regression model.\n        \"\"\"\n    pass",
                                                    "lineno": 262
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/algorithms/tree_ensemble/forest_voting_methods.py",
                                                    "parent": "QuantileRegressionForest",
                                                    "extra": {},
                                                    "code": "def predict(self, X: pd.DataFrame) -> dict:\n    \"\"\"\n        Predict quantiles for the given input data. Returns a dictionary mapping each quantile to an array\n        of predicted values.\n\n        Args:\n            X (pd.DataFrame): Data for prediction.\n\n        Returns:\n            dict: A dictionary where keys are quantile levels and values are arrays of predictions.\n        \"\"\"\n    pass",
                                                    "lineno": 275
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "type": "directory",
                            "name": "advanced_modeling",
                            "path": "./src/advanced_modeling",
                            "children": [
                                {
                                    "type": "file",
                                    "name": "base_evaluator_extension.py",
                                    "path": "./src/advanced_modeling/base_evaluator_extension.py",
                                    "code": "from abc import ABC, abstractmethod\nfrom general.base import BaseEvaluator\n\nclass BaseAdvancedEvaluator(BaseEvaluator, ABC):\n    \"\"\"\n    BaseAdvancedEvaluator extends the basic evaluation interface to support\n    advanced modeling techniques. Components such as comparative evaluation,\n    statistical testing, and metric visualization should inherit from this class.\n    \"\"\"\n    \n    @abstractmethod\n    def advanced_evaluate(self, predictions, ground_truth):\n        \"\"\"\n        Perform an advanced evaluation of model predictions, potentially including\n        statistical tests or multiple metric comparisons.\n\n        Args:\n            predictions (array-like): Model predictions.\n            ground_truth (array-like): True labels or values.\n\n        Returns:\n            dict: Detailed evaluation metrics.\n        \"\"\"\n        pass\n\n    def report(self, metrics):\n        \"\"\"\n        Generate a detailed report based on advanced evaluation metrics.\n\n        Args:\n            metrics (dict): Dictionary of advanced metric scores.\n\n        Returns:\n            str: A formatted evaluation report.\n        \"\"\"\n        pass",
                                    "feature_paths": [],
                                    "units": [
                                        {
                                            "name": "from abc import ABC, abstractmethod",
                                            "unit_type": "import",
                                            "file_path": "./src/advanced_modeling/base_evaluator_extension.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "from abc import ABC, abstractmethod",
                                            "lineno": 1
                                        },
                                        {
                                            "name": "from general.base import BaseEvaluator",
                                            "unit_type": "import",
                                            "file_path": "./src/advanced_modeling/base_evaluator_extension.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "from general.base import BaseEvaluator",
                                            "lineno": 2
                                        },
                                        {
                                            "name": "BaseAdvancedEvaluator",
                                            "unit_type": "class",
                                            "file_path": "./src/advanced_modeling/base_evaluator_extension.py",
                                            "parent": null,
                                            "extra": {},
                                            "code": "class BaseAdvancedEvaluator(BaseEvaluator, ABC):\n    \"\"\"\n    BaseAdvancedEvaluator extends the basic evaluation interface to support\n    advanced modeling techniques. Components such as comparative evaluation,\n    statistical testing, and metric visualization should inherit from this class.\n    \"\"\"\n\n    @abstractmethod\n    def advanced_evaluate(self, predictions, ground_truth):\n        \"\"\"\n        Perform an advanced evaluation of model predictions, potentially including\n        statistical tests or multiple metric comparisons.\n\n        Args:\n            predictions (array-like): Model predictions.\n            ground_truth (array-like): True labels or values.\n\n        Returns:\n            dict: Detailed evaluation metrics.\n        \"\"\"\n        pass\n\n    def report(self, metrics):\n        \"\"\"\n        Generate a detailed report based on advanced evaluation metrics.\n\n        Args:\n            metrics (dict): Dictionary of advanced metric scores.\n\n        Returns:\n            str: A formatted evaluation report.\n        \"\"\"\n        pass",
                                            "lineno": 4
                                        },
                                        {
                                            "name": "advanced_evaluate",
                                            "unit_type": "method",
                                            "file_path": "./src/advanced_modeling/base_evaluator_extension.py",
                                            "parent": "BaseAdvancedEvaluator",
                                            "extra": {},
                                            "code": "@abstractmethod\ndef advanced_evaluate(self, predictions, ground_truth):\n    \"\"\"\n        Perform an advanced evaluation of model predictions, potentially including\n        statistical tests or multiple metric comparisons.\n\n        Args:\n            predictions (array-like): Model predictions.\n            ground_truth (array-like): True labels or values.\n\n        Returns:\n            dict: Detailed evaluation metrics.\n        \"\"\"\n    pass",
                                            "lineno": 12
                                        },
                                        {
                                            "name": "report",
                                            "unit_type": "method",
                                            "file_path": "./src/advanced_modeling/base_evaluator_extension.py",
                                            "parent": "BaseAdvancedEvaluator",
                                            "extra": {},
                                            "code": "def report(self, metrics):\n    \"\"\"\n        Generate a detailed report based on advanced evaluation metrics.\n\n        Args:\n            metrics (dict): Dictionary of advanced metric scores.\n\n        Returns:\n            str: A formatted evaluation report.\n        \"\"\"\n    pass",
                                            "lineno": 26
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "model_evaluation",
                                    "path": "./src/advanced_modeling/model_evaluation",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "roc_analysis.py",
                                            "path": "./src/advanced_modeling/model_evaluation/roc_analysis.py",
                                            "code": "import numpy as np\n\nclass ROCCurveVisualizer:\n    \"\"\"\n    Provides functionalities for visualizing ROC curves along with threshold selection and AUC computation.\n\n    This class encapsulates methods to plot ROC curves, select optimal thresholds based on metrics,\n    and compute the Area Under the Curve (AUC) from the ROC data. This combined interface supports\n    interactive and analytical evaluation of classifier performance.\n\n    Methods:\n        plot_roc_curve:\n            Generate a plot of the ROC curve given arrays of false positive and true positive rates.\n        select_threshold:\n            Determine the optimal threshold value using a metric (e.g., Youden's index).\n        compute_auc:\n            Calculate the Area Under the ROC Curve (AUC) from input ROC data.\n\n    Usage:\n        Instantiate the class and invoke its methods with appropriate numpy arrays to perform visualization\n        and analysis of ROC performance.\n    \"\"\"\n\n    def plot_roc_curve(self, fpr: np.ndarray, tpr: np.ndarray) -> None:\n        \"\"\"\n        Plot the ROC curve using false positive rates and true positive rates.\n\n        Args:\n            fpr (np.ndarray): Array of false positive rates.\n            tpr (np.ndarray): Array of true positive rates.\n\n        Returns:\n            None\n\n        Edge Cases:\n            - It is assumed that fpr and tpr are of equal length. Inconsistent array lengths must be handled externally.\n        \"\"\"\n        pass\n\n    def select_threshold(self, thresholds: np.ndarray, metric_values: np.ndarray) -> float:\n        \"\"\"\n        Select the optimal threshold based on a given metric (e.g., maximizing Youden's index).\n\n        Args:\n            thresholds (np.ndarray): Array of threshold values corresponding to the ROC points.\n            metric_values (np.ndarray): Array of computed metric values at each threshold.\n\n        Returns:\n            float: The selected optimal threshold value.\n\n        Notes:\n            - In cases where multiple thresholds achieve the same optimal metric value, the first occurrence is returned.\n        \"\"\"\n        pass\n\n    def compute_auc(self, fpr: np.ndarray, tpr: np.ndarray) -> float:\n        \"\"\"\n        Compute the Area Under the Curve (AUC) for the provided ROC data.\n\n        Args:\n            fpr (np.ndarray): Array of false positive rates.\n            tpr (np.ndarray): Array of true positive rates.\n\n        Returns:\n            float: The computed AUC value.\n\n        Edge Cases:\n            - The method assumes that the input arrays are non-empty and of equal length.\n        \"\"\"\n        pass\n\ndef compute_multiclass_roc(y_true: np.ndarray, y_score: np.ndarray, classes: list) -> dict:\n    \"\"\"\n    Compute ROC metrics for multi-class classification.\n\n    This function calculates the ROC curve for each class in a multi-class classification setting.\n    It returns false positive rates, true positive rates, and corresponding thresholds for each class.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix indicating true classes.\n        y_score (np.ndarray): Array of probability estimates or decision scores from the classifier.\n        classes (list): List of class identifiers corresponding to columns in y_score.\n\n    Returns:\n        dict: A dictionary where each key is a class identifier and each value is another dictionary\n              containing 'fpr' (np.ndarray), 'tpr' (np.ndarray), and 'thresholds' (np.ndarray).\n\n    Notes:\n        - It is assumed that y_true and y_score have matching dimensions and ordering.\n        - The function does not handle any plotting or visualization.\n    \"\"\"\n    pass\n\ndef compute_micro_average_roc(y_true: np.ndarray, y_score: np.ndarray) -> dict:\n    \"\"\"\n    Compute the micro-average ROC curve for multi-label or multi-class classification.\n\n    This function aggregates the performance across all classes to generate a single ROC curve,\n    which is particularly useful when dealing with imbalanced or multi-label datasets.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix.\n        y_score (np.ndarray): Array of probability estimates or decision scores for each class.\n\n    Returns:\n        dict: A dictionary containing 'fpr' (np.ndarray), 'tpr' (np.ndarray), and 'thresholds' (np.ndarray)\n              representing the micro-average ROC curve metrics.\n\n    Edge Cases:\n        - If the arrays are mismatched in dimensions, it is assumed that validation occurs externally.\n    \"\"\"\n    pass\n\ndef compute_precision_recall_curve(y_true: np.ndarray, y_score: np.ndarray, average: str='binary') -> dict:\n    \"\"\"\n    Compute the precision-recall curve for model evaluation.\n\n    This function calculates precision and recall at various threshold settings, aiding in the analysis\n    of the trade-off between precision and recall in classification tasks.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix for multi-class problems.\n        y_score (np.ndarray): Array of predicted probabilities or decision scores.\n        average (str, optional): Specifies the averaging method ('binary', 'macro', 'micro'). Defaults to 'binary'.\n\n    Returns:\n        dict: A dictionary containing arrays for 'precision', 'recall', and 'thresholds' (each as np.ndarray).\n\n    Edge Cases:\n        - The function assumes that y_true and y_score are properly formatted and that the 'average'\n          parameter is appropriately set for the task.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "import numpy as np",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/roc_analysis.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import numpy as np",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "ROCCurveVisualizer",
                                                    "unit_type": "class",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/roc_analysis.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class ROCCurveVisualizer:\n    \"\"\"\n    Provides functionalities for visualizing ROC curves along with threshold selection and AUC computation.\n\n    This class encapsulates methods to plot ROC curves, select optimal thresholds based on metrics,\n    and compute the Area Under the Curve (AUC) from the ROC data. This combined interface supports\n    interactive and analytical evaluation of classifier performance.\n\n    Methods:\n        plot_roc_curve:\n            Generate a plot of the ROC curve given arrays of false positive and true positive rates.\n        select_threshold:\n            Determine the optimal threshold value using a metric (e.g., Youden's index).\n        compute_auc:\n            Calculate the Area Under the ROC Curve (AUC) from input ROC data.\n\n    Usage:\n        Instantiate the class and invoke its methods with appropriate numpy arrays to perform visualization\n        and analysis of ROC performance.\n    \"\"\"\n\n    def plot_roc_curve(self, fpr: np.ndarray, tpr: np.ndarray) -> None:\n        \"\"\"\n        Plot the ROC curve using false positive rates and true positive rates.\n\n        Args:\n            fpr (np.ndarray): Array of false positive rates.\n            tpr (np.ndarray): Array of true positive rates.\n\n        Returns:\n            None\n\n        Edge Cases:\n            - It is assumed that fpr and tpr are of equal length. Inconsistent array lengths must be handled externally.\n        \"\"\"\n        pass\n\n    def select_threshold(self, thresholds: np.ndarray, metric_values: np.ndarray) -> float:\n        \"\"\"\n        Select the optimal threshold based on a given metric (e.g., maximizing Youden's index).\n\n        Args:\n            thresholds (np.ndarray): Array of threshold values corresponding to the ROC points.\n            metric_values (np.ndarray): Array of computed metric values at each threshold.\n\n        Returns:\n            float: The selected optimal threshold value.\n\n        Notes:\n            - In cases where multiple thresholds achieve the same optimal metric value, the first occurrence is returned.\n        \"\"\"\n        pass\n\n    def compute_auc(self, fpr: np.ndarray, tpr: np.ndarray) -> float:\n        \"\"\"\n        Compute the Area Under the Curve (AUC) for the provided ROC data.\n\n        Args:\n            fpr (np.ndarray): Array of false positive rates.\n            tpr (np.ndarray): Array of true positive rates.\n\n        Returns:\n            float: The computed AUC value.\n\n        Edge Cases:\n            - The method assumes that the input arrays are non-empty and of equal length.\n        \"\"\"\n        pass",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "plot_roc_curve",
                                                    "unit_type": "method",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/roc_analysis.py",
                                                    "parent": "ROCCurveVisualizer",
                                                    "extra": {},
                                                    "code": "def plot_roc_curve(self, fpr: np.ndarray, tpr: np.ndarray) -> None:\n    \"\"\"\n        Plot the ROC curve using false positive rates and true positive rates.\n\n        Args:\n            fpr (np.ndarray): Array of false positive rates.\n            tpr (np.ndarray): Array of true positive rates.\n\n        Returns:\n            None\n\n        Edge Cases:\n            - It is assumed that fpr and tpr are of equal length. Inconsistent array lengths must be handled externally.\n        \"\"\"\n    pass",
                                                    "lineno": 24
                                                },
                                                {
                                                    "name": "select_threshold",
                                                    "unit_type": "method",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/roc_analysis.py",
                                                    "parent": "ROCCurveVisualizer",
                                                    "extra": {},
                                                    "code": "def select_threshold(self, thresholds: np.ndarray, metric_values: np.ndarray) -> float:\n    \"\"\"\n        Select the optimal threshold based on a given metric (e.g., maximizing Youden's index).\n\n        Args:\n            thresholds (np.ndarray): Array of threshold values corresponding to the ROC points.\n            metric_values (np.ndarray): Array of computed metric values at each threshold.\n\n        Returns:\n            float: The selected optimal threshold value.\n\n        Notes:\n            - In cases where multiple thresholds achieve the same optimal metric value, the first occurrence is returned.\n        \"\"\"\n    pass",
                                                    "lineno": 40
                                                },
                                                {
                                                    "name": "compute_auc",
                                                    "unit_type": "method",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/roc_analysis.py",
                                                    "parent": "ROCCurveVisualizer",
                                                    "extra": {},
                                                    "code": "def compute_auc(self, fpr: np.ndarray, tpr: np.ndarray) -> float:\n    \"\"\"\n        Compute the Area Under the Curve (AUC) for the provided ROC data.\n\n        Args:\n            fpr (np.ndarray): Array of false positive rates.\n            tpr (np.ndarray): Array of true positive rates.\n\n        Returns:\n            float: The computed AUC value.\n\n        Edge Cases:\n            - The method assumes that the input arrays are non-empty and of equal length.\n        \"\"\"\n    pass",
                                                    "lineno": 56
                                                },
                                                {
                                                    "name": "compute_multiclass_roc",
                                                    "unit_type": "function",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/roc_analysis.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def compute_multiclass_roc(y_true: np.ndarray, y_score: np.ndarray, classes: list) -> dict:\n    \"\"\"\n    Compute ROC metrics for multi-class classification.\n\n    This function calculates the ROC curve for each class in a multi-class classification setting.\n    It returns false positive rates, true positive rates, and corresponding thresholds for each class.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix indicating true classes.\n        y_score (np.ndarray): Array of probability estimates or decision scores from the classifier.\n        classes (list): List of class identifiers corresponding to columns in y_score.\n\n    Returns:\n        dict: A dictionary where each key is a class identifier and each value is another dictionary\n              containing 'fpr' (np.ndarray), 'tpr' (np.ndarray), and 'thresholds' (np.ndarray).\n\n    Notes:\n        - It is assumed that y_true and y_score have matching dimensions and ordering.\n        - The function does not handle any plotting or visualization.\n    \"\"\"\n    pass",
                                                    "lineno": 72
                                                },
                                                {
                                                    "name": "compute_micro_average_roc",
                                                    "unit_type": "function",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/roc_analysis.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def compute_micro_average_roc(y_true: np.ndarray, y_score: np.ndarray) -> dict:\n    \"\"\"\n    Compute the micro-average ROC curve for multi-label or multi-class classification.\n\n    This function aggregates the performance across all classes to generate a single ROC curve,\n    which is particularly useful when dealing with imbalanced or multi-label datasets.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix.\n        y_score (np.ndarray): Array of probability estimates or decision scores for each class.\n\n    Returns:\n        dict: A dictionary containing 'fpr' (np.ndarray), 'tpr' (np.ndarray), and 'thresholds' (np.ndarray)\n              representing the micro-average ROC curve metrics.\n\n    Edge Cases:\n        - If the arrays are mismatched in dimensions, it is assumed that validation occurs externally.\n    \"\"\"\n    pass",
                                                    "lineno": 94
                                                },
                                                {
                                                    "name": "compute_precision_recall_curve",
                                                    "unit_type": "function",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/roc_analysis.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def compute_precision_recall_curve(y_true: np.ndarray, y_score: np.ndarray, average: str='binary') -> dict:\n    \"\"\"\n    Compute the precision-recall curve for model evaluation.\n\n    This function calculates precision and recall at various threshold settings, aiding in the analysis\n    of the trade-off between precision and recall in classification tasks.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix for multi-class problems.\n        y_score (np.ndarray): Array of predicted probabilities or decision scores.\n        average (str, optional): Specifies the averaging method ('binary', 'macro', 'micro'). Defaults to 'binary'.\n\n    Returns:\n        dict: A dictionary containing arrays for 'precision', 'recall', and 'thresholds' (each as np.ndarray).\n\n    Edge Cases:\n        - The function assumes that y_true and y_score are properly formatted and that the 'average'\n          parameter is appropriately set for the task.\n    \"\"\"\n    pass",
                                                    "lineno": 114
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "precision_recall.py",
                                            "path": "./src/advanced_modeling/model_evaluation/precision_recall.py",
                                            "code": "from typing import Optional\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_precision_recall_curve(precisions: np.ndarray, recalls: np.ndarray, average_precision: Optional[float]=None, title: str='Precision-Recall Curve') -> None:\n    \"\"\"\n    Plot a precision-recall curve given the precision and recall values, and optionally display the average precision score.\n\n    This function takes as input the computed precision and recall values, and generates a plot of the\n    precision-recall curve. The plot can include an annotation for average precision if provided. It is used\n    as part of the model evaluation process to visually assess the trade-off between precision and recall\n    across different threshold levels.\n\n    Args:\n        precisions (np.ndarray): An array of precision values calculated for different threshold values.\n        recalls (np.ndarray): An array of recall values corresponding to the precision values.\n        average_precision (Optional[float]): An optional scalar representing the average precision score.\n        title (str): Title for the plot. Defaults to \"Precision-Recall Curve\".\n\n    Returns:\n        None: This function does not return any value; it simply generates a plot.\n\n    Edge Cases:\n        - The lengths of 'precisions' and 'recalls' must be equal; otherwise, the plot cannot be generated.\n        - If the arrays are empty, the function will not produce a meaningful plot.\n        - It is assumed that 'precisions' and 'recalls' are properly precomputed and correspond to sorted threshold values.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "from typing import Optional",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/precision_recall.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Optional",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "import numpy as np",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/precision_recall.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import numpy as np",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "import matplotlib.pyplot as plt",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/precision_recall.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import matplotlib.pyplot as plt",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "plot_precision_recall_curve",
                                                    "unit_type": "function",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/precision_recall.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def plot_precision_recall_curve(precisions: np.ndarray, recalls: np.ndarray, average_precision: Optional[float]=None, title: str='Precision-Recall Curve') -> None:\n    \"\"\"\n    Plot a precision-recall curve given the precision and recall values, and optionally display the average precision score.\n\n    This function takes as input the computed precision and recall values, and generates a plot of the\n    precision-recall curve. The plot can include an annotation for average precision if provided. It is used\n    as part of the model evaluation process to visually assess the trade-off between precision and recall\n    across different threshold levels.\n\n    Args:\n        precisions (np.ndarray): An array of precision values calculated for different threshold values.\n        recalls (np.ndarray): An array of recall values corresponding to the precision values.\n        average_precision (Optional[float]): An optional scalar representing the average precision score.\n        title (str): Title for the plot. Defaults to \"Precision-Recall Curve\".\n\n    Returns:\n        None: This function does not return any value; it simply generates a plot.\n\n    Edge Cases:\n        - The lengths of 'precisions' and 'recalls' must be equal; otherwise, the plot cannot be generated.\n        - If the arrays are empty, the function will not produce a meaningful plot.\n        - It is assumed that 'precisions' and 'recalls' are properly precomputed and correspond to sorted threshold values.\n    \"\"\"\n    pass",
                                                    "lineno": 5
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "statistical_testing.py",
                                            "path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                            "code": "from typing import Any\nfrom typing import Union\nimport matplotlib.pyplot as plt\nfrom typing import Dict, Tuple, Union\nfrom typing import Dict, Any\nimport numpy as np\nfrom typing import Tuple, Union\n\ndef two_tailed_t_test(sample1: Union[np.ndarray, list], sample2: Union[np.ndarray, list], alpha: float=0.05) -> float:\n    \"\"\"\n    Perform a two-tailed t-test to compare the means of two independent samples.\n\n    This function calculates the t-test statistic and corresponding p-value for two samples under a two-tailed hypothesis.\n    It is used to assess whether the means of two independent groups are statistically different.\n\n    Args:\n        sample1 (np.ndarray or list): The first set of observations.\n        sample2 (np.ndarray or list): The second set of observations.\n        alpha (float, optional): The significance level to determine statistical threshold; default is 0.05.\n\n    Returns:\n        float: The p-value indicating the probability of observing the data under the null hypothesis.\n    \n    Edge Cases:\n        - Samples with very small sizes may result in low statistical power.\n        - Non-normal data distributions may affect test validity.\n    \"\"\"\n    pass\n\ndef one_tailed_t_test(sample1: Union[np.ndarray, list], sample2: Union[np.ndarray, list], tail: str='right', alpha: float=0.05) -> float:\n    \"\"\"\n    Perform a one-tailed t-test to compare the means of two independent samples.\n\n    This function calculates the t-test statistic and p-value for a one-tailed hypothesis, where the alternative hypothesis \n    specifies a direction (either 'right' indicating sample1 > sample2 or 'left' indicating sample1 < sample2).\n\n    Args:\n        sample1 (np.ndarray or list): The first set of observations.\n        sample2 (np.ndarray or list): The second set of observations.\n        tail (str, optional): Indicates the test direction; valid options are \"right\" or \"left\". Default is \"right\".\n        alpha (float, optional): The significance level; default is 0.05.\n\n    Returns:\n        float: The p-value for the one-tailed test.\n\n    Raises:\n        ValueError: If the 'tail' argument is not \"right\" or \"left\".\n\n    Edge Cases:\n        - Small sample sizes or non-normal data can impact the test's reliability.\n        - The function does not handle equal variances issues explicitly.\n    \"\"\"\n    pass\n\ndef paired_t_test_with_confidence_interval(before: Union[np.ndarray, list], after: Union[np.ndarray, list], confidence: float=0.95) -> Tuple[float, Tuple[float, float]]:\n    \"\"\"\n    Perform a paired t-test and calculate the confidence interval for the mean difference between paired observations.\n\n    This function is used when comparing paired samples (e.g., pre-test and post-test measurements) and needs to\n    report both the p-value and the confidence interval of the difference.\n\n    Args:\n        before (np.ndarray or list): Measurements from the first condition (e.g., before treatment).\n        after (np.ndarray or list): Measurements from the second condition (e.g., after treatment).\n        confidence (float, optional): The confidence level for the confidence interval; default is 0.95.\n\n    Returns:\n        Tuple[float, Tuple[float, float]]:\n            - float: The p-value from the paired t-test.\n            - Tuple[float, float]: The lower and upper bounds of the confidence interval for the mean difference.\n\n    Edge Cases:\n        - Requires that both inputs have the same length.\n        - Results may be influenced by outliers.\n    \"\"\"\n    pass\n\ndef paired_t_test_multiple_metrics(metrics_data: Dict[str, Tuple[Union[np.ndarray, list], Union[np.ndarray, list]]], alpha: float=0.05) -> Dict[str, float]:\n    \"\"\"\n    Perform paired t-tests on multiple metrics simultaneously, returning p-values for each metric.\n\n    This function processes a dictionary of paired data for various metrics and computes the p-value \n    for the paired t-test for each metric. It is useful in multi-metric evaluation scenarios.\n\n    Args:\n        metrics_data (Dict[str, Tuple[np.ndarray or list, np.ndarray or list]]):\n            A dictionary where each key is a metric name and its value is a tuple containing two lists or arrays,\n            representing paired observations.\n        alpha (float, optional): The significance level for the tests; default is 0.05.\n\n    Returns:\n        Dict[str, float]: A mapping from each metric name to its corresponding p-value from the paired t-test.\n\n    Edge Cases:\n        - Each metric's paired data must have matching lengths.\n        - The function does not correct for multiple comparisons by default.\n    \"\"\"\n    pass\n\ndef interpret_test_results(test_statistic: float, p_value: float, alpha: float=0.05) -> str:\n    \"\"\"\n    Interpret the results of a statistical test by providing a textual explanation based on the test statistic and p-value.\n\n    This function helps in understanding the outcome of statistical tests by comparing the p-value to the significance level\n    and summarizing the result in plain language.\n\n    Args:\n        test_statistic (float): The computed test statistic.\n        p_value (float): The p-value obtained from the statistical test.\n        alpha (float, optional): The significance level used in the test; default is 0.05.\n\n    Returns:\n        str: A textual interpretation of the statistical test result detailing whether the null hypothesis is rejected.\n\n    Edge Cases:\n        - If the p-value is exactly equal to alpha, the interpretation will note the borderline significance.\n    \"\"\"\n    pass\n\ndef visualize_paired_t_test_results(test_results: Any, title: str='Paired T-Test Results') -> None:\n    \"\"\"\n    Visualize the results of a paired t-test, optionally including the confidence interval and significant differences.\n\n    This function generates a plot to help illustrate the outcome of a paired t-test including elements such as \n    mean differences, confidence intervals, and individual sample points if applicable.\n\n    Args:\n        test_results (Any): A data structure containing the paired t-test results. This can include p-values, \n                            confidence intervals, and mean differences.\n        title (str, optional): The title for the plot; default is \"Paired T-Test Results\".\n\n    Returns:\n        None: The function creates a plot but does not return any data.\n\n    Edge Cases:\n        - Expects the input test_results to be in a format compatible with the plotting logic.\n    \"\"\"\n    pass\n\ndef test_statistical_assumptions(data: np.ndarray, assumptions: Dict[str, Any]=None) -> Dict[str, Any]:\n    \"\"\"\n    Test the statistical assumptions underlying the use of t-tests (e.g., normality, variance homogeneity).\n\n    This function evaluates whether the input data meets key assumptions required for valid t-testing.\n    It returns a dictionary with the results of these assumption tests.\n\n    Args:\n        data (np.ndarray): The dataset to be tested for normality, homogeneity of variances, and other relevant assumptions.\n        assumptions (Dict[str, Any], optional): A configuration dictionary specifying which assumption tests to perform \n                                                  and their parameters; if None, default tests are applied.\n\n    Returns:\n        Dict[str, Any]: A mapping from assumption names to their test outcomes (e.g., p-values or pass/fail indicators).\n\n    Edge Cases:\n        - The function assumes that the data is one-dimensional or structured in a way suitable for assumption testing.\n        - When assumptions dictionary is provided, keys must match recognized assumption tests.\n    \"\"\"\n    pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "from typing import Any",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Any",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "from typing import Union",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Union",
                                                    "lineno": 2
                                                },
                                                {
                                                    "name": "import matplotlib.pyplot as plt",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import matplotlib.pyplot as plt",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "from typing import Dict, Tuple, Union",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Dict, Tuple, Union",
                                                    "lineno": 4
                                                },
                                                {
                                                    "name": "from typing import Dict, Any",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Dict, Any",
                                                    "lineno": 5
                                                },
                                                {
                                                    "name": "import numpy as np",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "import numpy as np",
                                                    "lineno": 6
                                                },
                                                {
                                                    "name": "from typing import Tuple, Union",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Tuple, Union",
                                                    "lineno": 7
                                                },
                                                {
                                                    "name": "two_tailed_t_test",
                                                    "unit_type": "function",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def two_tailed_t_test(sample1: Union[np.ndarray, list], sample2: Union[np.ndarray, list], alpha: float=0.05) -> float:\n    \"\"\"\n    Perform a two-tailed t-test to compare the means of two independent samples.\n\n    This function calculates the t-test statistic and corresponding p-value for two samples under a two-tailed hypothesis.\n    It is used to assess whether the means of two independent groups are statistically different.\n\n    Args:\n        sample1 (np.ndarray or list): The first set of observations.\n        sample2 (np.ndarray or list): The second set of observations.\n        alpha (float, optional): The significance level to determine statistical threshold; default is 0.05.\n\n    Returns:\n        float: The p-value indicating the probability of observing the data under the null hypothesis.\n    \n    Edge Cases:\n        - Samples with very small sizes may result in low statistical power.\n        - Non-normal data distributions may affect test validity.\n    \"\"\"\n    pass",
                                                    "lineno": 9
                                                },
                                                {
                                                    "name": "one_tailed_t_test",
                                                    "unit_type": "function",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def one_tailed_t_test(sample1: Union[np.ndarray, list], sample2: Union[np.ndarray, list], tail: str='right', alpha: float=0.05) -> float:\n    \"\"\"\n    Perform a one-tailed t-test to compare the means of two independent samples.\n\n    This function calculates the t-test statistic and p-value for a one-tailed hypothesis, where the alternative hypothesis \n    specifies a direction (either 'right' indicating sample1 > sample2 or 'left' indicating sample1 < sample2).\n\n    Args:\n        sample1 (np.ndarray or list): The first set of observations.\n        sample2 (np.ndarray or list): The second set of observations.\n        tail (str, optional): Indicates the test direction; valid options are \"right\" or \"left\". Default is \"right\".\n        alpha (float, optional): The significance level; default is 0.05.\n\n    Returns:\n        float: The p-value for the one-tailed test.\n\n    Raises:\n        ValueError: If the 'tail' argument is not \"right\" or \"left\".\n\n    Edge Cases:\n        - Small sample sizes or non-normal data can impact the test's reliability.\n        - The function does not handle equal variances issues explicitly.\n    \"\"\"\n    pass",
                                                    "lineno": 30
                                                },
                                                {
                                                    "name": "paired_t_test_with_confidence_interval",
                                                    "unit_type": "function",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def paired_t_test_with_confidence_interval(before: Union[np.ndarray, list], after: Union[np.ndarray, list], confidence: float=0.95) -> Tuple[float, Tuple[float, float]]:\n    \"\"\"\n    Perform a paired t-test and calculate the confidence interval for the mean difference between paired observations.\n\n    This function is used when comparing paired samples (e.g., pre-test and post-test measurements) and needs to\n    report both the p-value and the confidence interval of the difference.\n\n    Args:\n        before (np.ndarray or list): Measurements from the first condition (e.g., before treatment).\n        after (np.ndarray or list): Measurements from the second condition (e.g., after treatment).\n        confidence (float, optional): The confidence level for the confidence interval; default is 0.95.\n\n    Returns:\n        Tuple[float, Tuple[float, float]]:\n            - float: The p-value from the paired t-test.\n            - Tuple[float, float]: The lower and upper bounds of the confidence interval for the mean difference.\n\n    Edge Cases:\n        - Requires that both inputs have the same length.\n        - Results may be influenced by outliers.\n    \"\"\"\n    pass",
                                                    "lineno": 55
                                                },
                                                {
                                                    "name": "paired_t_test_multiple_metrics",
                                                    "unit_type": "function",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def paired_t_test_multiple_metrics(metrics_data: Dict[str, Tuple[Union[np.ndarray, list], Union[np.ndarray, list]]], alpha: float=0.05) -> Dict[str, float]:\n    \"\"\"\n    Perform paired t-tests on multiple metrics simultaneously, returning p-values for each metric.\n\n    This function processes a dictionary of paired data for various metrics and computes the p-value \n    for the paired t-test for each metric. It is useful in multi-metric evaluation scenarios.\n\n    Args:\n        metrics_data (Dict[str, Tuple[np.ndarray or list, np.ndarray or list]]):\n            A dictionary where each key is a metric name and its value is a tuple containing two lists or arrays,\n            representing paired observations.\n        alpha (float, optional): The significance level for the tests; default is 0.05.\n\n    Returns:\n        Dict[str, float]: A mapping from each metric name to its corresponding p-value from the paired t-test.\n\n    Edge Cases:\n        - Each metric's paired data must have matching lengths.\n        - The function does not correct for multiple comparisons by default.\n    \"\"\"\n    pass",
                                                    "lineno": 78
                                                },
                                                {
                                                    "name": "interpret_test_results",
                                                    "unit_type": "function",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def interpret_test_results(test_statistic: float, p_value: float, alpha: float=0.05) -> str:\n    \"\"\"\n    Interpret the results of a statistical test by providing a textual explanation based on the test statistic and p-value.\n\n    This function helps in understanding the outcome of statistical tests by comparing the p-value to the significance level\n    and summarizing the result in plain language.\n\n    Args:\n        test_statistic (float): The computed test statistic.\n        p_value (float): The p-value obtained from the statistical test.\n        alpha (float, optional): The significance level used in the test; default is 0.05.\n\n    Returns:\n        str: A textual interpretation of the statistical test result detailing whether the null hypothesis is rejected.\n\n    Edge Cases:\n        - If the p-value is exactly equal to alpha, the interpretation will note the borderline significance.\n    \"\"\"\n    pass",
                                                    "lineno": 100
                                                },
                                                {
                                                    "name": "visualize_paired_t_test_results",
                                                    "unit_type": "function",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def visualize_paired_t_test_results(test_results: Any, title: str='Paired T-Test Results') -> None:\n    \"\"\"\n    Visualize the results of a paired t-test, optionally including the confidence interval and significant differences.\n\n    This function generates a plot to help illustrate the outcome of a paired t-test including elements such as \n    mean differences, confidence intervals, and individual sample points if applicable.\n\n    Args:\n        test_results (Any): A data structure containing the paired t-test results. This can include p-values, \n                            confidence intervals, and mean differences.\n        title (str, optional): The title for the plot; default is \"Paired T-Test Results\".\n\n    Returns:\n        None: The function creates a plot but does not return any data.\n\n    Edge Cases:\n        - Expects the input test_results to be in a format compatible with the plotting logic.\n    \"\"\"\n    pass",
                                                    "lineno": 120
                                                },
                                                {
                                                    "name": "test_statistical_assumptions",
                                                    "unit_type": "function",
                                                    "file_path": "./src/advanced_modeling/model_evaluation/statistical_testing.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "def test_statistical_assumptions(data: np.ndarray, assumptions: Dict[str, Any]=None) -> Dict[str, Any]:\n    \"\"\"\n    Test the statistical assumptions underlying the use of t-tests (e.g., normality, variance homogeneity).\n\n    This function evaluates whether the input data meets key assumptions required for valid t-testing.\n    It returns a dictionary with the results of these assumption tests.\n\n    Args:\n        data (np.ndarray): The dataset to be tested for normality, homogeneity of variances, and other relevant assumptions.\n        assumptions (Dict[str, Any], optional): A configuration dictionary specifying which assumption tests to perform \n                                                  and their parameters; if None, default tests are applied.\n\n    Returns:\n        Dict[str, Any]: A mapping from assumption names to their test outcomes (e.g., p-values or pass/fail indicators).\n\n    Edge Cases:\n        - The function assumes that the data is one-dimensional or structured in a way suitable for assumption testing.\n        - When assumptions dictionary is provided, keys must match recognized assumption tests.\n    \"\"\"\n    pass",
                                                    "lineno": 140
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "type": "directory",
                                    "name": "optimization",
                                    "path": "./src/advanced_modeling/optimization",
                                    "children": [
                                        {
                                            "type": "file",
                                            "name": "genetic_algorithms.py",
                                            "path": "./src/advanced_modeling/optimization/genetic_algorithms.py",
                                            "code": "from typing import List, Callable, Any, Optional\n\nclass GeneticAlgorithm:\n    \"\"\"\n    Implements a genetic algorithm incorporating elitism strategy,\n    customizable fitness function, and a crossover operation for generating new individuals.\n    \n    This class encapsulates the core operations of a genetic algorithm including:\n      - Preserving a fraction of the best individuals (elitism).\n      - Computing fitness using a custom fitness function.\n      - Generating offspring via a crossover operation.\n    \n    Attributes:\n        population (List[Any]): The current population of individuals.\n        fitness_function (Callable[[Any], float]): A callable that computes the fitness score of an individual.\n        elitism_rate (float): The fraction of top individuals to retain for the next generation.\n        mutation_rate (float): The probability of mutation for offspring.\n    \n    Methods:\n        run(generations: int) -> List[Any]:\n            Executes the genetic algorithm for a specified number of generations.\n            \n        crossover(parent1: Any, parent2: Any) -> Any:\n            Combines two parent individuals to create a new offspring.\n    \"\"\"\n\n    def __init__(self, population: List[Any], fitness_function: Callable[[Any], float], elitism_rate: float=0.1, mutation_rate: float=0.01) -> None:\n        \"\"\"\n        Initializes the genetic algorithm with the given population, fitness function, and parameters for elitism and mutation.\n        \n        Args:\n            population (List[Any]): Initial list of individuals representing the starting population.\n            fitness_function (Callable[[Any], float]): A custom function to evaluate the fitness of an individual.\n            elitism_rate (float, optional): Proportion of top individuals to be carried over unaltered to the next generation. Defaults to 0.1.\n            mutation_rate (float, optional): Probability of mutation during offspring generation. Defaults to 0.01.\n        \n        Assumptions:\n            - The population is non-empty and diverse enough for evolutionary processes.\n            - The fitness_function properly returns a numeric score indicating individual performance.\n        \"\"\"\n        self.population = population\n        self.fitness_function = fitness_function\n        self.elitism_rate = elitism_rate\n        self.mutation_rate = mutation_rate\n        pass\n\n    def run(self, generations: int) -> List[Any]:\n        \"\"\"\n        Executes the genetic algorithm over a specified number of generations.\n        \n        Incorporates elitism by retaining the top-performing individuals,\n        applies the custom fitness function to evaluate each generation,\n        and uses the crossover operation to create new offspring.\n        \n        Args:\n            generations (int): The number of generations (iterations) for which the algorithm should run.\n        \n        Returns:\n            List[Any]: The final population after completion of all generations.\n        \n        Edge Cases:\n            - If generations is less than or equal to zero, the function should either return the initial population or handle it gracefully.\n        \"\"\"\n        pass\n\n    def crossover(self, parent1: Any, parent2: Any) -> Any:\n        \"\"\"\n        Performs a crossover operation between two parent individuals to produce a new offspring.\n        \n        This function is responsible for combining genetic information from both parents,\n        potentially incorporating techniques such as one-point or two-point crossover.\n        \n        Args:\n            parent1 (Any): The first parent individual.\n            parent2 (Any): The second parent individual.\n        \n        Returns:\n            Any: A new offspring individual created by merging the attributes of the two parents.\n        \n        Assumptions:\n            - Both parents are compatible in structure such that their genetic material can be sensibly combined.\n            - The method does not enforce mutation; mutation is handled separately in the algorithm.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "from typing import List, Callable, Any, Optional",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/optimization/genetic_algorithms.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import List, Callable, Any, Optional",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "GeneticAlgorithm",
                                                    "unit_type": "class",
                                                    "file_path": "./src/advanced_modeling/optimization/genetic_algorithms.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class GeneticAlgorithm:\n    \"\"\"\n    Implements a genetic algorithm incorporating elitism strategy,\n    customizable fitness function, and a crossover operation for generating new individuals.\n    \n    This class encapsulates the core operations of a genetic algorithm including:\n      - Preserving a fraction of the best individuals (elitism).\n      - Computing fitness using a custom fitness function.\n      - Generating offspring via a crossover operation.\n    \n    Attributes:\n        population (List[Any]): The current population of individuals.\n        fitness_function (Callable[[Any], float]): A callable that computes the fitness score of an individual.\n        elitism_rate (float): The fraction of top individuals to retain for the next generation.\n        mutation_rate (float): The probability of mutation for offspring.\n    \n    Methods:\n        run(generations: int) -> List[Any]:\n            Executes the genetic algorithm for a specified number of generations.\n            \n        crossover(parent1: Any, parent2: Any) -> Any:\n            Combines two parent individuals to create a new offspring.\n    \"\"\"\n\n    def __init__(self, population: List[Any], fitness_function: Callable[[Any], float], elitism_rate: float=0.1, mutation_rate: float=0.01) -> None:\n        \"\"\"\n        Initializes the genetic algorithm with the given population, fitness function, and parameters for elitism and mutation.\n        \n        Args:\n            population (List[Any]): Initial list of individuals representing the starting population.\n            fitness_function (Callable[[Any], float]): A custom function to evaluate the fitness of an individual.\n            elitism_rate (float, optional): Proportion of top individuals to be carried over unaltered to the next generation. Defaults to 0.1.\n            mutation_rate (float, optional): Probability of mutation during offspring generation. Defaults to 0.01.\n        \n        Assumptions:\n            - The population is non-empty and diverse enough for evolutionary processes.\n            - The fitness_function properly returns a numeric score indicating individual performance.\n        \"\"\"\n        self.population = population\n        self.fitness_function = fitness_function\n        self.elitism_rate = elitism_rate\n        self.mutation_rate = mutation_rate\n        pass\n\n    def run(self, generations: int) -> List[Any]:\n        \"\"\"\n        Executes the genetic algorithm over a specified number of generations.\n        \n        Incorporates elitism by retaining the top-performing individuals,\n        applies the custom fitness function to evaluate each generation,\n        and uses the crossover operation to create new offspring.\n        \n        Args:\n            generations (int): The number of generations (iterations) for which the algorithm should run.\n        \n        Returns:\n            List[Any]: The final population after completion of all generations.\n        \n        Edge Cases:\n            - If generations is less than or equal to zero, the function should either return the initial population or handle it gracefully.\n        \"\"\"\n        pass\n\n    def crossover(self, parent1: Any, parent2: Any) -> Any:\n        \"\"\"\n        Performs a crossover operation between two parent individuals to produce a new offspring.\n        \n        This function is responsible for combining genetic information from both parents,\n        potentially incorporating techniques such as one-point or two-point crossover.\n        \n        Args:\n            parent1 (Any): The first parent individual.\n            parent2 (Any): The second parent individual.\n        \n        Returns:\n            Any: A new offspring individual created by merging the attributes of the two parents.\n        \n        Assumptions:\n            - Both parents are compatible in structure such that their genetic material can be sensibly combined.\n            - The method does not enforce mutation; mutation is handled separately in the algorithm.\n        \"\"\"\n        pass",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/advanced_modeling/optimization/genetic_algorithms.py",
                                                    "parent": "GeneticAlgorithm",
                                                    "extra": {},
                                                    "code": "def __init__(self, population: List[Any], fitness_function: Callable[[Any], float], elitism_rate: float=0.1, mutation_rate: float=0.01) -> None:\n    \"\"\"\n        Initializes the genetic algorithm with the given population, fitness function, and parameters for elitism and mutation.\n        \n        Args:\n            population (List[Any]): Initial list of individuals representing the starting population.\n            fitness_function (Callable[[Any], float]): A custom function to evaluate the fitness of an individual.\n            elitism_rate (float, optional): Proportion of top individuals to be carried over unaltered to the next generation. Defaults to 0.1.\n            mutation_rate (float, optional): Probability of mutation during offspring generation. Defaults to 0.01.\n        \n        Assumptions:\n            - The population is non-empty and diverse enough for evolutionary processes.\n            - The fitness_function properly returns a numeric score indicating individual performance.\n        \"\"\"\n    self.population = population\n    self.fitness_function = fitness_function\n    self.elitism_rate = elitism_rate\n    self.mutation_rate = mutation_rate\n    pass",
                                                    "lineno": 27
                                                },
                                                {
                                                    "name": "run",
                                                    "unit_type": "method",
                                                    "file_path": "./src/advanced_modeling/optimization/genetic_algorithms.py",
                                                    "parent": "GeneticAlgorithm",
                                                    "extra": {},
                                                    "code": "def run(self, generations: int) -> List[Any]:\n    \"\"\"\n        Executes the genetic algorithm over a specified number of generations.\n        \n        Incorporates elitism by retaining the top-performing individuals,\n        applies the custom fitness function to evaluate each generation,\n        and uses the crossover operation to create new offspring.\n        \n        Args:\n            generations (int): The number of generations (iterations) for which the algorithm should run.\n        \n        Returns:\n            List[Any]: The final population after completion of all generations.\n        \n        Edge Cases:\n            - If generations is less than or equal to zero, the function should either return the initial population or handle it gracefully.\n        \"\"\"\n    pass",
                                                    "lineno": 47
                                                },
                                                {
                                                    "name": "crossover",
                                                    "unit_type": "method",
                                                    "file_path": "./src/advanced_modeling/optimization/genetic_algorithms.py",
                                                    "parent": "GeneticAlgorithm",
                                                    "extra": {},
                                                    "code": "def crossover(self, parent1: Any, parent2: Any) -> Any:\n    \"\"\"\n        Performs a crossover operation between two parent individuals to produce a new offspring.\n        \n        This function is responsible for combining genetic information from both parents,\n        potentially incorporating techniques such as one-point or two-point crossover.\n        \n        Args:\n            parent1 (Any): The first parent individual.\n            parent2 (Any): The second parent individual.\n        \n        Returns:\n            Any: A new offspring individual created by merging the attributes of the two parents.\n        \n        Assumptions:\n            - Both parents are compatible in structure such that their genetic material can be sensibly combined.\n            - The method does not enforce mutation; mutation is handled separately in the algorithm.\n        \"\"\"\n    pass",
                                                    "lineno": 66
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "hyperparameter.py",
                                            "path": "./src/advanced_modeling/optimization/hyperparameter.py",
                                            "code": "from typing import Any, Callable, Dict, List\n\nclass HyperbandOptimizer:\n    \"\"\"\n    Interface for hyperparameter optimization using the Hyperband tuning strategy.\n\n    HyperbandOptimizer implements the Hyperband algorithm to efficiently search a\n    large hyperparameter space. It dynamically allocates resources to promising\n    configurations while discarding poor performers early using successive halving.\n\n    Usage:\n        1. Instantiate the optimizer with the maximum iteration count, a downsampling factor,\n           and a user-defined evaluation function.\n        2. Call the 'tune' method with a list of hyperparameter configurations.\n        3. Retrieve the best found configuration according to the given evaluation function.\n\n    Args:\n        max_iter (int): The maximum resource allocation (e.g., maximum iterations or epochs)\n            for the tuning process.\n        eta (int): The reduction factor; determines the proportion of configurations discarded\n            in each round (typically, eta > 1).\n        evaluation_fn (Callable[[Dict[str, Any], int], float]): A callable function that evaluates\n            each hyperparameter configuration with a given allocated resource. It accepts a configuration\n            dictionary and an integer resource allocation, returning a performance metric (e.g., loss).\n\n    Methods:\n        tune(configurations: List[Dict[str, Any]]) -> Dict[str, Any]:\n            Executes the Hyperband algorithm on a list of hyperparameter configurations and returns\n            the configuration that achieved the best performance metric.\n    \"\"\"\n\n    def __init__(self, max_iter: int, eta: int, evaluation_fn: Callable[[Dict[str, Any], int], float]) -> None:\n        \"\"\"\n        Initialize the HyperbandOptimizer with tuning parameters.\n\n        Args:\n            max_iter (int): Maximum resource (iterations/epochs) available for hyperparameter tuning.\n            eta (int): Downsampling factor for successive halving; number of configurations to retain in each round.\n            evaluation_fn (Callable[[Dict[str, Any], int], float]): Function to evaluate a hyperparameter\n                configuration with a given resource allocation.\n        \"\"\"\n        pass\n\n    def tune(self, configurations: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Run the Hyperband algorithm to select the best hyperparameter configuration.\n\n        This method evaluates the provided hyperparameter configurations using the Hyperband\n        strategy. It allocates resources dynamically and prunes configurations based on their\n        performance, ultimately returning the best configuration.\n\n        Args:\n            configurations (List[Dict[str, Any]]): A list of hyperparameter configurations where\n                each configuration is represented as a dictionary mapping parameter names to values.\n\n        Returns:\n            Dict[str, Any]: The best hyperparameter configuration determined by the evaluation function.\n\n        Raises:\n            ValueError: If the provided configurations list is empty.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "from typing import Any, Callable, Dict, List",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/optimization/hyperparameter.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Any, Callable, Dict, List",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "HyperbandOptimizer",
                                                    "unit_type": "class",
                                                    "file_path": "./src/advanced_modeling/optimization/hyperparameter.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class HyperbandOptimizer:\n    \"\"\"\n    Interface for hyperparameter optimization using the Hyperband tuning strategy.\n\n    HyperbandOptimizer implements the Hyperband algorithm to efficiently search a\n    large hyperparameter space. It dynamically allocates resources to promising\n    configurations while discarding poor performers early using successive halving.\n\n    Usage:\n        1. Instantiate the optimizer with the maximum iteration count, a downsampling factor,\n           and a user-defined evaluation function.\n        2. Call the 'tune' method with a list of hyperparameter configurations.\n        3. Retrieve the best found configuration according to the given evaluation function.\n\n    Args:\n        max_iter (int): The maximum resource allocation (e.g., maximum iterations or epochs)\n            for the tuning process.\n        eta (int): The reduction factor; determines the proportion of configurations discarded\n            in each round (typically, eta > 1).\n        evaluation_fn (Callable[[Dict[str, Any], int], float]): A callable function that evaluates\n            each hyperparameter configuration with a given allocated resource. It accepts a configuration\n            dictionary and an integer resource allocation, returning a performance metric (e.g., loss).\n\n    Methods:\n        tune(configurations: List[Dict[str, Any]]) -> Dict[str, Any]:\n            Executes the Hyperband algorithm on a list of hyperparameter configurations and returns\n            the configuration that achieved the best performance metric.\n    \"\"\"\n\n    def __init__(self, max_iter: int, eta: int, evaluation_fn: Callable[[Dict[str, Any], int], float]) -> None:\n        \"\"\"\n        Initialize the HyperbandOptimizer with tuning parameters.\n\n        Args:\n            max_iter (int): Maximum resource (iterations/epochs) available for hyperparameter tuning.\n            eta (int): Downsampling factor for successive halving; number of configurations to retain in each round.\n            evaluation_fn (Callable[[Dict[str, Any], int], float]): Function to evaluate a hyperparameter\n                configuration with a given resource allocation.\n        \"\"\"\n        pass\n\n    def tune(self, configurations: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Run the Hyperband algorithm to select the best hyperparameter configuration.\n\n        This method evaluates the provided hyperparameter configurations using the Hyperband\n        strategy. It allocates resources dynamically and prunes configurations based on their\n        performance, ultimately returning the best configuration.\n\n        Args:\n            configurations (List[Dict[str, Any]]): A list of hyperparameter configurations where\n                each configuration is represented as a dictionary mapping parameter names to values.\n\n        Returns:\n            Dict[str, Any]: The best hyperparameter configuration determined by the evaluation function.\n\n        Raises:\n            ValueError: If the provided configurations list is empty.\n        \"\"\"\n        pass",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/advanced_modeling/optimization/hyperparameter.py",
                                                    "parent": "HyperbandOptimizer",
                                                    "extra": {},
                                                    "code": "def __init__(self, max_iter: int, eta: int, evaluation_fn: Callable[[Dict[str, Any], int], float]) -> None:\n    \"\"\"\n        Initialize the HyperbandOptimizer with tuning parameters.\n\n        Args:\n            max_iter (int): Maximum resource (iterations/epochs) available for hyperparameter tuning.\n            eta (int): Downsampling factor for successive halving; number of configurations to retain in each round.\n            evaluation_fn (Callable[[Dict[str, Any], int], float]): Function to evaluate a hyperparameter\n                configuration with a given resource allocation.\n        \"\"\"\n    pass",
                                                    "lineno": 32
                                                },
                                                {
                                                    "name": "tune",
                                                    "unit_type": "method",
                                                    "file_path": "./src/advanced_modeling/optimization/hyperparameter.py",
                                                    "parent": "HyperbandOptimizer",
                                                    "extra": {},
                                                    "code": "def tune(self, configurations: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"\n        Run the Hyperband algorithm to select the best hyperparameter configuration.\n\n        This method evaluates the provided hyperparameter configurations using the Hyperband\n        strategy. It allocates resources dynamically and prunes configurations based on their\n        performance, ultimately returning the best configuration.\n\n        Args:\n            configurations (List[Dict[str, Any]]): A list of hyperparameter configurations where\n                each configuration is represented as a dictionary mapping parameter names to values.\n\n        Returns:\n            Dict[str, Any]: The best hyperparameter configuration determined by the evaluation function.\n\n        Raises:\n            ValueError: If the provided configurations list is empty.\n        \"\"\"\n    pass",
                                                    "lineno": 44
                                                }
                                            ]
                                        },
                                        {
                                            "type": "file",
                                            "name": "meta_learning.py",
                                            "path": "./src/advanced_modeling/optimization/meta_learning.py",
                                            "code": "from typing import Any, List\n\nclass MetaLearningModel:\n    \"\"\"\n    A meta-learning model that integrates multiple base learners using a meta-learner.\n    \n    This class implements a meta-learning approach, in which predictions from multiple base \n    models are combined by a meta-learner to improve overall performance. It encapsulates the \n    functionality for training and prediction in ensemble strategies, specifically within the \n    meta-learning paradigm.\n    \n    Attributes:\n        base_learners (List[Any]): A list of trained base learning models.\n        meta_learner (Any): A model that aggregates the predictions from the base learners.\n    \"\"\"\n\n    def __init__(self, base_learners: List[Any], meta_learner: Any) -> None:\n        \"\"\"\n        Initialize the meta-learning model with the given base learners and meta-learner.\n        \n        Args:\n            base_learners (List[Any]): A list of pre-configured base model instances.\n            meta_learner (Any): A meta-model responsible for combining base learners' outputs.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def fit(self, X: Any, y: Any, **kwargs) -> 'MetaLearningModel':\n        \"\"\"\n        Train the meta-learning model on the provided dataset.\n        \n        This method fits both the base learners and the meta-learner, based on the input \n        data and targets, so that the meta-learner can learn to integrate predictions effectively.\n        \n        Args:\n            X (Any): The feature set used for training (e.g., pd.DataFrame or np.ndarray).\n            y (Any): The target variable used for training (e.g., pd.Series or np.ndarray).\n            **kwargs: Additional keyword arguments for configuring the training process.\n        \n        Returns:\n            MetaLearningModel: The fitted meta-learning model instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: Any) -> Any:\n        \"\"\"\n        Generate predictions using the trained meta-learning model.\n        \n        This method aggregates predictions from base learners and applies the meta-learner to produce\n        a final output.\n        \n        Args:\n            X (Any): Input data for which predictions are to be generated (e.g., pd.DataFrame or np.ndarray).\n        \n        Returns:\n            Any: The aggregated predictions from the meta-learning model.\n        \"\"\"\n        pass\n",
                                            "feature_paths": [],
                                            "units": [
                                                {
                                                    "name": "from typing import Any, List",
                                                    "unit_type": "import",
                                                    "file_path": "./src/advanced_modeling/optimization/meta_learning.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "from typing import Any, List",
                                                    "lineno": 1
                                                },
                                                {
                                                    "name": "MetaLearningModel",
                                                    "unit_type": "class",
                                                    "file_path": "./src/advanced_modeling/optimization/meta_learning.py",
                                                    "parent": null,
                                                    "extra": {},
                                                    "code": "class MetaLearningModel:\n    \"\"\"\n    A meta-learning model that integrates multiple base learners using a meta-learner.\n    \n    This class implements a meta-learning approach, in which predictions from multiple base \n    models are combined by a meta-learner to improve overall performance. It encapsulates the \n    functionality for training and prediction in ensemble strategies, specifically within the \n    meta-learning paradigm.\n    \n    Attributes:\n        base_learners (List[Any]): A list of trained base learning models.\n        meta_learner (Any): A model that aggregates the predictions from the base learners.\n    \"\"\"\n\n    def __init__(self, base_learners: List[Any], meta_learner: Any) -> None:\n        \"\"\"\n        Initialize the meta-learning model with the given base learners and meta-learner.\n        \n        Args:\n            base_learners (List[Any]): A list of pre-configured base model instances.\n            meta_learner (Any): A meta-model responsible for combining base learners' outputs.\n        \n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def fit(self, X: Any, y: Any, **kwargs) -> 'MetaLearningModel':\n        \"\"\"\n        Train the meta-learning model on the provided dataset.\n        \n        This method fits both the base learners and the meta-learner, based on the input \n        data and targets, so that the meta-learner can learn to integrate predictions effectively.\n        \n        Args:\n            X (Any): The feature set used for training (e.g., pd.DataFrame or np.ndarray).\n            y (Any): The target variable used for training (e.g., pd.Series or np.ndarray).\n            **kwargs: Additional keyword arguments for configuring the training process.\n        \n        Returns:\n            MetaLearningModel: The fitted meta-learning model instance.\n        \"\"\"\n        pass\n\n    def predict(self, X: Any) -> Any:\n        \"\"\"\n        Generate predictions using the trained meta-learning model.\n        \n        This method aggregates predictions from base learners and applies the meta-learner to produce\n        a final output.\n        \n        Args:\n            X (Any): Input data for which predictions are to be generated (e.g., pd.DataFrame or np.ndarray).\n        \n        Returns:\n            Any: The aggregated predictions from the meta-learning model.\n        \"\"\"\n        pass",
                                                    "lineno": 3
                                                },
                                                {
                                                    "name": "__init__",
                                                    "unit_type": "method",
                                                    "file_path": "./src/advanced_modeling/optimization/meta_learning.py",
                                                    "parent": "MetaLearningModel",
                                                    "extra": {},
                                                    "code": "def __init__(self, base_learners: List[Any], meta_learner: Any) -> None:\n    \"\"\"\n        Initialize the meta-learning model with the given base learners and meta-learner.\n        \n        Args:\n            base_learners (List[Any]): A list of pre-configured base model instances.\n            meta_learner (Any): A meta-model responsible for combining base learners' outputs.\n        \n        Returns:\n            None\n        \"\"\"\n    pass",
                                                    "lineno": 17
                                                },
                                                {
                                                    "name": "fit",
                                                    "unit_type": "method",
                                                    "file_path": "./src/advanced_modeling/optimization/meta_learning.py",
                                                    "parent": "MetaLearningModel",
                                                    "extra": {},
                                                    "code": "def fit(self, X: Any, y: Any, **kwargs) -> 'MetaLearningModel':\n    \"\"\"\n        Train the meta-learning model on the provided dataset.\n        \n        This method fits both the base learners and the meta-learner, based on the input \n        data and targets, so that the meta-learner can learn to integrate predictions effectively.\n        \n        Args:\n            X (Any): The feature set used for training (e.g., pd.DataFrame or np.ndarray).\n            y (Any): The target variable used for training (e.g., pd.Series or np.ndarray).\n            **kwargs: Additional keyword arguments for configuring the training process.\n        \n        Returns:\n            MetaLearningModel: The fitted meta-learning model instance.\n        \"\"\"\n    pass",
                                                    "lineno": 30
                                                },
                                                {
                                                    "name": "predict",
                                                    "unit_type": "method",
                                                    "file_path": "./src/advanced_modeling/optimization/meta_learning.py",
                                                    "parent": "MetaLearningModel",
                                                    "extra": {},
                                                    "code": "def predict(self, X: Any) -> Any:\n    \"\"\"\n        Generate predictions using the trained meta-learning model.\n        \n        This method aggregates predictions from base learners and applies the meta-learner to produce\n        a final output.\n        \n        Args:\n            X (Any): Input data for which predictions are to be generated (e.g., pd.DataFrame or np.ndarray).\n        \n        Returns:\n            Any: The aggregated predictions from the meta-learning model.\n        \"\"\"\n    pass",
                                                    "lineno": 47
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        }
    }
}
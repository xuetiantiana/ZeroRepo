{
    "metadata": {
        "Algorithms": [
            {
                "node": "Optimization Techniques",
                "feature_path": "Algorithms/Optimization Techniques",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/optimization/gradient_descent"
                    }
                ]
            },
            {
                "node": "Gradient Descent Methods",
                "feature_path": "Algorithms/Optimization Techniques/Gradient Descent Methods",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/optimization/gradient_descent"
                    }
                ]
            },
            {
                "node": "Adaptive & Batch Methods",
                "feature_path": "Algorithms/Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/optimization/gradient_descent/adaptive_batch.py"
                    }
                ]
            },
            {
                "node": "adagrad",
                "feature_path": "Algorithms/Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/adagrad",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "AdagradOptimizer",
                        "extra": {
                            "docstring": "Optimizer implementing the Adagrad algorithm which adapts learning rate for each parameter.\n\nThis optimizer adjusts the learning rate for each parameter based on the history of gradients.\nIt is typically used when dealing with sparse data or scenarios where different parameters\nrequire different updates.\n\nArgs:\n    learning_rate (float): The initial step size for gradient updates.\n    epsilon (float): A small constant to prevent division by zero, typically 1e-8.\n\nMethods:\n    step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        Updates the parameters using the Adagrad algorithm.\n        \n        Args:\n            params (np.ndarray): Current parameters to be updated.\n            grads (np.ndarray): Gradients computed from the loss function.\n        \n        Returns:\n            np.ndarray: Updated parameters.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "learning_rate",
                                        "epsilon"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "step",
                                    "args": [
                                        "self",
                                        "params",
                                        "grads"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": null
                                }
                            ]
                        },
                        "path": "src/algorithms/optimization/gradient_descent/adaptive_batch.py"
                    }
                ]
            },
            {
                "node": "adam optimizer",
                "feature_path": "Algorithms/Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/adam optimizer",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "AdamOptimizer",
                        "extra": {
                            "docstring": "Optimizer implementing the Adam algorithm, which combines adaptive learning rates and momentum.\n\nAdam maintains exponential moving averages of both the gradient and its square, and it\nincludes bias-correction terms. It is widely used for training deep learning models due\nto its computational efficiency and robust performance on large datasets.\n\nArgs:\n    learning_rate (float): The initial step size for parameter updates.\n    beta1 (float): Exponential decay rate for the first moment estimates.\n    beta2 (float): Exponential decay rate for the second moment estimates.\n    epsilon (float): A small constant to prevent division by zero (usually around 1e-8).\n\nMethods:\n    step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        Performs an update on the parameters using the Adam optimization algorithm.\n        \n        Args:\n            params (np.ndarray): Current parameters as a numpy array.\n            grads (np.ndarray): Gradients calculated from the loss function.\n        \n        Returns:\n            np.ndarray: Updated parameters after the Adam step.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "learning_rate",
                                        "beta1",
                                        "beta2",
                                        "epsilon"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "step",
                                    "args": [
                                        "self",
                                        "params",
                                        "grads"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": null
                                }
                            ]
                        },
                        "path": "src/algorithms/optimization/gradient_descent/adaptive_batch.py"
                    }
                ]
            },
            {
                "node": "gd with line search",
                "feature_path": "Algorithms/Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/gd with line search",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "GDWithLineSearch",
                        "extra": {
                            "docstring": "Optimizer that uses gradient descent augmented with line search.\n\nThis method performs standard gradient descent updates while dynamically determining\nan optimal learning rate through a line search strategy. The line search procedure iteratively\nselects a step size that sufficiently decreases the loss function.\n\nArgs:\n    initial_learning_rate (float): The starting step size before line search adjustment.\n    line_search_fn (Callable[[np.ndarray, np.ndarray], float]): A callable that computes the optimal \n        step size given the current parameters and gradient. This function should accept the\n        current parameter vector and gradient and return a suitable learning rate.\n\nMethods:\n    step(params: np.ndarray, grads: np.ndarray, loss_fn: Callable[[np.ndarray], float]) -> np.ndarray:\n        Updates the parameters using gradient descent with a dynamically determined step size.\n        \n        Args:\n            params (np.ndarray): Current model parameters.\n            grads (np.ndarray): Gradients computed for the current parameters.\n            loss_fn (Callable[[np.ndarray], float]): The loss function to evaluate parameter updates.\n        \n        Returns:\n            np.ndarray: Updated parameters after applying the line search-based gradient descent step.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "initial_learning_rate",
                                        "line_search_fn"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "step",
                                    "args": [
                                        "self",
                                        "params",
                                        "grads",
                                        "loss_fn"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": null
                                }
                            ]
                        },
                        "path": "src/algorithms/optimization/gradient_descent/adaptive_batch.py"
                    }
                ]
            },
            {
                "node": "nesterov accelerated gd",
                "feature_path": "Algorithms/Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/nesterov accelerated gd",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "NesterovAcceleratedGradient",
                        "extra": {
                            "docstring": "Optimizer implementing Nesterov Accelerated Gradient (NAG) method.\n\nThis optimizer improves upon the standard momentum method by computing the gradient \nat the approximate future position of the parameters, leading to faster convergence.\n\nArgs:\n    learning_rate (float): The step size for updating parameters.\n    momentum (float): The momentum factor (typically between 0 and 1).\n\nMethods:\n    step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        Perform a parameter update using the Nesterov accelerated gradient method.\n        \n        Args:\n            params (np.ndarray): The current model parameters.\n            grads (np.ndarray): The computed gradients based on the lookahead position.\n        \n        Returns:\n            np.ndarray: Updated parameters after applying the NAG update.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "learning_rate",
                                        "momentum"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "step",
                                    "args": [
                                        "self",
                                        "params",
                                        "grads"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": null
                                }
                            ]
                        },
                        "path": "src/algorithms/optimization/gradient_descent/adaptive_batch.py"
                    }
                ]
            },
            {
                "node": "rmsprop",
                "feature_path": "Algorithms/Optimization Techniques/Gradient Descent Methods/Adaptive & Batch Methods/rmsprop",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "RMSPropOptimizer",
                        "extra": {
                            "docstring": "Optimizer implementing the RMSProp algorithm.\n\nRMSProp adapts the learning rate for each parameter by dividing the learning rate\nby a running average of the magnitudes of recent gradients, which helps with faster and \nmore stable convergence.\n\nArgs:\n    learning_rate (float): Initial learning rate for the optimizer.\n    decay_rate (float): Decay rate for the moving average of squared gradients.\n    epsilon (float): A small constant to prevent division by zero, typically set to 1e-8.\n\nMethods:\n    step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        Updates parameters using RMSProp optimization.\n        \n        Args:\n            params (np.ndarray): Array of current parameters.\n            grads (np.ndarray): Array of gradients computed from the loss.\n        \n        Returns:\n            np.ndarray: Updated parameters after applying the RMSProp step.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "learning_rate",
                                        "decay_rate",
                                        "epsilon"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "step",
                                    "args": [
                                        "self",
                                        "params",
                                        "grads"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": null
                                }
                            ]
                        },
                        "path": "src/algorithms/optimization/gradient_descent/adaptive_batch.py"
                    }
                ]
            },
            {
                "node": "Stochastic Methods",
                "feature_path": "Algorithms/Optimization Techniques/Gradient Descent Methods/Stochastic Methods",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/optimization/gradient_descent/stochastic.py"
                    }
                ]
            },
            {
                "node": "mini-batch sgd",
                "feature_path": "Algorithms/Optimization Techniques/Gradient Descent Methods/Stochastic Methods/mini-batch sgd",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "MiniBatchSGD",
                        "extra": {
                            "docstring": "Implements Mini-Batch Stochastic Gradient Descent.\n\nThis optimizer updates model parameters using mini-batch gradient descent, which combines \nthe advantages of both stochastic and batch gradient descent. It processes a subset of the \ntraining data (mini-batch) to compute the gradients for each update.\n\nAttributes:\n    learning_rate (float): The step size used for each parameter update.\n    batch_size (int): The number of training examples used to compute a single gradient update.\n\nMethods:\n    step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        Performs an optimization step using the computed mini-batch gradients.\n\nArgs:\n    learning_rate (float): The learning rate for parameter updates.\n    batch_size (int): The number of samples in each mini-batch.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "learning_rate",
                                        "batch_size"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "step",
                                    "args": [
                                        "self",
                                        "params",
                                        "grads"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Perform a single optimization step using mini-batch SGD.\n\nArgs:\n    params (np.ndarray): The current model parameters.\n    grads (np.ndarray): The gradients computed from a mini-batch.\n\nReturns:\n    np.ndarray: Updated model parameters after applying the mini-batch SGD step.\n\nAssumptions:\n    - The gradients are computed from a mini-batch of the dataset.\n    - The shapes of params and grads are identical."
                                }
                            ]
                        },
                        "path": "src/algorithms/optimization/gradient_descent/stochastic.py"
                    }
                ]
            },
            {
                "node": "sgd with momentum",
                "feature_path": "Algorithms/Optimization Techniques/Gradient Descent Methods/Stochastic Methods/sgd with momentum",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "SGDWithMomentum",
                        "extra": {
                            "docstring": "Implements Stochastic Gradient Descent with Momentum.\n\nThis optimizer updates model parameters using a stochastic gradient descent approach\nenhanced with momentum to accelerate convergence and dampen oscillations.\n\nAttributes:\n    learning_rate (float): The step size used for each parameter update.\n    momentum (float): The momentum factor, typically between 0 and 1, that determines\n                      the contribution of past gradients.\n    velocity (np.ndarray): The accumulated velocity vector, initialized during the first call to step.\n\nMethods:\n    step(params: np.ndarray, grads: np.ndarray) -> np.ndarray:\n        Computes the update for parameters based on gradients and momentum.\n\nArgs:\n    learning_rate (float): The learning rate for parameter updates.\n    momentum (float): Momentum coefficient for smoothing updates.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "learning_rate",
                                        "momentum"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "step",
                                    "args": [
                                        "self",
                                        "params",
                                        "grads"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Perform a single optimization step using SGD with momentum.\n\nArgs:\n    params (np.ndarray): The current model parameters.\n    grads (np.ndarray): The gradients computed for the parameters.\n    \nReturns:\n    np.ndarray: Updated model parameters after applying momentum-based SGD step.\n    \nEdge Cases:\n    - If velocity is uninitialized, it should be set to zeros with the same shape as params.\n    - The method assumes that params and grads have identical shapes."
                                }
                            ]
                        },
                        "path": "src/algorithms/optimization/gradient_descent/stochastic.py"
                    }
                ]
            },
            {
                "node": "SVM Optimization",
                "feature_path": "Algorithms/Optimization Techniques/SVM Optimization",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/optimization"
                    }
                ]
            },
            {
                "node": "Optimization Algorithms",
                "feature_path": "Algorithms/Optimization Techniques/SVM Optimization/Optimization Algorithms",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/optimization/svm_optimization.py"
                    }
                ]
            },
            {
                "node": "smo algorithm",
                "feature_path": "Algorithms/Optimization Techniques/SVM Optimization/Optimization Algorithms/smo algorithm",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "SMOSVMOptimizer",
                        "extra": {
                            "docstring": "Optimizer for SVM classification using the Sequential Minimal Optimization (SMO) algorithm.\n\nThis class provides an interface for optimizing SVM quadratic programming problems\nby applying the SMO algorithm. It incrementally adjusts the Lagrange multipliers to\nfind the optimal decision boundary. The implementation is designed to ensure efficient\nhandling of constraints and calculation of the optimum parameters.\n\nAttributes:\n    C (float): Regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n    tolerance (float): Tolerance for the optimization to determine convergence.\n    max_iter (int): Maximum number of iterations to attempt optimization.\n\nMethods:\n    optimize(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        Executes the SMO algorithm on the provided training data to compute the optimal\n        support vectors and corresponding model parameters.\n\nArgs:\n    C (float): The regularization parameter for SVM.\n    tolerance (float): A small value to detect convergence of the algorithm.\n    max_iter (int): The limit on the number of iterations for the optimization process.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "C",
                                        "tolerance",
                                        "max_iter"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "optimize",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Optimize the SVM model parameters using the SMO algorithm.\n\nArgs:\n    X (np.ndarray): A 2D numpy array containing the input features.\n    y (np.ndarray): A 1D numpy array of target labels corresponding to X.\n\nReturns:\n    np.ndarray: The vector of optimized parameters (e.g., Lagrange multipliers or weight vector)\n    resulting from the SMO optimization process."
                                }
                            ]
                        },
                        "path": "src/algorithms/optimization/svm_optimization.py"
                    }
                ]
            },
            {
                "node": "stochastic gradient descent",
                "feature_path": "Algorithms/Optimization Techniques/SVM Optimization/Optimization Algorithms/stochastic gradient descent",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "SVMStochasticGradientDescentOptimizer",
                        "extra": {
                            "docstring": "Optimizer for SVM classification using stochastic gradient descent.\n\nThis class implements the optimization of the SVM objective function using\nstochastic gradient descent (SGD). It is designed to handle large datasets by\niteratively updating the model weights based on mini-batches or single samples.\n\nAttributes:\n    learning_rate (float): The step size used for each update.\n    max_iter (int): The maximum number of iterations to run the optimizer.\n    tolerance (float): The threshold for stopping criterion based on weight change.\n\nMethods:\n    optimize(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        Performs the optimization to determine the optimal parameters (weights)\n        for the SVM model given the training data and labels.\n\nArgs:\n    learning_rate (float): Learning rate for the SGD updates.\n    max_iter (int): Maximum number of iterations to perform.\n    tolerance (float): Tolerance for convergence; if updates fall below this value,\n        optimization stops.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "learning_rate",
                                        "max_iter",
                                        "tolerance"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "optimize",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Optimize the SVM objective function using stochastic gradient descent.\n\nProcesses the feature matrix X and associated labels y, iteratively updating\nthe model's weights to minimize the SVM loss function.\n\nArgs:\n    X (np.ndarray): A 2D array representing the input features.\n    y (np.ndarray): A 1D array of target labels corresponding to X.\n\nReturns:\n    np.ndarray: The optimized weight vector after convergence or after\n    reaching the maximum number of iterations."
                                }
                            ]
                        },
                        "path": "src/algorithms/optimization/svm_optimization.py"
                    }
                ]
            },
            {
                "node": "Supervised Learning",
                "feature_path": "Algorithms/Supervised Learning",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/supervised"
                    }
                ]
            },
            {
                "node": "Additional Supervised Methods",
                "feature_path": "Algorithms/Supervised Learning/Additional Supervised Methods",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/supervised"
                    }
                ]
            },
            {
                "node": "Miscellaneous",
                "feature_path": "Algorithms/Supervised Learning/Additional Supervised Methods/Miscellaneous",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/supervised/unclassified_method.py"
                    }
                ]
            },
            {
                "node": "5",
                "feature_path": "Algorithms/Supervised Learning/Additional Supervised Methods/Miscellaneous/5",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "UnclassifiedMethod",
                        "extra": {
                            "docstring": "A placeholder class for implementing a miscellaneous supervised learning method (feature '5').\n\nThis class serves as an unclassified algorithm for supervised learning that does not\nfit into traditional algorithm categories. It implements the basic estimator interface\nfrom BaseAlgorithm, including methods for fitting to training data and making predictions.\nThe specific logic for this method should be provided by further implementation.\n\nMethods:\n    __init__(*args, **kwargs):\n        Initializes the algorithm with optional parameters.\n    \n    fit(X: pd.DataFrame, y: pd.Series) -> 'UnclassifiedMethod':\n        Trains the algorithm on the given training data and labels.\n    \n    predict(X: pd.DataFrame) -> pd.Series:\n        Predicts target values for the given input features.\n\nArgs:\n    *args: Variable length argument list for initialization parameters.\n    **kwargs: Arbitrary keyword arguments for configuration.\n\nReturns:\n    UnclassifiedMethod: An instance of the fitted algorithm after calling fit.\n\nRaises:\n    NotImplementedError: The methods are not implemented and will raise an error if called.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": null,
                                    "docstring": "Initialize the UnclassifiedMethod algorithm with optional parameters.\n\nArgs:\n    *args: Variable length argument list.\n    **kwargs: Arbitrary keyword arguments for algorithm configuration."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'UnclassifiedMethod'",
                                    "docstring": "Fit the unclassified method on the training data.\n\nArgs:\n    X (pd.DataFrame): Training data features.\n    y (pd.Series): True target values for training.\n\nReturns:\n    UnclassifiedMethod: The fitted model instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Generate predictions from the fitted unclassified method.\n\nArgs:\n    X (pd.DataFrame): Input data features for which predictions are to be made.\n\nReturns:\n    pd.Series: Predicted target values."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/unclassified_method.py"
                    }
                ]
            },
            {
                "node": "Classification Algorithms",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/supervised/classification"
                    }
                ]
            },
            {
                "node": "Decision Trees",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Decision Trees",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/supervised/classification/decision_trees.py"
                    }
                ]
            },
            {
                "node": "decision trees",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Decision Trees/decision trees",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "DecisionTreeClassifier",
                        "extra": {
                            "docstring": "DecisionTreeClassifier implements a decision tree based classifier that partitions the feature space\nand assigns class labels based on learned decision rules.\n\nThis classifier recursively splits the input feature space based on criteria that maximize the\nseparation between classes. It is designed to handle both categorical and numerical features.\n\nArgs:\n    max_depth (int, optional): The maximum depth of the tree. If None, the tree is expanded until leaves are pure.\n    min_samples_split (int): The minimum number of samples required to split an internal node.\n\nMethods:\n    fit(pd.DataFrame, pd.Series) -> DecisionTreeClassifier:\n        Trains the decision tree classifier using the provided training data and corresponding labels.\n    predict(pd.DataFrame) -> pd.Series:\n        Predicts class labels for the given input data based on the learned decision tree.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "max_depth",
                                        "min_samples_split"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the DecisionTreeClassifier with specific hyperparameters.\n\nArgs:\n    max_depth (int, optional): Maximum depth that the tree can grow. Use None for unlimited depth.\n    min_samples_split (int): Minimum number of samples required to split an internal node."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'DecisionTreeClassifier'",
                                    "docstring": "Build the decision tree classifier from the training set.\n\nArgs:\n    X (pd.DataFrame): The input training features.\n    y (pd.Series): The target class labels.\n\nReturns:\n    DecisionTreeClassifier: The trained decision tree classifier instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Predict class labels for the provided input data using the trained decision tree.\n\nArgs:\n    X (pd.DataFrame): Input features for which to predict class labels.\n\nReturns:\n    pd.Series: Predicted class labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/decision_trees.py"
                    }
                ]
            },
            {
                "node": "Imbalanced Classification",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Imbalanced Classification",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/supervised/classification/imbalanced_classification.py"
                    }
                ]
            },
            {
                "node": "imbalanced classification",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Imbalanced Classification/imbalanced classification",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ImbalancedClassifier",
                        "extra": {
                            "docstring": "A classifier tailored to address imbalanced classification problems.\n\nThis classifier implements the core methods for fitting and predicting on datasets\nwhere class distribution is skewed. It inherits from the BaseAlgorithm and integrates\nseamlessly with the overall machine learning framework. The classifier allows for the\nspecification of a strategy to handle class imbalances (e.g., undersampling, oversampling,\nor synthetic sample generation).\n\nArgs:\n    strategy (str): The strategy used to mitigate class imbalance. Common options include\n                    'undersample', 'oversample', or 'smote'. Defaults to 'undersample'.\n\nMethods:\n    __init__(strategy: str = 'undersample') -> None:\n        Initializes the classifier with the given imbalance handling strategy.\n    \n    fit(X: pd.DataFrame, y: pd.Series) -> ImbalancedClassifier:\n        Trains the classifier on the provided training data and labels.\n    \n    predict(X: pd.DataFrame) -> pd.Series:\n        Generates predictions for the specified input features.\n\nReturns:\n    An instance of the ImbalancedClassifier after fitting the model.\n\nAssumptions and Edge Cases:\n    - Assumes preprocessed input where missing data and data scaling have been addressed.\n    - The actual internal logic for handling imbalanced data is not implemented here.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "strategy"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'ImbalancedClassifier'",
                                    "docstring": "Train the imbalanced classifier on the provided dataset.\n\nArgs:\n    X (pd.DataFrame): The feature set used for training.\n    y (pd.Series): The target labels corresponding to the training data.\n\nReturns:\n    ImbalancedClassifier: The fitted classifier instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Generate predictions from the imbalanced classifier for the provided input data.\n\nArgs:\n    X (pd.DataFrame): The feature set for which predictions are to be made.\n\nReturns:\n    pd.Series: The predicted class labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/imbalanced_classification.py"
                    }
                ]
            },
            {
                "node": "Logistic Regression",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Logistic Regression",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/supervised/classification/logistic_regression.py"
                    }
                ]
            },
            {
                "node": "cross-entropy loss",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Logistic Regression/cross-entropy loss",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "compute_cross_entropy_loss",
                        "extra": {
                            "args": [
                                "y_true",
                                "y_pred"
                            ],
                            "return_type": "float",
                            "docstring": "Computes the cross-entropy loss between true labels and predicted probabilities.\n\nCross-entropy loss is commonly used as a cost function for classification models,\nparticularly in logistic and multinomial logistic regression.\n\nArgs:\n    y_true (np.ndarray): Array of true labels (one-hot encoded or as indices).\n    y_pred (np.ndarray): Array of predicted probabilities for each class.\n\nReturns:\n    float: The computed cross-entropy loss value.\n\nEdge Cases:\n    - Handles cases where predicted probabilities might be zero by clipping values.\n    - Assumes y_true and y_pred have compatible shapes."
                        },
                        "path": "src/algorithms/supervised/classification/logistic_regression.py"
                    }
                ]
            },
            {
                "node": "l1 regularization",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Logistic Regression/l1 regularization",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "logistic_regression_l1_penalty",
                        "extra": {
                            "args": [
                                "coefficients",
                                "alpha"
                            ],
                            "return_type": "float",
                            "docstring": "Calculate the L1 regularization penalty for logistic regression.\n\nThis function computes the L1 penalty, defined as the sum of absolute\nvalues of the coefficients scaled by the regularization parameter alpha.\nIt is specifically tailored for logistic regression models in a classification\nsetting.\n\nArgs:\n    coefficients (np.ndarray): Array of model coefficients.\n    alpha (float): Regularization strength parameter.\n\nReturns:\n    float: The computed L1 penalty for logistic regression.\n\nEdge Cases:\n    - Expects coefficients to be a non-empty numpy array.\n    - alpha should be a non-negative float."
                        },
                        "path": "src/algorithms/supervised/regression/regularized_regression.py"
                    }
                ]
            },
            {
                "node": "log loss",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Logistic Regression/log loss",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "compute_log_loss",
                        "extra": {
                            "args": [
                                "y_true",
                                "y_pred"
                            ],
                            "return_type": "float",
                            "docstring": "Computes the log loss (negative log-likelihood) for classification predictions.\n\nLog loss is equivalent to cross-entropy loss and is used to evaluate the performance of\nclassifiers built using probabilistic approaches like logistic regression.\n\nArgs:\n    y_true (np.ndarray): Array of true binary or categorical labels (appropriately encoded).\n    y_pred (np.ndarray): Array of predicted probabilities for the positive class or each class.\n\nReturns:\n    float: The calculated log loss value.\n\nEdge Cases:\n    - Applies clipping to y_pred to prevent taking log of zero.\n    - Assumes the input arrays are of compatible sizes."
                        },
                        "path": "src/algorithms/supervised/classification/logistic_regression.py"
                    }
                ]
            },
            {
                "node": "logistic regression",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Logistic Regression/logistic regression",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "LogisticRegressionClassifier",
                        "extra": {
                            "docstring": "LogisticRegressionClassifier implements the basic logistic regression algorithm.\n\nThis classifier provides methods to train a logistic regression model on provided data\nand to generate predictions. It encapsulates the standard logistic regression functionality,\naddressing binary classification tasks.\n\nMethods:\n    __init__(self, penalty: str = 'l2', C: float = 1.0) -> None:\n        Initializes the classifier with regularization parameters.\n    \n    fit(self, X: pd.DataFrame, y: pd.Series) -> 'LogisticRegressionClassifier':\n        Fits the logistic regression model to the training data.\n    \n    predict(self, X: pd.DataFrame) -> pd.Series:\n        Predicts class labels for the input data.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "penalty",
                                        "C"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'LogisticRegressionClassifier'",
                                    "docstring": null
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": null
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/logistic_regression.py"
                    }
                ]
            },
            {
                "node": "multinomial logistic regression",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Logistic Regression/multinomial logistic regression",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "MultinomialLogisticRegressionClassifier",
                        "extra": {
                            "docstring": "MultinomialLogisticRegressionClassifier extends logistic regression for multiclass problems.\n\nThis classifier is designed to handle multinomial (multiclass) logistic regression tasks\nwhere the response variable can take more than two classes. It incorporates methods to fit\nthe model and predict class probabilities.\n\nMethods:\n    __init__(self, penalty: str = 'l2', C: float = 1.0) -> None:\n        Initializes the classifier with parameters suitable for multinomial regression.\n    \n    fit(self, X: pd.DataFrame, y: pd.Series) -> 'MultinomialLogisticRegressionClassifier':\n        Trains the model using the provided training data.\n    \n    predict(self, X: pd.DataFrame) -> pd.Series:\n        Generates predictions for the input data.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "penalty",
                                        "C"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'MultinomialLogisticRegressionClassifier'",
                                    "docstring": null
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": null
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/logistic_regression.py"
                    }
                ]
            },
            {
                "node": "probit regression",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Logistic Regression/probit regression",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ProbitRegressionClassifier",
                        "extra": {
                            "docstring": "ProbitRegressionClassifier implements the probit regression model.\n\nThis classifier uses the cumulative distribution function of the standard normal distribution\nas the link function. It is useful in scenarios where the assumption of logistic regression may not hold.\n\nMethods:\n    __init__(self) -> None:\n        Initializes the probit regression classifier.\n    \n    fit(self, X: pd.DataFrame, y: pd.Series) -> 'ProbitRegressionClassifier':\n        Fits the probit regression model to the training data.\n    \n    predict(self, X: pd.DataFrame) -> pd.Series:\n        Generates predictions based on the fitted model.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'ProbitRegressionClassifier'",
                                    "docstring": null
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": null
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/logistic_regression.py"
                    }
                ]
            },
            {
                "node": "softmax function",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Logistic Regression/softmax function",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "softmax",
                        "extra": {
                            "args": [
                                "z"
                            ],
                            "return_type": "np.ndarray",
                            "docstring": "Computes the softmax function for each row of the input array.\n\nThe softmax function is used to convert raw model output scores into probabilities that sum to 1.\nThis is particularly useful for multiclass classification problems.\n\nArgs:\n    z (np.ndarray): A 2D array of shape (n_samples, n_classes) containing raw scores.\n\nReturns:\n    np.ndarray: A 2D array of the same shape as z, where each row represents a probability distribution\n                over the classes.\n\nEdge Cases:\n    - If the input is empty, returns an empty array.\n    - Numerical stability should be ensured by subtracting the max value in each row before exponentiation."
                        },
                        "path": "src/algorithms/supervised/classification/logistic_regression.py"
                    }
                ]
            },
            {
                "node": "threshold optimization",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Logistic Regression/threshold optimization",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "optimize_threshold",
                        "extra": {
                            "args": [
                                "y_true",
                                "y_scores",
                                "metric"
                            ],
                            "return_type": "float",
                            "docstring": "Optimizes the classification threshold based on a given performance metric.\n\nThis function iterates over possible threshold values to determine the threshold that maximizes \nthe specified evaluation metric (e.g., F1 score). It is particularly useful in logistic regression,\nwhere adjusting the threshold can balance precision and recall.\n\nArgs:\n    y_true (np.ndarray): Array of true binary labels.\n    y_scores (np.ndarray): Array of predicted scores or probabilities.\n    metric (str, optional): The performance metric to optimize. Default is \"f1\".\n\nReturns:\n    float: The optimal threshold value that maximizes the specified metric.\n\nEdge Cases:\n    - If y_scores is empty, the function should handle the error gracefully.\n    - Assumes that y_true and y_scores are aligned and of equal length."
                        },
                        "path": "src/algorithms/supervised/classification/logistic_regression.py"
                    }
                ]
            },
            {
                "node": "Multi-label Classification",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Multi-label Classification",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/supervised/classification/multi_label.py"
                    }
                ]
            },
            {
                "node": "classifier chains",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Multi-label Classification/classifier chains",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ClassifierChainsClassifier",
                        "extra": {
                            "docstring": "ClassifierChainsClassifier implements the classifier chains approach for multi-label classification.\n\nThis technique builds a chain of binary classifiers where each classifier predicts one label while taking into\naccount the predictions of previous classifiers in the chain, enabling the capture of label dependencies.\n\nArgs:\n    base_estimator (Any): The base classifier to be used for each link in the chain.\n    order (List[int], optional): A list defining the order in which labels are predicted. If None, a default order is applied.\n\nMethods:\n    fit(X, y): Train the chain of classifiers on the training data.\n    predict(X): Generate multi-label predictions by processing the input sequentially through the chain.\n\nAttributes:\n    chain (List[Any]): List of individual classifiers fitted in the chain.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "base_estimator",
                                        "order"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'ClassifierChainsClassifier'",
                                    "docstring": "Fit the classifier chain using the provided training data.\n\nArgs:\n    X (pd.DataFrame): Feature set for training.\n    y (pd.DataFrame): Dataframe of multi-label targets.\n\nReturns:\n    ClassifierChainsClassifier: The fitted classifier chain instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Predict multi-label outputs by applying the classifier chain to the input data.\n\nArgs:\n    X (pd.DataFrame): Feature dataframe for prediction.\n\nReturns:\n    pd.DataFrame: A dataframe containing the multi-label predictions."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/multi_label.py"
                    }
                ]
            },
            {
                "node": "multi-label decision trees",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Multi-label Classification/multi-label decision trees",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "MultiLabelDecisionTreeClassifier",
                        "extra": {
                            "docstring": "MultiLabelDecisionTreeClassifier implements a decision tree approach adapted for multi-label classification.\n\nThis classifier extends traditional decision tree methodologies to handle cases where instances may belong to multiple\nclasses. It adjusts split criteria and node decisions to accommodate the simultaneous prediction of multiple labels.\n\nArgs:\n    max_depth (Optional[int]): Maximum depth of the decision tree. If None, the tree is expanded until all leaves are pure.\n    min_samples_split (int): Minimum number of samples required to split an internal node. Defaults to 2.\n\nMethods:\n    fit(X, y): Train the multi-label decision tree using the provided training data.\n    predict(X): Predict multi-label targets for new data instances.\n\nAttributes:\n    tree_structure (Any): An internal representation of the constructed decision tree.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "max_depth",
                                        "min_samples_split"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'MultiLabelDecisionTreeClassifier'",
                                    "docstring": "Fit the multi-label decision tree classifier on the training data.\n\nArgs:\n    X (pd.DataFrame): Feature dataframe for training.\n    y (pd.DataFrame): Dataframe containing multi-label target values.\n\nReturns:\n    MultiLabelDecisionTreeClassifier: The fitted decision tree classifier."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Predict multi-label outcomes for new data using the trained decision tree.\n\nArgs:\n    X (pd.DataFrame): Feature dataframe for which predictions are required.\n\nReturns:\n    pd.DataFrame: A dataframe containing the predicted multi-label outputs."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/multi_label.py"
                    }
                ]
            },
            {
                "node": "multi-label knn",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Multi-label Classification/multi-label knn",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "MultiLabelKNNClassifier",
                        "extra": {
                            "docstring": "MultiLabelKNNClassifier implements a k-nearest neighbors method tailored for multi-label classification.\n\nThis classifier applies the KNN algorithm in scenarios where each instance may be associated with multiple labels.\nIt computes distances between instances and aggregates the labels from the k nearest neighbors to derive multi-label predictions.\n\nArgs:\n    k (int): Number of nearest neighbors to consider.\n    metric (str): Distance metric to be used (e.g., 'euclidean', 'manhattan'). Defaults to 'euclidean'.\n\nMethods:\n    fit(X, y): Fit the classifier using the training data.\n    predict(X): Predict multi-label targets for the provided input data.\n\nAttributes:\n    trained (bool): Indicates whether the classifier has been fitted.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "k",
                                        "metric"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'MultiLabelKNNClassifier'",
                                    "docstring": "Fit the multi-label KNN classifier on the training data.\n\nArgs:\n    X (pd.DataFrame): Feature dataframe used for training.\n    y (pd.DataFrame): Dataframe containing multi-label targets.\n\nReturns:\n    MultiLabelKNNClassifier: The fitted classifier instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Predict multi-label targets for new data instances.\n\nArgs:\n    X (pd.DataFrame): Feature dataframe for which predictions are to be made.\n\nReturns:\n    pd.DataFrame: A dataframe containing the predicted multi-label outputs."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/multi_label.py"
                    }
                ]
            },
            {
                "node": "Naive Bayes",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Naive Bayes",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/supervised/classification/naive_bayes.py"
                    }
                ]
            },
            {
                "node": "naive bayes",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/Naive Bayes/naive bayes",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "NaiveBayesClassifier",
                        "extra": {
                            "docstring": "NaiveBayesClassifier implements the Naive Bayes algorithm for classification tasks.\n\nThis classifier uses Bayes' theorem with the assumption of strong (naive) independence between features.\nIt calculates prior probabilities and likelihoods based on the training data, and employs these to predict\nthe class labels for new data instances.\n\nAttributes:\n    smoothing (float): The Laplace smoothing parameter to ensure robustness against zero-frequency issues \n                       in likelihood estimates.\n\nMethods:\n    fit(X, y): Estimates the prior probabilities and feature likelihoods from the training data.\n    predict(X): Predicts class labels for new input data based on the computed probabilities.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "smoothing"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the NaiveBayesClassifier with a smoothing parameter.\n\nArgs:\n    smoothing (float, optional): Laplace smoothing constant to handle zero probabilities. Defaults to 1.0."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'NaiveBayesClassifier'",
                                    "docstring": "Fit the Naive Bayes classifier by computing the class prior probabilities and feature likelihoods from training data.\n\nArgs:\n    X (pd.DataFrame): A DataFrame containing the training features.\n    y (pd.Series): A Series containing the training class labels.\n\nReturns:\n    NaiveBayesClassifier: The fitted classifier instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Predict class labels for given input data using the Naive Bayes probabilistic model.\n\nArgs:\n    X (pd.DataFrame): A DataFrame containing the features for which to predict class labels.\n\nReturns:\n    pd.Series: A Series containing the predicted class labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/naive_bayes.py"
                    }
                ]
            },
            {
                "node": "SVM Classification",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/SVM Classification",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/supervised/classification/svm_classification.py"
                    }
                ]
            },
            {
                "node": "hard-margin svm",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/SVM Classification/hard-margin svm",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "HardMarginSVMClassifier",
                        "extra": {
                            "docstring": "HardMarginSVMClassifier implements a variant of the support vector machine classifier\nthat assumes strict linear separability. This model enforces a hard margin, meaning no\nmisclassifications are allowed in the training data, resulting in no slack variables.\n\nAttributes:\n    kernel (str): The kernel function used; typically 'linear' for hard-margin SVM.\n    C (float): Regularization parameter; for hard-margin, C is set to a very high value to enforce strict separation.\n\nMethods:\n    fit(X, y):\n        Train the hard-margin SVM classifier on provided training data.\n    predict(X):\n        Predict class labels for new data based on the hard-margin decision boundary.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "kernel",
                                        "C"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the HardMarginSVMClassifier with strict margin parameters.\n\nArgs:\n    kernel (str): Specifies the kernel type. The hard-margin version typically relies on a linear kernel.\n    C (float): A large regularization constant to enforce the hard margin constraint."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'HardMarginSVMClassifier'",
                                    "docstring": "Fit the hard-margin SVM classifier on the training data.\n\nArgs:\n    X (pd.DataFrame): The input feature data.\n    y (pd.Series): The target class labels.\n\nReturns:\n    HardMarginSVMClassifier: The fitted classifier instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Predict class labels for the input data using the hard-margin decision boundary.\n\nArgs:\n    X (pd.DataFrame): The input feature data.\n\nReturns:\n    pd.Series: Predicted class labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/svm_classification.py"
                    }
                ]
            },
            {
                "node": "kernel svm",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/SVM Classification/kernel svm",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KernelSVMClassifier",
                        "extra": {
                            "docstring": "KernelSVMClassifier implements a support vector machine (SVM) classifier using kernel methods.\nThis classifier supports various kernel functions (e.g., linear, polynomial, rbf) to map input data\ninto higher-dimensional spaces for improved separation in cases where data is not linearly separable.\n\nAttributes:\n    kernel (str): The kernel type to be used in the algorithm (e.g., 'linear', 'rbf', 'poly').\n    C (float): Regularization parameter. The strength of the regularization is inversely proportional to C.\n    gamma (str or float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. Can be 'scale', 'auto', or a numeric value.\n\nMethods:\n    fit(X, y):\n        Fit the SVM classifier on the training data.\n    predict(X):\n        Predict the class labels for the provided data.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "kernel",
                                        "C",
                                        "gamma"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the KernelSVMClassifier with specified hyperparameters.\n\nArgs:\n    kernel (str): Specifies the kernel type to be used in the algorithm.\n    C (float): Regularization parameter. Lower values create a softer margin.\n    gamma (str or float): Kernel coefficient, controlling the influence of individual training examples."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'KernelSVMClassifier'",
                                    "docstring": "Fit the SVM classifier on the provided training data.\n\nArgs:\n    X (pd.DataFrame): The input feature data.\n    y (pd.Series): The target class labels.\n\nReturns:\n    KernelSVMClassifier: The fitted classifier instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Predict class labels for the input data using the fitted model.\n\nArgs:\n    X (pd.DataFrame): The input feature data.\n\nReturns:\n    pd.Series: Predicted class labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/svm_classification.py"
                    }
                ]
            },
            {
                "node": "smo algorithm",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/SVM Classification/smo algorithm",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "SMOSVMOptimizer",
                        "extra": {
                            "docstring": "Optimizer for SVM classification using the Sequential Minimal Optimization (SMO) algorithm.\n\nThis class provides an interface for optimizing SVM quadratic programming problems\nby applying the SMO algorithm. It incrementally adjusts the Lagrange multipliers to\nfind the optimal decision boundary. The implementation is designed to ensure efficient\nhandling of constraints and calculation of the optimum parameters.\n\nAttributes:\n    C (float): Regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n    tolerance (float): Tolerance for the optimization to determine convergence.\n    max_iter (int): Maximum number of iterations to attempt optimization.\n\nMethods:\n    optimize(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        Executes the SMO algorithm on the provided training data to compute the optimal\n        support vectors and corresponding model parameters.\n\nArgs:\n    C (float): The regularization parameter for SVM.\n    tolerance (float): A small value to detect convergence of the algorithm.\n    max_iter (int): The limit on the number of iterations for the optimization process.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "C",
                                        "tolerance",
                                        "max_iter"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "optimize",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Optimize the SVM model parameters using the SMO algorithm.\n\nArgs:\n    X (np.ndarray): A 2D numpy array containing the input features.\n    y (np.ndarray): A 1D numpy array of target labels corresponding to X.\n\nReturns:\n    np.ndarray: The vector of optimized parameters (e.g., Lagrange multipliers or weight vector)\n    resulting from the SMO optimization process."
                                }
                            ]
                        },
                        "path": "src/algorithms/optimization/svm_optimization.py"
                    }
                ]
            },
            {
                "node": "stochastic gradient descent",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/SVM Classification/stochastic gradient descent",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "SVMStochasticGradientDescentOptimizer",
                        "extra": {
                            "docstring": "Optimizer for SVM classification using stochastic gradient descent.\n\nThis class implements the optimization of the SVM objective function using\nstochastic gradient descent (SGD). It is designed to handle large datasets by\niteratively updating the model weights based on mini-batches or single samples.\n\nAttributes:\n    learning_rate (float): The step size used for each update.\n    max_iter (int): The maximum number of iterations to run the optimizer.\n    tolerance (float): The threshold for stopping criterion based on weight change.\n\nMethods:\n    optimize(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        Performs the optimization to determine the optimal parameters (weights)\n        for the SVM model given the training data and labels.\n\nArgs:\n    learning_rate (float): Learning rate for the SGD updates.\n    max_iter (int): Maximum number of iterations to perform.\n    tolerance (float): Tolerance for convergence; if updates fall below this value,\n        optimization stops.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "learning_rate",
                                        "max_iter",
                                        "tolerance"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "optimize",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Optimize the SVM objective function using stochastic gradient descent.\n\nProcesses the feature matrix X and associated labels y, iteratively updating\nthe model's weights to minimize the SVM loss function.\n\nArgs:\n    X (np.ndarray): A 2D array representing the input features.\n    y (np.ndarray): A 1D array of target labels corresponding to X.\n\nReturns:\n    np.ndarray: The optimized weight vector after convergence or after\n    reaching the maximum number of iterations."
                                }
                            ]
                        },
                        "path": "src/algorithms/optimization/svm_optimization.py"
                    }
                ]
            },
            {
                "node": "support vector machines",
                "feature_path": "Algorithms/Supervised Learning/Classification Algorithms/SVM Classification/support vector machines",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KernelSVMClassifier",
                        "extra": {
                            "docstring": "KernelSVMClassifier implements a support vector machine (SVM) classifier using kernel methods.\nThis classifier supports various kernel functions (e.g., linear, polynomial, rbf) to map input data\ninto higher-dimensional spaces for improved separation in cases where data is not linearly separable.\n\nAttributes:\n    kernel (str): The kernel type to be used in the algorithm (e.g., 'linear', 'rbf', 'poly').\n    C (float): Regularization parameter. The strength of the regularization is inversely proportional to C.\n    gamma (str or float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. Can be 'scale', 'auto', or a numeric value.\n\nMethods:\n    fit(X, y):\n        Fit the SVM classifier on the training data.\n    predict(X):\n        Predict the class labels for the provided data.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "kernel",
                                        "C",
                                        "gamma"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the KernelSVMClassifier with specified hyperparameters.\n\nArgs:\n    kernel (str): Specifies the kernel type to be used in the algorithm.\n    C (float): Regularization parameter. Lower values create a softer margin.\n    gamma (str or float): Kernel coefficient, controlling the influence of individual training examples."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'KernelSVMClassifier'",
                                    "docstring": "Fit the SVM classifier on the provided training data.\n\nArgs:\n    X (pd.DataFrame): The input feature data.\n    y (pd.Series): The target class labels.\n\nReturns:\n    KernelSVMClassifier: The fitted classifier instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Predict class labels for the input data using the fitted model.\n\nArgs:\n    X (pd.DataFrame): The input feature data.\n\nReturns:\n    pd.Series: Predicted class labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/classification/svm_classification.py"
                    }
                ]
            },
            {
                "node": "Regression Algorithms",
                "feature_path": "Algorithms/Supervised Learning/Regression Algorithms",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/supervised/regression"
                    }
                ]
            },
            {
                "node": "Linear Regression",
                "feature_path": "Algorithms/Supervised Learning/Regression Algorithms/Linear Regression",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/supervised/regression/linear_regression.py"
                    }
                ]
            },
            {
                "node": "linear regression",
                "feature_path": "Algorithms/Supervised Learning/Regression Algorithms/Linear Regression/linear regression",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "LinearRegression",
                        "extra": {
                            "docstring": "LinearRegression implements various linear regression techniques including:\n  - Simple linear regression: to model a relationship between one independent variable and the dependent variable.\n  - Multiple linear regression: to model the relationship between multiple independent variables and the dependent variable.\n  - Ordinary Least Squares (OLS): the standard method for estimating the parameters in linear regression.\n  - Generic linear regression behavior encapsulated as basic linear regression.\n\nThis class leverages the ordinary least squares method to estimate coefficients. It can be configured to work for both simple and multiple regression scenarios.\n\nAttributes:\n    fit_intercept (bool): Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations.\n    coefficients (Any): Model coefficients computed during fitting.\n\nMethods:\n    fit(X: pd.DataFrame, y: pd.Series) -> 'LinearRegression':\n        Fit the linear regression model using the ordinary least squares method on the provided training data.\n        \n    predict(X: pd.DataFrame) -> pd.Series:\n        Predict the target values for the given input data based on the computed regression coefficients.\n\nEdge Cases and Assumptions:\n    - Assumes input features in X are numerical and properly preprocessed.\n    - The target y is expected to be continuous.\n    - May not handle singular matrices; preprocessing (e.g., feature selection) should be conducted to ensure numerical stability.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "fit_intercept"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the LinearRegression model.\n\nArgs:\n    fit_intercept (bool): Flag indicating whether to calculate the intercept. Default is True."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'LinearRegression'",
                                    "docstring": "Fit the linear regression model using the ordinary least squares method.\n\nArgs:\n    X (pd.DataFrame): Training data with one or multiple features.\n    y (pd.Series): Continuous target values corresponding to X.\n\nReturns:\n    LinearRegression: The fitted model instance with computed coefficients.\n\nRaises:\n    None: The method does not raise exceptions; input validation should be performed prior to calling."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Predict target values using the linear regression model.\n\nArgs:\n    X (pd.DataFrame): Data containing the same features as used during fitting.\n\nReturns:\n    pd.Series: Predicted continuous target values.\n\nRaises:\n    None: It is assumed that the input data X is preprocessed similarly to the training data."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/regression/linear_regression.py"
                    }
                ]
            },
            {
                "node": "multiple linear regression",
                "feature_path": "Algorithms/Supervised Learning/Regression Algorithms/Linear Regression/multiple linear regression",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "LinearRegression",
                        "extra": {
                            "docstring": "LinearRegression implements various linear regression techniques including:\n  - Simple linear regression: to model a relationship between one independent variable and the dependent variable.\n  - Multiple linear regression: to model the relationship between multiple independent variables and the dependent variable.\n  - Ordinary Least Squares (OLS): the standard method for estimating the parameters in linear regression.\n  - Generic linear regression behavior encapsulated as basic linear regression.\n\nThis class leverages the ordinary least squares method to estimate coefficients. It can be configured to work for both simple and multiple regression scenarios.\n\nAttributes:\n    fit_intercept (bool): Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations.\n    coefficients (Any): Model coefficients computed during fitting.\n\nMethods:\n    fit(X: pd.DataFrame, y: pd.Series) -> 'LinearRegression':\n        Fit the linear regression model using the ordinary least squares method on the provided training data.\n        \n    predict(X: pd.DataFrame) -> pd.Series:\n        Predict the target values for the given input data based on the computed regression coefficients.\n\nEdge Cases and Assumptions:\n    - Assumes input features in X are numerical and properly preprocessed.\n    - The target y is expected to be continuous.\n    - May not handle singular matrices; preprocessing (e.g., feature selection) should be conducted to ensure numerical stability.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "fit_intercept"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the LinearRegression model.\n\nArgs:\n    fit_intercept (bool): Flag indicating whether to calculate the intercept. Default is True."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'LinearRegression'",
                                    "docstring": "Fit the linear regression model using the ordinary least squares method.\n\nArgs:\n    X (pd.DataFrame): Training data with one or multiple features.\n    y (pd.Series): Continuous target values corresponding to X.\n\nReturns:\n    LinearRegression: The fitted model instance with computed coefficients.\n\nRaises:\n    None: The method does not raise exceptions; input validation should be performed prior to calling."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Predict target values using the linear regression model.\n\nArgs:\n    X (pd.DataFrame): Data containing the same features as used during fitting.\n\nReturns:\n    pd.Series: Predicted continuous target values.\n\nRaises:\n    None: It is assumed that the input data X is preprocessed similarly to the training data."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/regression/linear_regression.py"
                    }
                ]
            },
            {
                "node": "ordinary least squares",
                "feature_path": "Algorithms/Supervised Learning/Regression Algorithms/Linear Regression/ordinary least squares",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "LinearRegression",
                        "extra": {
                            "docstring": "LinearRegression implements various linear regression techniques including:\n  - Simple linear regression: to model a relationship between one independent variable and the dependent variable.\n  - Multiple linear regression: to model the relationship between multiple independent variables and the dependent variable.\n  - Ordinary Least Squares (OLS): the standard method for estimating the parameters in linear regression.\n  - Generic linear regression behavior encapsulated as basic linear regression.\n\nThis class leverages the ordinary least squares method to estimate coefficients. It can be configured to work for both simple and multiple regression scenarios.\n\nAttributes:\n    fit_intercept (bool): Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations.\n    coefficients (Any): Model coefficients computed during fitting.\n\nMethods:\n    fit(X: pd.DataFrame, y: pd.Series) -> 'LinearRegression':\n        Fit the linear regression model using the ordinary least squares method on the provided training data.\n        \n    predict(X: pd.DataFrame) -> pd.Series:\n        Predict the target values for the given input data based on the computed regression coefficients.\n\nEdge Cases and Assumptions:\n    - Assumes input features in X are numerical and properly preprocessed.\n    - The target y is expected to be continuous.\n    - May not handle singular matrices; preprocessing (e.g., feature selection) should be conducted to ensure numerical stability.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "fit_intercept"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the LinearRegression model.\n\nArgs:\n    fit_intercept (bool): Flag indicating whether to calculate the intercept. Default is True."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'LinearRegression'",
                                    "docstring": "Fit the linear regression model using the ordinary least squares method.\n\nArgs:\n    X (pd.DataFrame): Training data with one or multiple features.\n    y (pd.Series): Continuous target values corresponding to X.\n\nReturns:\n    LinearRegression: The fitted model instance with computed coefficients.\n\nRaises:\n    None: The method does not raise exceptions; input validation should be performed prior to calling."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Predict target values using the linear regression model.\n\nArgs:\n    X (pd.DataFrame): Data containing the same features as used during fitting.\n\nReturns:\n    pd.Series: Predicted continuous target values.\n\nRaises:\n    None: It is assumed that the input data X is preprocessed similarly to the training data."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/regression/linear_regression.py"
                    }
                ]
            },
            {
                "node": "simple linear regression",
                "feature_path": "Algorithms/Supervised Learning/Regression Algorithms/Linear Regression/simple linear regression",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "LinearRegression",
                        "extra": {
                            "docstring": "LinearRegression implements various linear regression techniques including:\n  - Simple linear regression: to model a relationship between one independent variable and the dependent variable.\n  - Multiple linear regression: to model the relationship between multiple independent variables and the dependent variable.\n  - Ordinary Least Squares (OLS): the standard method for estimating the parameters in linear regression.\n  - Generic linear regression behavior encapsulated as basic linear regression.\n\nThis class leverages the ordinary least squares method to estimate coefficients. It can be configured to work for both simple and multiple regression scenarios.\n\nAttributes:\n    fit_intercept (bool): Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations.\n    coefficients (Any): Model coefficients computed during fitting.\n\nMethods:\n    fit(X: pd.DataFrame, y: pd.Series) -> 'LinearRegression':\n        Fit the linear regression model using the ordinary least squares method on the provided training data.\n        \n    predict(X: pd.DataFrame) -> pd.Series:\n        Predict the target values for the given input data based on the computed regression coefficients.\n\nEdge Cases and Assumptions:\n    - Assumes input features in X are numerical and properly preprocessed.\n    - The target y is expected to be continuous.\n    - May not handle singular matrices; preprocessing (e.g., feature selection) should be conducted to ensure numerical stability.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "fit_intercept"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the LinearRegression model.\n\nArgs:\n    fit_intercept (bool): Flag indicating whether to calculate the intercept. Default is True."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'LinearRegression'",
                                    "docstring": "Fit the linear regression model using the ordinary least squares method.\n\nArgs:\n    X (pd.DataFrame): Training data with one or multiple features.\n    y (pd.Series): Continuous target values corresponding to X.\n\nReturns:\n    LinearRegression: The fitted model instance with computed coefficients.\n\nRaises:\n    None: The method does not raise exceptions; input validation should be performed prior to calling."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Predict target values using the linear regression model.\n\nArgs:\n    X (pd.DataFrame): Data containing the same features as used during fitting.\n\nReturns:\n    pd.Series: Predicted continuous target values.\n\nRaises:\n    None: It is assumed that the input data X is preprocessed similarly to the training data."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/regression/linear_regression.py"
                    }
                ]
            },
            {
                "node": "Regularized Regression",
                "feature_path": "Algorithms/Supervised Learning/Regression Algorithms/Regularized Regression",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/supervised/regression/regularized_regression.py"
                    }
                ]
            },
            {
                "node": "feature selection",
                "feature_path": "Algorithms/Supervised Learning/Regression Algorithms/Regularized Regression/feature selection",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "regularized_regression_feature_selection",
                        "extra": {
                            "args": [
                                "X",
                                "y",
                                "method"
                            ],
                            "return_type": "List[str]",
                            "docstring": "Perform feature selection for regularized regression models.\n\nThis function applies a feature selection strategy, typically leveraging L1\nregularization (e.g., LASSO), to identify a subset of important features\nfrom the input dataset. The selected features aim to improve model performance\nby reducing overfitting and enhancing interpretability.\n\nArgs:\n    X (pd.DataFrame): Input feature dataset.\n    y (pd.Series): Target variable.\n    method (str, optional): Feature selection method to apply. Defaults to \"lasso\".\n\nReturns:\n    List[str]: A list of selected feature names.\n\nEdge Cases:\n    - Assumes non-empty DataFrame and Series.\n    - The method parameter must be one of the supported feature selection strategies."
                        },
                        "path": "src/algorithms/supervised/regression/regularized_regression.py"
                    }
                ]
            },
            {
                "node": "kernel ridge regression",
                "feature_path": "Algorithms/Supervised Learning/Regression Algorithms/Regularized Regression/kernel ridge regression",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KernelRidgeRegression",
                        "extra": {
                            "docstring": "Implements Kernel Ridge Regression for non-linear regression tasks.\n\nThis class integrates kernel methods with ridge regression to allow modeling\nnon-linear relationships. It inherently uses L2 regularization and supports the\nuse of various kernel functions for flexible modeling.\n\nArgs:\n    alpha (float): Regularization strength for L2 penalty.\n    kernel (str): Specifies the kernel type to be used (e.g., 'linear', 'rbf').\n    gamma (float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels.\n\nMethods:\n    fit(X, y): Fit the Kernel Ridge Regression model on training data.\n    predict(X): Generate predictions using the fitted model.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "alpha",
                                        "kernel",
                                        "gamma"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'KernelRidgeRegression'",
                                    "docstring": "Fit the Kernel Ridge Regression model.\n\nArgs:\n    X (pd.DataFrame): Training data features.\n    y (pd.Series): Target values.\n\nReturns:\n    KernelRidgeRegression: The fitted model instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Predict target values using the fitted Kernel Ridge Regression model.\n\nArgs:\n    X (pd.DataFrame): Data for which predictions are to be made.\n\nReturns:\n    pd.Series: Predicted output values."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/regression/regularized_regression.py"
                    }
                ]
            },
            {
                "node": "l1 regularization",
                "feature_path": "Algorithms/Supervised Learning/Regression Algorithms/Regularized Regression/l1 regularization",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "regularized_regression_l1_penalty",
                        "extra": {
                            "args": [
                                "weights",
                                "lambda_value"
                            ],
                            "return_type": "float",
                            "docstring": "Compute the L1 regularization penalty for regularized regression models.\n\nThis function computes the L1 penalty as the sum of the absolute values of the weights,\nscaled by the regularization parameter lambda_value. This penalty is used to enforce sparsity\nin regression models and is distinct from the logistic regression context.\n\nArgs:\n    weights (np.ndarray): Array of model weights.\n    lambda_value (float): Regularization parameter controlling the strength of the penalty.\n\nReturns:\n    float: The calculated L1 regularization penalty.\n\nEdge Cases:\n    - Expects weights to be a non-empty numpy array.\n    - lambda_value should be a non-negative float."
                        },
                        "path": "src/algorithms/supervised/regression/regularized_regression.py"
                    }
                ]
            },
            {
                "node": "l2 regularization",
                "feature_path": "Algorithms/Supervised Learning/Regression Algorithms/Regularized Regression/l2 regularization",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "l2_regularization_penalty",
                        "extra": {
                            "args": [
                                "weights",
                                "lambda_value"
                            ],
                            "return_type": "float",
                            "docstring": "Compute the L2 regularization (ridge) penalty for a set of model weights.\n\nThis function calculates the L2 penalty as the sum of squared weights multiplied\nby the regularization factor lambda_value. It is commonly used in regularized regression\nto reduce overfitting by penalizing large coefficients.\n\nArgs:\n    weights (np.ndarray): Array of model weights.\n    lambda_value (float): Regularization factor (non-negative).\n\nReturns:\n    float: The computed L2 regularization penalty.\n\nEdge Cases:\n    - Assumes weights is a non-empty numpy array.\n    - lambda_value must be a non-negative float."
                        },
                        "path": "src/algorithms/supervised/regression/regularized_regression.py"
                    }
                ]
            },
            {
                "node": "Semi-Supervised Learning",
                "feature_path": "Algorithms/Supervised Learning/Semi-Supervised Learning",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/supervised"
                    }
                ]
            },
            {
                "node": "Semi-Supervised Techniques",
                "feature_path": "Algorithms/Supervised Learning/Semi-Supervised Learning/Semi-Supervised Techniques",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/supervised/semi_supervised.py"
                    }
                ]
            },
            {
                "node": "co-training",
                "feature_path": "Algorithms/Supervised Learning/Semi-Supervised Learning/Semi-Supervised Techniques/co-training",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "CoTrainingClassifier",
                        "extra": {
                            "docstring": "A classifier implementing the co-training semi-supervised learning technique.\n\nThis classifier trains two separate models on different views of the labeled data and \nthen iteratively refines each by pseudo-labeling the unlabeled data. The two models \nare expected to complement each other, leveraging unlabeled samples to enhance learning.\n\nMethods:\n    __init__(...): Initialize the classifier with hyperparameters and base learners.\n    fit(X, y): Fit the classifier on labeled (and optionally unlabeled) data.\n    predict(X): Generate predictions for the input data.\n\nArgs:\n    base_estimator1 (Any): The first base classifier instance.\n    base_estimator2 (Any): The second base classifier instance.\n    unlabeled_weight (float): Weighting factor for unlabeled data.\n    max_iter (int): Maximum number of iterations for the co-training process.\n\nReturns:\n    CoTrainingClassifier: An instance of the co-training classifier after fitting.\n\nEdge Cases:\n    - Assumes that input data is a pandas DataFrame.\n    - The classifier expects valid and compatible base estimators.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "base_estimator1",
                                        "base_estimator2",
                                        "unlabeled_weight",
                                        "max_iter"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'CoTrainingClassifier'",
                                    "docstring": "Fit the co-training classifier using labeled and unlabeled data.\n\nArgs:\n    X (pd.DataFrame): The input feature data.\n    y (pd.Series): The target labels for the labeled data.\n\nReturns:\n    CoTrainingClassifier: The fitted classifier instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Generate predictions for the given input data.\n\nArgs:\n    X (pd.DataFrame): The data for which predictions are required.\n\nReturns:\n    pd.Series: The predicted labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/semi_supervised.py"
                    }
                ]
            },
            {
                "node": "self-training",
                "feature_path": "Algorithms/Supervised Learning/Semi-Supervised Learning/Semi-Supervised Techniques/self-training",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "SelfTrainingClassifier",
                        "extra": {
                            "docstring": "A classifier implementing the self-training semi-supervised learning technique.\n\nThis classifier starts with a base estimator trained on labeled data and then iteratively \npseudo-labels the unlabeled data to expand the training set. The self-training mechanism \nrelies on the classifier's confidence to augment the labeled set over several iterations.\n\nMethods:\n    __init__(...): Initialize the self-training classifier with a base estimator and parameters.\n    fit(X, y): Fit the classifier, leveraging a combination of labeled and pseudo-labeled data.\n    predict(X): Produce predictions using the trained classifier.\n\nArgs:\n    base_estimator (Any): An instance of a base classifier to be used for training.\n    confidence_threshold (float): The threshold above which predictions are considered reliable for pseudo-labeling.\n    max_iter (int): The maximum number of iterations for the self-training process.\n\nReturns:\n    SelfTrainingClassifier: An instance of the self-training classifier after fitting.\n\nEdge Cases:\n    - Assumes input data is a pandas DataFrame.\n    - Relies on the base estimator having a predict_proba method for confidence estimation (if applicable).",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "base_estimator",
                                        "confidence_threshold",
                                        "max_iter"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'SelfTrainingClassifier'",
                                    "docstring": "Fit the self-training classifier using labeled and unlabeled data.\n\nArgs:\n    X (pd.DataFrame): The input feature data.\n    y (pd.Series): The target labels for the labeled portion of the data.\n\nReturns:\n    SelfTrainingClassifier: The fitted classifier instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.Series",
                                    "docstring": "Generate predictions for the provided data.\n\nArgs:\n    X (pd.DataFrame): The data for which the predictions are required.\n\nReturns:\n    pd.Series: The predicted labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/supervised/semi_supervised.py"
                    }
                ]
            },
            {
                "node": "Tree-based and Ensemble Methods",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/tree_ensemble"
                    }
                ]
            },
            {
                "node": "Ensemble and Tree Algorithms",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/tree_ensemble"
                    }
                ]
            },
            {
                "node": "Decision Tree Methods",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/tree_ensemble/decision_tree_methods.py"
                    }
                ]
            },
            {
                "node": "cost complexity pruning",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/cost complexity pruning",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "apply_cost_complexity_pruning",
                        "extra": {
                            "args": [
                                "tree_model",
                                "ccp_alpha"
                            ],
                            "return_type": "Any",
                            "docstring": "Apply cost complexity pruning to a decision tree model to balance complexity versus predictive accuracy.\n\nThis function prunes the decision tree based on a complexity parameter (ccp_alpha). The method\ncomputes a trade-off between the model complexity and its performance, removing branches that incur\na cost higher than the improvement in error reduction.\n\nArgs:\n    tree_model (Any): The decision tree model to be pruned.\n    ccp_alpha (float): The complexity parameter used to control the trade-off between tree size and accuracy.\n                       A larger value leads to more pruning.\n\nReturns:\n    Any: The pruned decision tree model after applying cost complexity adjustments."
                        },
                        "path": "src/algorithms/tree_ensemble/decision_tree_methods.py"
                    }
                ]
            },
            {
                "node": "depth limitation",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/depth limitation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "apply_depth_limitation",
                        "extra": {
                            "args": [
                                "tree_model",
                                "max_depth"
                            ],
                            "return_type": "Any",
                            "docstring": "Limit the depth of a decision tree model to prevent overfitting and excessive complexity.\n\nThis function enforces a maximum tree depth by pruning nodes that exceed the specified depth limit,\nensuring that the final tree maintains a controlled level of granularity.\n\nArgs:\n    tree_model (Any): The decision tree model to which the depth limitation is applied.\n    max_depth (int): The maximum allowed depth for the tree. Nodes beyond this depth will be pruned.\n\nReturns:\n    Any: The decision tree model with enforced depth limitation."
                        },
                        "path": "src/algorithms/tree_ensemble/decision_tree_methods.py"
                    }
                ]
            },
            {
                "node": "early stopping",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/early stopping",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "apply_early_stopping",
                        "extra": {
                            "args": [
                                "tree_model",
                                "X_val",
                                "y_val",
                                "patience"
                            ],
                            "return_type": "Any",
                            "docstring": "Apply early stopping as a pruning strategy during decision tree training.\n\nThis function monitors the performance of a decision tree model on a validation dataset\nand stops further growth when the performance fails to improve for a defined number of iterations\n(patience). This prevents the model from overfitting.\n\nArgs:\n    tree_model (Any): The decision tree model under training or intermediate stage.\n    X_val (pd.DataFrame): The validation features used to monitor performance improvements.\n    y_val (pd.Series): The validation labels corresponding to X_val.\n    patience (int): The number of iterations with no improvement after which training is stopped.\n\nReturns:\n    Any: The decision tree model that has been halted early to avoid overfitting."
                        },
                        "path": "src/algorithms/tree_ensemble/decision_tree_methods.py"
                    }
                ]
            },
            {
                "node": "node-limited pre-pruning",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/node-limited pre-pruning",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "apply_node_limited_pre_pruning",
                        "extra": {
                            "args": [
                                "tree_model",
                                "max_nodes"
                            ],
                            "return_type": "Any",
                            "docstring": "Perform node-limited pre-pruning on a decision tree model by restricting the total number of nodes.\n\nThis function applies a pre-pruning strategy where the tree growth is halted once the number\nof nodes reaches a specified maximum. This helps in controlling model complexity and overfitting.\n\nArgs:\n    tree_model (Any): The decision tree model to be pruned.\n    max_nodes (int): The maximum number of nodes allowed in the tree.\n\nReturns:\n    Any: The pruned decision tree model with node limitation."
                        },
                        "path": "src/algorithms/tree_ensemble/decision_tree_methods.py"
                    }
                ]
            },
            {
                "node": "reduced error pruning",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Decision Tree Methods/reduced error pruning",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "apply_reduced_error_pruning",
                        "extra": {
                            "args": [
                                "tree_model",
                                "X_val",
                                "y_val",
                                "tolerance"
                            ],
                            "return_type": "Any",
                            "docstring": "Apply reduced error pruning to a decision tree model using a validation dataset.\n\nThis function prunes the given decision tree model by iteratively removing nodes\nand evaluating the impact on validation accuracy. The pruning stops when the pruning\ndoes not yield an improvement greater than a specified tolerance.\n\nArgs:\n    tree_model (Any): The decision tree model to be pruned.\n    X_val (pd.DataFrame): The validation features used to evaluate pruning impact.\n    y_val (pd.Series): The validation labels corresponding to X_val.\n    tolerance (Optional[float]): The minimum improvement threshold required to continue pruning.\n                                 If None, a default threshold may be used by the underlying algorithm.\n\nReturns:\n    Any: The pruned decision tree model."
                        },
                        "path": "src/algorithms/tree_ensemble/decision_tree_methods.py"
                    }
                ]
            },
            {
                "node": "Forest & Voting Methods",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/tree_ensemble/forest_voting_methods.py"
                    }
                ]
            },
            {
                "node": "bagging",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/bagging",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "BaggingEnsemble",
                        "extra": {
                            "docstring": "BaggingEnsemble implements the bootstrap aggregating (bagging) method for ensemble learning.\nThis technique trains multiple instances of a base estimator on random subsets of the training data\nand aggregates their predictions to enhance stability and accuracy.\n\nAttributes:\n    base_estimator (Any): The base model used for bagging.\n    n_estimators (int): The number of base estimators to train.\n    max_samples (Optional[int]): The number of samples drawn from the training set for each estimator.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "base_estimator",
                                        "n_estimators",
                                        "max_samples"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'BaggingEnsemble'",
                                    "docstring": "Fit the bagging ensemble by training multiple instances of the base estimator on subsets of the data.\n\nArgs:\n    X (pd.DataFrame): Training features.\n    y (pd.Series): Training target labels.\n\nReturns:\n    BaggingEnsemble: The fitted bagging ensemble model."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predict outcomes by aggregating the predictions of individual base estimators.\n\nArgs:\n    X (pd.DataFrame): Data for which predictions are required.\n\nReturns:\n    np.ndarray: Aggregated predictions."
                                }
                            ]
                        },
                        "path": "src/algorithms/tree_ensemble/forest_voting_methods.py"
                    }
                ]
            },
            {
                "node": "binary classification",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/binary classification",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "BinaryVotingClassifier",
                        "extra": {
                            "docstring": "BinaryVotingClassifier specializes in ensemble classification for binary outcomes using a voting mechanism.\nIt aggregates predictions from multiple binary classifiers to determine the final binary label.\n\nAttributes:\n    estimators (List[Any]): A list of binary classifier estimators.\n    voting (str): The voting strategy ('hard' or 'soft').\n    weights (Optional[List[float]]): Optional weights for each classifier.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "estimators",
                                        "voting",
                                        "weights"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'BinaryVotingClassifier'",
                                    "docstring": "Fit the binary voting classifier with the training data.\n\nArgs:\n    X (pd.DataFrame): Training features.\n    y (pd.Series): Binary target labels.\n\nReturns:\n    BinaryVotingClassifier: The fitted classifier instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predict binary class labels for the given input data using a voting mechanism.\n\nArgs:\n    X (pd.DataFrame): Data for which to predict binary labels.\n\nReturns:\n    np.ndarray: Predicted binary class labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/tree_ensemble/forest_voting_methods.py"
                    }
                ]
            },
            {
                "node": "gradient boosting",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/gradient boosting",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "GradientBoostingEnsemble",
                        "extra": {
                            "docstring": "GradientBoostingEnsemble implements a gradient boosting ensemble method where\nmodels are trained sequentially to correct the errors of prior models. This interface\nis designed to support boosting with decision trees for improved predictive performance.\n\nAttributes:\n    n_estimators (int): The number of boosting stages to perform.\n    learning_rate (float): The contribution weight of each model.\n    max_depth (int): The maximum depth of individual regression estimators.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_estimators",
                                        "learning_rate",
                                        "max_depth"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'GradientBoostingEnsemble'",
                                    "docstring": "Fit the gradient boosting ensemble to the training data.\n\nArgs:\n    X (pd.DataFrame): Training features.\n    y (pd.Series): Target values.\n\nReturns:\n    GradientBoostingEnsemble: The fitted boosting ensemble instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Generate predictions for input data using the ensemble of boosted models.\n\nArgs:\n    X (pd.DataFrame): Data for prediction.\n\nReturns:\n    np.ndarray: Predicted values or class labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/tree_ensemble/forest_voting_methods.py"
                    }
                ]
            },
            {
                "node": "multi-class classification",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/multi-class classification",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "MultiClassVotingClassifier",
                        "extra": {
                            "docstring": "MultiClassVotingClassifier is designed to perform ensemble classification specifically\nfor multi-class problems using voting mechanisms. It aggregates predictions from several\nbase classifiers to determine the most likely class among multiple possible outcomes.\n\nAttributes:\n    estimators (List[Any]): A list of classifier estimators.\n    voting (str): Type of voting to use ('hard' or 'soft').\n    weights (Optional[List[float]]): Optional weightings for each estimator.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "estimators",
                                        "voting",
                                        "weights"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'MultiClassVotingClassifier'",
                                    "docstring": "Fit the multi-class voting classifier using training data.\n\nArgs:\n    X (pd.DataFrame): Input feature data.\n    y (pd.Series): Multi-class target labels.\n\nReturns:\n    MultiClassVotingClassifier: The fitted classifier instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predict class labels for multi-class classification using a voting scheme.\n\nArgs:\n    X (pd.DataFrame): Data for which to predict class labels.\n\nReturns:\n    np.ndarray: Predicted class labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/tree_ensemble/forest_voting_methods.py"
                    }
                ]
            },
            {
                "node": "quantile regression",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/quantile regression",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "QuantileRegressionForest",
                        "extra": {
                            "docstring": "QuantileRegressionForest implements an ensemble method for quantile regression using a collection \nof decision trees. This model is designed to estimate conditional quantiles, providing insight into \nthe distribution of the target variable rather than just the mean.\n\nAttributes:\n    n_estimators (int): The number of trees in the forest.\n    quantiles (List[float]): The list of quantiles to estimate (e.g., [0.1, 0.5, 0.9]).\n    max_depth (int): The maximum depth of each tree.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_estimators",
                                        "quantiles",
                                        "max_depth"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'QuantileRegressionForest'",
                                    "docstring": "Fit the quantile regression forest model on the training data.\n\nArgs:\n    X (pd.DataFrame): Training feature data.\n    y (pd.Series): Target values.\n\nReturns:\n    QuantileRegressionForest: The fitted quantile regression model."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Predict quantiles for the given input data. Returns a dictionary mapping each quantile to an array\nof predicted values.\n\nArgs:\n    X (pd.DataFrame): Data for prediction.\n\nReturns:\n    dict: A dictionary where keys are quantile levels and values are arrays of predictions."
                                }
                            ]
                        },
                        "path": "src/algorithms/tree_ensemble/forest_voting_methods.py"
                    }
                ]
            },
            {
                "node": "random forest",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/random forest",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "RandomForestEnsemble",
                        "extra": {
                            "docstring": "RandomForestEnsemble implements the random forest algorithm, an ensemble method that\nbuilds a multitude of decision trees at training time and outputs the mode of the classes\nfor classification or mean prediction for regression tasks.\n\nAttributes:\n    n_estimators (int): Number of trees in the forest.\n    max_features (Optional[int]): The number of features to consider when looking for the best split.\n    bootstrap (bool): Whether bootstrap samples are used when building trees.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_estimators",
                                        "max_features",
                                        "bootstrap"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'RandomForestEnsemble'",
                                    "docstring": "Fit the random forest ensemble on the training data.\n\nArgs:\n    X (pd.DataFrame): Training feature data.\n    y (pd.Series): Training target labels.\n\nReturns:\n    RandomForestEnsemble: The fitted ensemble model."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predict class labels or regression outputs using the random forest ensemble.\n\nArgs:\n    X (pd.DataFrame): Input data for prediction.\n\nReturns:\n    np.ndarray: Predicted outcomes."
                                }
                            ]
                        },
                        "path": "src/algorithms/tree_ensemble/forest_voting_methods.py"
                    }
                ]
            },
            {
                "node": "voting",
                "feature_path": "Algorithms/Tree-based and Ensemble Methods/Ensemble and Tree Algorithms/Forest & Voting Methods/voting",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "VotingEnsemble",
                        "extra": {
                            "docstring": "VotingEnsemble provides an ensemble method that aggregates predictions from multiple models\nusing a voting mechanism. It supports both hard voting, where predictions are determined by\nmajority rule, and soft voting, where averaged probabilities determine the final outcome.\n\nAttributes:\n    estimators (List[Any]): A list of fitted estimators whose predictions will be aggregated.\n    voting (str): The voting type, either 'hard' for majority vote or 'soft' for probability-weighted vote.\n    weights (Optional[List[float]]): Optional list of weights corresponding to each estimator.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "estimators",
                                        "voting",
                                        "weights"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'VotingEnsemble'",
                                    "docstring": "Fit the ensemble using the provided training data.\n\nArgs:\n    X (pd.DataFrame): Training feature data.\n    y (pd.Series): True labels for training.\n\nReturns:\n    VotingEnsemble: The fitted ensemble instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predict the class labels for the given input data using the voting mechanism.\n\nArgs:\n    X (pd.DataFrame): Input data on which to predict.\n\nReturns:\n    np.ndarray: Predicted class labels."
                                }
                            ]
                        },
                        "path": "src/algorithms/tree_ensemble/forest_voting_methods.py"
                    }
                ]
            },
            {
                "node": "Unsupervised Learning",
                "feature_path": "Algorithms/Unsupervised Learning",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/unsupervised"
                    }
                ]
            },
            {
                "node": "Anomaly and Self-Organizing",
                "feature_path": "Algorithms/Unsupervised Learning/Anomaly and Self-Organizing",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/unsupervised"
                    }
                ]
            },
            {
                "node": "Anomaly Detection",
                "feature_path": "Algorithms/Unsupervised Learning/Anomaly and Self-Organizing/Anomaly Detection",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/unsupervised/anomaly_detection.py"
                    }
                ]
            },
            {
                "node": "isolation forest",
                "feature_path": "Algorithms/Unsupervised Learning/Anomaly and Self-Organizing/Anomaly Detection/isolation forest",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "IsolationForestAnomalyDetector",
                        "extra": {
                            "docstring": "Anomaly detector using the Isolation Forest algorithm.\n\nThis class implements the Isolation Forest approach for unsupervised anomaly detection.\nThe algorithm isolates anomalies instead of profiling normal data points, making it effective\nfor handling high-dimensional datasets.\n\nMethods:\n    __init__(n_estimators: int = 100, contamination: float = 0.1, max_samples: Optional[int] = None)\n        Initializes the Isolation Forest with the specified number of trees (estimators), expected\n        contamination (proportion of anomalies), and the number of samples to draw from X to train each tree.\n\n    fit(X: pd.DataFrame) -> None:\n        Build the isolation forest on the training data.\n\n    predict(X: pd.DataFrame) -> np.ndarray:\n        Predict whether the samples are anomalies, returning an array where anomalies are usually\n        marked with -1.\n\nArgs:\n    n_estimators (int): The number of base estimators in the ensemble (default is 100).\n    contamination (float): The expected proportion of outliers in the data (default is 0.1).\n    max_samples (Optional[int]): The number of samples to draw to train each base estimator. If None,\n                                 the maximum available samples are used.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_estimators",
                                        "contamination",
                                        "max_samples"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fit the Isolation Forest model on the training data.\n\nArgs:\n    X (pd.DataFrame): The input training data.\n\nReturns:\n    None"
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predict anomalies using the Isolation Forest model.\n\nArgs:\n    X (pd.DataFrame): Data for which anomaly detection is to be performed.\n\nReturns:\n    np.ndarray: An array with predictions where typically -1 indicates an anomaly and 1 indicates normal."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/anomaly_detection.py"
                    }
                ]
            },
            {
                "node": "local outlier factor",
                "feature_path": "Algorithms/Unsupervised Learning/Anomaly and Self-Organizing/Anomaly Detection/local outlier factor",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "LocalOutlierFactorAnomalyDetector",
                        "extra": {
                            "docstring": "Anomaly detector using the Local Outlier Factor (LOF) algorithm.\n\nThis class implements the Local Outlier Factor method for detecting anomalies by measuring the\nlocal deviation of density of a given data point with respect to its neighbors. A lower density\ncompared to its neighbors indicates a potential outlier.\n\nMethods:\n    __init__(n_neighbors: int = 20, contamination: float = 0.1)\n        Initializes the LOF detector with the number of neighbors to use and the expected\n        proportion of anomalies.\n\n    fit(X: pd.DataFrame) -> None:\n        Compute the LOF scores from the training data.\n\n    predict(X: pd.DataFrame) -> np.ndarray:\n        Predict whether data instances are anomalies based on their LOF scores, with an output\n        array where -1 indicates an anomaly.\n\nArgs:\n    n_neighbors (int): The number of neighbors to use for computing the local density (default is 20).\n    contamination (float): The expected proportion of anomalies in the data (default is 0.1).",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_neighbors",
                                        "contamination"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fit the Local Outlier Factor model by computing neighbor densities.\n\nArgs:\n    X (pd.DataFrame): The input training data.\n\nReturns:\n    None"
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predict anomalies using the LOF model.\n\nArgs:\n    X (pd.DataFrame): Data for which anomaly detection is performed.\n\nReturns:\n    np.ndarray: An array with predictions where -1 indicates an anomaly and 1 indicates a normal point."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/anomaly_detection.py"
                    }
                ]
            },
            {
                "node": "one-class svm",
                "feature_path": "Algorithms/Unsupervised Learning/Anomaly and Self-Organizing/Anomaly Detection/one-class svm",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "OneClassSVMAnomalyDetector",
                        "extra": {
                            "docstring": "Anomaly detector using the One-Class SVM algorithm.\n\nThis class implements the one-class SVM approach for detecting anomalies in an unsupervised\nlearning setting. It is designed to learn the boundary of normal data points during training\nand subsequently predict whether a new observation is an anomaly.\n\nMethods:\n    __init__(kernel: str = 'rbf', nu: float = 0.5, gamma: Optional[float] = None)\n        Initializes the anomaly detector with parameters for the SVM kernel, the nu parameter,\n        and the gamma value.\n\n    fit(X: pd.DataFrame) -> None:\n        Train the anomaly detector on the provided dataset.\n\n    predict(X: pd.DataFrame) -> np.ndarray:\n        Predict if the samples in the dataset are anomalies. Returns an array where typically\n        -1 indicates an anomaly and 1 indicates a normal data point.\n\nArgs:\n    kernel (str): Specifies the kernel type to be used in the algorithm (default is 'rbf').\n    nu (float): An upper bound on the fraction of training errors and a lower bound of the fraction\n                of support vectors (default is 0.5).\n    gamma (Optional[float]): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. If None, it is set\n                             to 'scale' by convention.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "kernel",
                                        "nu",
                                        "gamma"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fit the One-Class SVM model on the training data.\n\nArgs:\n    X (pd.DataFrame): The input training data.\n\nReturns:\n    None"
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predict anomalies in the provided data.\n\nArgs:\n    X (pd.DataFrame): Data to be evaluated for anomalies.\n\nReturns:\n    np.ndarray: An array with predicted labels (e.g., 1 for normal, -1 for anomaly)."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/anomaly_detection.py"
                    }
                ]
            },
            {
                "node": "Self-Organizing Maps",
                "feature_path": "Algorithms/Unsupervised Learning/Anomaly and Self-Organizing/Self-Organizing Maps",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/unsupervised/self_organizing_maps.py"
                    }
                ]
            },
            {
                "node": "learning rate",
                "feature_path": "Algorithms/Unsupervised Learning/Anomaly and Self-Organizing/Self-Organizing Maps/learning rate",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "SelfOrganizingMap",
                        "extra": {
                            "docstring": "Represents a Self-Organizing Map (SOM) with adjustable learning rate and topology preservation mechanisms.\n\nThis class encapsulates the functionality required to train a Self-Organizing Map. It allows the\nspecification of an initial learning rate, which controls the magnitude of weight updates during training, \nand includes functionality to maintain and assess topology preservation, ensuring that the spatial relationships \nin the input data are maintained in the resulting map.\n\nAttributes:\n    learning_rate (float): The initial learning rate to be used during training.\n    topology_preservation (bool): A flag indicating whether topology preservation mechanisms should be applied.\n    map_dimensions (tuple): The dimensions of the SOM grid (e.g., number of rows and columns).",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "learning_rate",
                                        "topology_preservation",
                                        "map_dimensions"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the Self-Organizing Map with specified learning rate and topology preservation settings.\n\nArgs:\n    learning_rate (float): The initial learning rate for the training process. Must be a positive float.\n    topology_preservation (bool): Determines if topology preservation is enforced during training.\n    map_dimensions (tuple, optional): Dimensions of the SOM grid as (rows, columns). Defaults to (10, 10).\n\nRaises:\n    ValueError: If learning_rate is not positive or if map_dimensions is not a valid tuple."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data",
                                        "num_iterations"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Train the Self-Organizing Map on the provided dataset.\n\nThe fit method updates the internal map (i.e., weights) based on the input data over a number of iterations.\nThe learning rate may be adjusted during the training, and topology preservation is maintained through\nappropriate update rules.\n\nArgs:\n    data (np.ndarray): A 2D numpy array representing the training input features.\n    num_iterations (int): The total number of iterations for training the SOM.\n\nReturns:\n    None\n\nRaises:\n    ValueError: If the input data is not in the expected format or if num_iterations is not a positive integer."
                                },
                                {
                                    "name": "get_topology_preservation_metric",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": "float",
                                    "docstring": "Compute and return a metric measuring the quality of topology preservation in the SOM.\n\nThis method evaluates how well the SOM maintains the relative spatial relationships of the input data.\nA higher value typically indicates better preservation of the topological structure.\n\nReturns:\n    float: A numerical value representing the quality of topology preservation."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/self_organizing_maps.py"
                    }
                ]
            },
            {
                "node": "topology preservation",
                "feature_path": "Algorithms/Unsupervised Learning/Anomaly and Self-Organizing/Self-Organizing Maps/topology preservation",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "SelfOrganizingMap",
                        "extra": {
                            "docstring": "Represents a Self-Organizing Map (SOM) with adjustable learning rate and topology preservation mechanisms.\n\nThis class encapsulates the functionality required to train a Self-Organizing Map. It allows the\nspecification of an initial learning rate, which controls the magnitude of weight updates during training, \nand includes functionality to maintain and assess topology preservation, ensuring that the spatial relationships \nin the input data are maintained in the resulting map.\n\nAttributes:\n    learning_rate (float): The initial learning rate to be used during training.\n    topology_preservation (bool): A flag indicating whether topology preservation mechanisms should be applied.\n    map_dimensions (tuple): The dimensions of the SOM grid (e.g., number of rows and columns).",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "learning_rate",
                                        "topology_preservation",
                                        "map_dimensions"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the Self-Organizing Map with specified learning rate and topology preservation settings.\n\nArgs:\n    learning_rate (float): The initial learning rate for the training process. Must be a positive float.\n    topology_preservation (bool): Determines if topology preservation is enforced during training.\n    map_dimensions (tuple, optional): Dimensions of the SOM grid as (rows, columns). Defaults to (10, 10).\n\nRaises:\n    ValueError: If learning_rate is not positive or if map_dimensions is not a valid tuple."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data",
                                        "num_iterations"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Train the Self-Organizing Map on the provided dataset.\n\nThe fit method updates the internal map (i.e., weights) based on the input data over a number of iterations.\nThe learning rate may be adjusted during the training, and topology preservation is maintained through\nappropriate update rules.\n\nArgs:\n    data (np.ndarray): A 2D numpy array representing the training input features.\n    num_iterations (int): The total number of iterations for training the SOM.\n\nReturns:\n    None\n\nRaises:\n    ValueError: If the input data is not in the expected format or if num_iterations is not a positive integer."
                                },
                                {
                                    "name": "get_topology_preservation_metric",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": "float",
                                    "docstring": "Compute and return a metric measuring the quality of topology preservation in the SOM.\n\nThis method evaluates how well the SOM maintains the relative spatial relationships of the input data.\nA higher value typically indicates better preservation of the topological structure.\n\nReturns:\n    float: A numerical value representing the quality of topology preservation."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/self_organizing_maps.py"
                    }
                ]
            },
            {
                "node": "Clustering",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/unsupervised/clustering"
                    }
                ]
            },
            {
                "node": "Hierarchical & Density Clustering",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/Hierarchical & Density Clustering",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/unsupervised/clustering/hierarchical_density.py"
                    }
                ]
            },
            {
                "node": "agglomerative",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/Hierarchical & Density Clustering/agglomerative",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "AgglomerativeClustering",
                        "extra": {
                            "docstring": "Interface for performing agglomerative hierarchical clustering.\n\nThis class implements the agglomerative (bottom-up) clustering strategy by merging the\nclosest clusters iteratively based on a chosen linkage criterion. It supports extracting\nthe final cluster labels as well as generating a dendrogram representation of the hierarchical structure.\n\nArgs:\n    linkage (str): The linkage criterion to use (e.g., 'ward', 'complete', 'average').\n    distance_metric (str, optional): The metric to measure distance between data points. Defaults to 'euclidean'.\n\nMethods:\n    fit(data: np.ndarray) -> None:\n        Perform the agglomerative clustering on the input data.\n    predict(data: np.ndarray) -> np.ndarray:\n        Return cluster labels for the input data based on the fitted model.\n    get_dendrogram() -> Any:\n        Retrieve a dendrogram structure representing the hierarchical cluster merge history.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "linkage",
                                        "distance_metric"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fit the agglomerative clustering model on the provided dataset.\n\nArgs:\n    data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\nReturns:\n    None\n\nNotes:\n    This method should execute the bottom-up merging process based on the specified linkage."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predict cluster labels for the input data using the agglomerative clustering model.\n\nArgs:\n    data (np.ndarray): Data as a NumPy array for which cluster labels are required.\n\nReturns:\n    np.ndarray: An array of cluster labels corresponding to each sample."
                                },
                                {
                                    "name": "get_dendrogram",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": "Any",
                                    "docstring": "Retrieve the dendrogram structure representing the hierarchical clustering tree.\n\nReturns:\n    Any: An object representing the dendrogram, which may be used for visualization or further analysis."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/hierarchical_density.py"
                    }
                ]
            },
            {
                "node": "density-based clustering",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/Hierarchical & Density Clustering/density-based clustering",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "DensityBasedClustering",
                        "extra": {
                            "docstring": "Interface for performing density-based clustering.\n\nThis class provides a framework to implement density-based clustering techniques,\nsuch as DBSCAN or similar algorithms that group data based on the local density of points.\nIt encapsulates configuration parameters such as the neighborhood radius and the minimum number\nof points required to form a cluster.\n\nArgs:\n    eps (float): The radius within which to search for neighboring points.\n    min_samples (int): The minimum number of points required to form a dense region.\n\nMethods:\n    fit(data: np.ndarray) -> None:\n        Compute the clustering structure from the input data.\n    predict(data: np.ndarray) -> np.ndarray:\n        Assign cluster labels to new or existing data points based on the learned clustering structure.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "eps",
                                        "min_samples"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fit the density-based clustering model on the provided data.\n\nArgs:\n    data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\nReturns:\n    None\n\nNotes:\n    The method should compute the clustering structure by analyzing point density."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predict cluster labels for the provided data with the density-based clustering model.\n\nArgs:\n    data (np.ndarray): New or training data as a NumPy array with shape (n_samples, n_features).\n\nReturns:\n    np.ndarray: An array of cluster labels, where noise points may be labeled as -1.\n\nNotes:\n    The output labels should be consistent with the clustering computed in the fit method."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/hierarchical_density.py"
                    }
                ]
            },
            {
                "node": "divisive",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/Hierarchical & Density Clustering/divisive",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "DivisiveClustering",
                        "extra": {
                            "docstring": "Interface for performing divisive hierarchical clustering.\n\nThis class implements the divisive (top-down) clustering strategy by recursively splitting\nthe dataset into clusters based on specified criteria. It provides methods to obtain cluster\nlabels and to generate a dendrogram that represents the recursive division of the data.\n\nArgs:\n    criterion (Any, optional): The criterion for splitting clusters (e.g., maximization of between-cluster variance).\n    distance_metric (str, optional): The metric used for evaluating splits. Defaults to 'euclidean'.\n\nMethods:\n    fit(data: np.ndarray) -> None:\n        Perform the divisive clustering on the input data.\n    predict(data: np.ndarray) -> np.ndarray:\n        Return cluster labels for the input data based on the fitted model.\n    get_dendrogram() -> Any:\n        Retrieve a dendrogram that illustrates the hierarchical splits made by the model.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "criterion",
                                        "distance_metric"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fit the divisive clustering model on the provided dataset by recursively splitting the data.\n\nArgs:\n    data (np.ndarray): Input data as a NumPy array with shape (n_samples, n_features).\n\nReturns:\n    None\n\nNotes:\n    This method should implement a top-down clustering approach, recursively dividing the data."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predict cluster labels for the given data using the divisive clustering model.\n\nArgs:\n    data (np.ndarray): Data as a NumPy array for which cluster labels are to be computed.\n\nReturns:\n    np.ndarray: An array of cluster labels reflecting the outcome of the divisive clustering."
                                },
                                {
                                    "name": "get_dendrogram",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": "Any",
                                    "docstring": "Retrieve the hierarchical dendrogram representing the splits generated by the divisive clustering process.\n\nReturns:\n    Any: A dendrogram object that can be used to analyze or visualize the hierarchical structure."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/hierarchical_density.py"
                    }
                ]
            },
            {
                "node": "K-means Clustering",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "centroid initialization",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering/centroid initialization",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KMeansInitializer",
                        "extra": {
                            "docstring": "Provides various centroid initialization methods for K-means clustering.\n\nThis class encapsulates methods for initializing cluster centroids using different strategies including:\n- Generic initialization methods\n- Direct centroid initialization\n- K-means++ initialization for improved convergence.\n\nMethods should be used to set up the initial state for clustering algorithms by selecting starting centroids.\n\nUsage:\n    initializer = KMeansInitializer()\n    centroids = initializer.initialize(data, method=\"k-means++\", num_clusters=3)\n\nArgs:\n    None\n\nReturns:\n    Various initialization outputs as numpy.ndarray or list of centroids.\n\nEdge Cases:\n    Implementations should handle cases where the input data is insufficient or contains NaN values.",
                            "methods": [
                                {
                                    "name": "initialize",
                                    "args": [
                                        "self",
                                        "data",
                                        "method",
                                        "num_clusters"
                                    ],
                                    "return_type": "Any",
                                    "docstring": "Initialize cluster centroids for k-means clustering.\n\nArgs:\n    data (np.ndarray): The input data array for clustering.\n    method (str): The initialization method (\"default\", \"centroid\", \"k-means++\").\n    num_clusters (int): Number of clusters for which to initialize centroids.\n    \nReturns:\n    Any: The initialized centroids (format can vary based on method)."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "convergence criteria",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering/convergence criteria",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KMeansOptimizer",
                        "extra": {
                            "docstring": "Provides utilities to assess convergence, evaluate the elbow method, and optimize the number of clusters \nfor k-means based clustering algorithms.\n\nThis class offers methods to:\n- Check convergence criteria during iterative updates.\n- Compute the elbow metric to help in identifying the optimal number of clusters.\n- Optimize the cluster count automatically based on provided metrics.\n\nUsage:\n    optimizer = KMeansOptimizer()\n    is_converged = optimizer.check_convergence(old_centroids, new_centroids, tol=0.001)\n    elbow_score = optimizer.compute_elbow_method(data, max_k=10)\n    optimal_k = optimizer.optimize_clusters(data, max_k=10)\n\nArgs:\n    None\n\nReturns:\n    Convergence flags, elbow scores, or optimal cluster counts as integers or floats.\n\nEdge Cases:\n    Methods should effectively handle edge cases such as not reaching convergence within a fixed number of iterations.",
                            "methods": [
                                {
                                    "name": "check_convergence",
                                    "args": [
                                        "self",
                                        "old_centroids",
                                        "new_centroids",
                                        "tol"
                                    ],
                                    "return_type": "bool",
                                    "docstring": "Checks whether the centroids have converged within a specified tolerance.\n\nArgs:\n    old_centroids (np.ndarray): Centroid positions from the previous iteration.\n    new_centroids (np.ndarray): Updated centroid positions.\n    tol (float): Tolerance threshold for convergence.\n    \nReturns:\n    bool: True if the centroids have converged; False otherwise."
                                },
                                {
                                    "name": "compute_elbow_method",
                                    "args": [
                                        "self",
                                        "data",
                                        "max_k"
                                    ],
                                    "return_type": "Dict[int, float]",
                                    "docstring": "Computes the elbow metric for a range of clusters to aid in identifying the optimal number of clusters.\n\nArgs:\n    data (np.ndarray): Input data for computing the metric.\n    max_k (int): Maximum number of clusters to consider.\n    \nReturns:\n    Dict[int, float]: Mapping of number of clusters to computed elbow values."
                                },
                                {
                                    "name": "optimize_clusters",
                                    "args": [
                                        "self",
                                        "data",
                                        "max_k"
                                    ],
                                    "return_type": "int",
                                    "docstring": "Determines the optimal number of clusters based on optimization criteria.\n\nArgs:\n    data (np.ndarray): Input data for clustering.\n    max_k (int): Maximum number of clusters to be evaluated.\n    \nReturns:\n    int: The optimal number of clusters."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "elbow method",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering/elbow method",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KMeansOptimizer",
                        "extra": {
                            "docstring": "Provides utilities to assess convergence, evaluate the elbow method, and optimize the number of clusters \nfor k-means based clustering algorithms.\n\nThis class offers methods to:\n- Check convergence criteria during iterative updates.\n- Compute the elbow metric to help in identifying the optimal number of clusters.\n- Optimize the cluster count automatically based on provided metrics.\n\nUsage:\n    optimizer = KMeansOptimizer()\n    is_converged = optimizer.check_convergence(old_centroids, new_centroids, tol=0.001)\n    elbow_score = optimizer.compute_elbow_method(data, max_k=10)\n    optimal_k = optimizer.optimize_clusters(data, max_k=10)\n\nArgs:\n    None\n\nReturns:\n    Convergence flags, elbow scores, or optimal cluster counts as integers or floats.\n\nEdge Cases:\n    Methods should effectively handle edge cases such as not reaching convergence within a fixed number of iterations.",
                            "methods": [
                                {
                                    "name": "check_convergence",
                                    "args": [
                                        "self",
                                        "old_centroids",
                                        "new_centroids",
                                        "tol"
                                    ],
                                    "return_type": "bool",
                                    "docstring": "Checks whether the centroids have converged within a specified tolerance.\n\nArgs:\n    old_centroids (np.ndarray): Centroid positions from the previous iteration.\n    new_centroids (np.ndarray): Updated centroid positions.\n    tol (float): Tolerance threshold for convergence.\n    \nReturns:\n    bool: True if the centroids have converged; False otherwise."
                                },
                                {
                                    "name": "compute_elbow_method",
                                    "args": [
                                        "self",
                                        "data",
                                        "max_k"
                                    ],
                                    "return_type": "Dict[int, float]",
                                    "docstring": "Computes the elbow metric for a range of clusters to aid in identifying the optimal number of clusters.\n\nArgs:\n    data (np.ndarray): Input data for computing the metric.\n    max_k (int): Maximum number of clusters to consider.\n    \nReturns:\n    Dict[int, float]: Mapping of number of clusters to computed elbow values."
                                },
                                {
                                    "name": "optimize_clusters",
                                    "args": [
                                        "self",
                                        "data",
                                        "max_k"
                                    ],
                                    "return_type": "int",
                                    "docstring": "Determines the optimal number of clusters based on optimization criteria.\n\nArgs:\n    data (np.ndarray): Input data for clustering.\n    max_k (int): Maximum number of clusters to be evaluated.\n    \nReturns:\n    int: The optimal number of clusters."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "fuzzy k-means",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering/fuzzy k-means",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "FuzzyKMeansClustering",
                        "extra": {
                            "docstring": "Implements the fuzzy k-means (soft clustering) algorithm where each data point can belong\nto multiple clusters with varying degrees of membership.\n\nUsage:\n    fuzzy_kmeans = FuzzyKMeansClustering(num_clusters=3, m=2.0, max_iter=300)\n    memberships = fuzzy_kmeans.fit_predict(data)\n\nArgs:\n    num_clusters (int): Number of clusters.\n    m (float): Fuzziness parameter that controls the degree of membership sharing.\n    max_iter (int): Maximum number of iterations for convergence.\n\nReturns:\n    np.ndarray: Membership matrix indicating the degree of belonging to each cluster.\n\nEdge Cases:\n    Should handle cases where the fuzziness parameter may lead to degenerate membership assignments.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "num_clusters",
                                        "m",
                                        "max_iter"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fits the fuzzy k-means model to the input data.\n\nArgs:\n    data (np.ndarray): Input data for clustering.\n    \nReturns:\n    None"
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predicts the fuzzy cluster memberships for the provided data.\n\nArgs:\n    data (np.ndarray): Data for which to compute cluster memberships.\n    \nReturns:\n    np.ndarray: A membership matrix where each row sums to 1."
                                },
                                {
                                    "name": "fit_predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Combines fitting and predicting of fuzzy cluster memberships.\n\nArgs:\n    data (np.ndarray): Input data for fuzzy clustering.\n    \nReturns:\n    np.ndarray: The membership matrix."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "initialization methods",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering/initialization methods",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KMeansInitializer",
                        "extra": {
                            "docstring": "Provides various centroid initialization methods for K-means clustering.\n\nThis class encapsulates methods for initializing cluster centroids using different strategies including:\n- Generic initialization methods\n- Direct centroid initialization\n- K-means++ initialization for improved convergence.\n\nMethods should be used to set up the initial state for clustering algorithms by selecting starting centroids.\n\nUsage:\n    initializer = KMeansInitializer()\n    centroids = initializer.initialize(data, method=\"k-means++\", num_clusters=3)\n\nArgs:\n    None\n\nReturns:\n    Various initialization outputs as numpy.ndarray or list of centroids.\n\nEdge Cases:\n    Implementations should handle cases where the input data is insufficient or contains NaN values.",
                            "methods": [
                                {
                                    "name": "initialize",
                                    "args": [
                                        "self",
                                        "data",
                                        "method",
                                        "num_clusters"
                                    ],
                                    "return_type": "Any",
                                    "docstring": "Initialize cluster centroids for k-means clustering.\n\nArgs:\n    data (np.ndarray): The input data array for clustering.\n    method (str): The initialization method (\"default\", \"centroid\", \"k-means++\").\n    num_clusters (int): Number of clusters for which to initialize centroids.\n    \nReturns:\n    Any: The initialized centroids (format can vary based on method)."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "k-means clustering",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering/k-means clustering",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KMeansClustering",
                        "extra": {
                            "docstring": "Implements the standard K-means clustering algorithm.\n\nThis class is responsible for performing the standard iterative clustering process, including:\n- Assignment of data points to the nearest centroids.\n- Updating centroid positions based on cluster membership.\n\nUsage:\n    kmeans = KMeansClustering(num_clusters=3, max_iter=300)\n    cluster_labels = kmeans.fit_predict(data)\n\nArgs:\n    num_clusters (int): Number of clusters to form.\n    max_iter (int): Maximum iterations for the algorithm.\n\nReturns:\n    fit_predict returns cluster labels as a numpy.ndarray.\n\nEdge Cases:\n    The implementation should handle cases with empty clusters, convergence conditions, and data containing outliers.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "num_clusters",
                                        "max_iter"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fits the K-means clustering on the provided data.\n\nArgs:\n    data (np.ndarray): Input data for clustering.\n\nReturns:\n    None"
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predicts the cluster labels for the provided data.\n\nArgs:\n    data (np.ndarray): Data for which to determine cluster membership.\n    \nReturns:\n    np.ndarray: Array of predicted cluster labels."
                                },
                                {
                                    "name": "fit_predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Combines fit and predict steps for convenience.\n\nArgs:\n    data (np.ndarray): Input data for clustering.\n\nReturns:\n    np.ndarray: Cluster labels for the input data."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "k-means++ initialization",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering/k-means++ initialization",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KMeansInitializer",
                        "extra": {
                            "docstring": "Provides various centroid initialization methods for K-means clustering.\n\nThis class encapsulates methods for initializing cluster centroids using different strategies including:\n- Generic initialization methods\n- Direct centroid initialization\n- K-means++ initialization for improved convergence.\n\nMethods should be used to set up the initial state for clustering algorithms by selecting starting centroids.\n\nUsage:\n    initializer = KMeansInitializer()\n    centroids = initializer.initialize(data, method=\"k-means++\", num_clusters=3)\n\nArgs:\n    None\n\nReturns:\n    Various initialization outputs as numpy.ndarray or list of centroids.\n\nEdge Cases:\n    Implementations should handle cases where the input data is insufficient or contains NaN values.",
                            "methods": [
                                {
                                    "name": "initialize",
                                    "args": [
                                        "self",
                                        "data",
                                        "method",
                                        "num_clusters"
                                    ],
                                    "return_type": "Any",
                                    "docstring": "Initialize cluster centroids for k-means clustering.\n\nArgs:\n    data (np.ndarray): The input data array for clustering.\n    method (str): The initialization method (\"default\", \"centroid\", \"k-means++\").\n    num_clusters (int): Number of clusters for which to initialize centroids.\n    \nReturns:\n    Any: The initialized centroids (format can vary based on method)."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "k-medoids",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering/k-medoids",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KMedoidsClustering",
                        "extra": {
                            "docstring": "Implements the k-medoids clustering algorithm, an alternative to k-means that selects actual data points as centers.\n\nUsage:\n    kmedoids = KMedoidsClustering(num_clusters=3, max_iter=300)\n    labels = kmedoids.fit_predict(data)\n\nArgs:\n    num_clusters (int): Number of clusters.\n    max_iter (int): Maximum number of iterations.\n\nReturns:\n    np.ndarray: Predicted cluster labels.\n\nEdge Cases:\n    Should handle data with outliers robustly by selecting medoids that minimize dissimilarity.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "num_clusters",
                                        "max_iter"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fits the k-medoids model to the input data.\n\nArgs:\n    data (np.ndarray): Data used for clustering.\n\nReturns:\n    None"
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Predicts cluster membership for input data based on medoid distances.\n\nArgs:\n    data (np.ndarray): Input data.\n    \nReturns:\n    np.ndarray: Cluster labels."
                                },
                                {
                                    "name": "fit_predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Executes fitting and prediction in a single step.\n\nArgs:\n    data (np.ndarray): Data for clustering.\n    \nReturns:\n    np.ndarray: Cluster assignments."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "mini-batch k-means",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering/mini-batch k-means",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "MiniBatchKMeansClustering",
                        "extra": {
                            "docstring": "Implements the Mini-Batch K-means clustering algorithm.\n\nThis variant follows the standard k-means algorithm but uses mini-batches to reduce computation time.\n\nUsage:\n    mbkmeans = MiniBatchKMeansClustering(num_clusters=3, batch_size=100)\n    labels = mbkmeans.fit_predict(data)\n\nArgs:\n    num_clusters (int): Number of clusters.\n    batch_size (int): Size of the mini-batch.\n    max_iter (int): Maximum number of iterations.\n\nReturns:\n    Cluster labels as a numpy.ndarray from fit_predict.\n\nEdge Cases:\n    The implementation should address cases where batch size exceeds dataset length and ensure consistent convergence.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "num_clusters",
                                        "batch_size",
                                        "max_iter"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fits the Mini-Batch K-means model on provided data.\n\nArgs:\n    data (np.ndarray): Input data for clustering.\n\nReturns:\n    None"
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Assigns data points to the nearest cluster centroid based on the fitted model.\n\nArgs:\n    data (np.ndarray): Data for clustering prediction.\n\nReturns:\n    np.ndarray: Predicted cluster labels."
                                },
                                {
                                    "name": "fit_predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Combines the fit and predict operations.\n\nArgs:\n    data (np.ndarray): Input data for clustering.\n    \nReturns:\n    np.ndarray: Cluster labels derived from the model."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "optimize clusters",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering/optimize clusters",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KMeansOptimizer",
                        "extra": {
                            "docstring": "Provides utilities to assess convergence, evaluate the elbow method, and optimize the number of clusters \nfor k-means based clustering algorithms.\n\nThis class offers methods to:\n- Check convergence criteria during iterative updates.\n- Compute the elbow metric to help in identifying the optimal number of clusters.\n- Optimize the cluster count automatically based on provided metrics.\n\nUsage:\n    optimizer = KMeansOptimizer()\n    is_converged = optimizer.check_convergence(old_centroids, new_centroids, tol=0.001)\n    elbow_score = optimizer.compute_elbow_method(data, max_k=10)\n    optimal_k = optimizer.optimize_clusters(data, max_k=10)\n\nArgs:\n    None\n\nReturns:\n    Convergence flags, elbow scores, or optimal cluster counts as integers or floats.\n\nEdge Cases:\n    Methods should effectively handle edge cases such as not reaching convergence within a fixed number of iterations.",
                            "methods": [
                                {
                                    "name": "check_convergence",
                                    "args": [
                                        "self",
                                        "old_centroids",
                                        "new_centroids",
                                        "tol"
                                    ],
                                    "return_type": "bool",
                                    "docstring": "Checks whether the centroids have converged within a specified tolerance.\n\nArgs:\n    old_centroids (np.ndarray): Centroid positions from the previous iteration.\n    new_centroids (np.ndarray): Updated centroid positions.\n    tol (float): Tolerance threshold for convergence.\n    \nReturns:\n    bool: True if the centroids have converged; False otherwise."
                                },
                                {
                                    "name": "compute_elbow_method",
                                    "args": [
                                        "self",
                                        "data",
                                        "max_k"
                                    ],
                                    "return_type": "Dict[int, float]",
                                    "docstring": "Computes the elbow metric for a range of clusters to aid in identifying the optimal number of clusters.\n\nArgs:\n    data (np.ndarray): Input data for computing the metric.\n    max_k (int): Maximum number of clusters to consider.\n    \nReturns:\n    Dict[int, float]: Mapping of number of clusters to computed elbow values."
                                },
                                {
                                    "name": "optimize_clusters",
                                    "args": [
                                        "self",
                                        "data",
                                        "max_k"
                                    ],
                                    "return_type": "int",
                                    "docstring": "Determines the optimal number of clusters based on optimization criteria.\n\nArgs:\n    data (np.ndarray): Input data for clustering.\n    max_k (int): Maximum number of clusters to be evaluated.\n    \nReturns:\n    int: The optimal number of clusters."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "spectral clustering",
                "feature_path": "Algorithms/Unsupervised Learning/Clustering/K-means Clustering/spectral clustering",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "SpectralClustering",
                        "extra": {
                            "docstring": "Implements the spectral clustering method in the context of k-means clustering based workflows.\n\nThis algorithm uses eigen decomposition on a similarity matrix to reduce dimensionality before clustering.\n\nUsage:\n    spectral = SpectralClustering(num_clusters=3, affinity=\"rbf\")\n    labels = spectral.fit_predict(data)\n\nArgs:\n    num_clusters (int): Desired number of clusters.\n    affinity (str): Affinity type to construct the similarity matrix.\n    n_neighbors (int, optional): Number of neighbors for graph construction.\n\nReturns:\n    Cluster labels as a numpy.ndarray.\n\nEdge Cases:\n    The method should handle situations where the similarity matrix fails to capture structure in the data.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "num_clusters",
                                        "affinity",
                                        "n_neighbors"
                                    ],
                                    "return_type": "None",
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fits the spectral clustering model on the input data.\n\nArgs:\n    data (np.ndarray): Data for clustering.\n    \nReturns:\n    None"
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Assigns cluster labels based on the spectral clustering.\n\nArgs:\n    data (np.ndarray): Data for which clustering is to be predicted.\n    \nReturns:\n    np.ndarray: Predicted cluster labels."
                                },
                                {
                                    "name": "fit_predict",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "np.ndarray",
                                    "docstring": "Combines fitting and predicting into a single step.\n\nArgs:\n    data (np.ndarray): Data to be clustered.\n    \nReturns:\n    np.ndarray: Cluster labels for the input data."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/clustering/k_means.py"
                    }
                ]
            },
            {
                "node": "Dimensionality Reduction",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/algorithms/unsupervised/dimensionality_reduction"
                    }
                ]
            },
            {
                "node": "Other Reduction Methods",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/other_methods.py"
                    }
                ]
            },
            {
                "node": "latent topics",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/latent topics",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "LDATopicModeler",
                        "extra": {
                            "docstring": "LDATopicModeler performs topic modeling using Latent Dirichlet Allocation (LDA).\n\nThis class applies LDA to extract latent topics from a corpus or a feature data matrix and\nprovides functionality to retrieve both the latent topics and the topic distribution across\ndocuments/samples. It is primarily used for uncovering underlying topics in text or feature data.\n\nAttributes:\n    n_topics (int): The number of latent topics to extract.\n    random_state (Optional[int]): Seed used by the random number generator.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_topics",
                                        "random_state"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the LDA modeler with specified parameters.\n\nArgs:\n    n_topics (int): The number of topics to extract.\n    random_state (Optional[int]): Seed for reproducibility."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fit the LDA model on the given data.\n\nArgs:\n    data (pd.DataFrame or np.ndarray): Data matrix where rows correspond to samples/documents\n      and columns correspond to features (e.g., word counts or TF-IDF values)."
                                },
                                {
                                    "name": "get_latent_topics",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": "Union[pd.DataFrame, np.ndarray]",
                                    "docstring": "Retrieve the latent topic representation of the training data.\n\nReturns:\n    pd.DataFrame or np.ndarray: Matrix where each row corresponds to the topic distribution for a sample."
                                },
                                {
                                    "name": "get_topic_distribution",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": "Union[pd.DataFrame, np.ndarray]",
                                    "docstring": "Obtain the topic distribution over the vocabulary.\n\nReturns:\n    pd.DataFrame or np.ndarray: Matrix where each row contains the distribution of words for a topic."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/other_methods.py"
                    }
                ]
            },
            {
                "node": "lda",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/lda",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "LDATopicModeler",
                        "extra": {
                            "docstring": "LDATopicModeler performs topic modeling using Latent Dirichlet Allocation (LDA).\n\nThis class applies LDA to extract latent topics from a corpus or a feature data matrix and\nprovides functionality to retrieve both the latent topics and the topic distribution across\ndocuments/samples. It is primarily used for uncovering underlying topics in text or feature data.\n\nAttributes:\n    n_topics (int): The number of latent topics to extract.\n    random_state (Optional[int]): Seed used by the random number generator.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_topics",
                                        "random_state"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the LDA modeler with specified parameters.\n\nArgs:\n    n_topics (int): The number of topics to extract.\n    random_state (Optional[int]): Seed for reproducibility."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fit the LDA model on the given data.\n\nArgs:\n    data (pd.DataFrame or np.ndarray): Data matrix where rows correspond to samples/documents\n      and columns correspond to features (e.g., word counts or TF-IDF values)."
                                },
                                {
                                    "name": "get_latent_topics",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": "Union[pd.DataFrame, np.ndarray]",
                                    "docstring": "Retrieve the latent topic representation of the training data.\n\nReturns:\n    pd.DataFrame or np.ndarray: Matrix where each row corresponds to the topic distribution for a sample."
                                },
                                {
                                    "name": "get_topic_distribution",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": "Union[pd.DataFrame, np.ndarray]",
                                    "docstring": "Obtain the topic distribution over the vocabulary.\n\nReturns:\n    pd.DataFrame or np.ndarray: Matrix where each row contains the distribution of words for a topic."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/other_methods.py"
                    }
                ]
            },
            {
                "node": "min_dist parameter",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/min_dist parameter",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "UMAPReducer",
                        "extra": {
                            "docstring": "UMAPReducer performs dimensionality reduction using the UMAP algorithm.\n\nThis class implements the UMAP algorithm with configurable minimum distance and number of neighbors.\nIt is used to project high-dimensional data into a lower-dimensional space while preserving the\nunderlying structure according to the parameters provided.\n\nAttributes:\n    min_dist (float): The minimum distance between embedded points.\n    n_neighbors (int): The number of neighboring points used to balance local versus global structure.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "min_dist",
                                        "n_neighbors"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the UMAP reducer with specified parameters.\n\nArgs:\n    min_dist (float): The minimum distance between points in the embedded space.\n    n_neighbors (int): The number of neighbors to consider for each point."
                                },
                                {
                                    "name": "reduce",
                                    "args": [
                                        "self",
                                        "data",
                                        "n_components"
                                    ],
                                    "return_type": "Union[pd.DataFrame, np.ndarray]",
                                    "docstring": "Reduce the dimensionality of input data using UMAP.\n\nArgs:\n    data (pd.DataFrame or np.ndarray): The high-dimensional input data.\n    n_components (int): The target number of dimensions. Defaults to 2.\n\nReturns:\n    pd.DataFrame or np.ndarray: The dimensionally-reduced representation of the input data."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/other_methods.py"
                    }
                ]
            },
            {
                "node": "number of neighbors",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/number of neighbors",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "UMAPReducer",
                        "extra": {
                            "docstring": "UMAPReducer performs dimensionality reduction using the UMAP algorithm.\n\nThis class implements the UMAP algorithm with configurable minimum distance and number of neighbors.\nIt is used to project high-dimensional data into a lower-dimensional space while preserving the\nunderlying structure according to the parameters provided.\n\nAttributes:\n    min_dist (float): The minimum distance between embedded points.\n    n_neighbors (int): The number of neighboring points used to balance local versus global structure.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "min_dist",
                                        "n_neighbors"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the UMAP reducer with specified parameters.\n\nArgs:\n    min_dist (float): The minimum distance between points in the embedded space.\n    n_neighbors (int): The number of neighbors to consider for each point."
                                },
                                {
                                    "name": "reduce",
                                    "args": [
                                        "self",
                                        "data",
                                        "n_components"
                                    ],
                                    "return_type": "Union[pd.DataFrame, np.ndarray]",
                                    "docstring": "Reduce the dimensionality of input data using UMAP.\n\nArgs:\n    data (pd.DataFrame or np.ndarray): The high-dimensional input data.\n    n_components (int): The target number of dimensions. Defaults to 2.\n\nReturns:\n    pd.DataFrame or np.ndarray: The dimensionally-reduced representation of the input data."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/other_methods.py"
                    }
                ]
            },
            {
                "node": "t-sne",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/t-sne",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "TSNEReducer",
                        "extra": {
                            "docstring": "TSNEReducer performs dimensionality reduction using t-SNE.\n\nThis class encapsulates the t-SNE algorithm for mapping high-dimensional data to a lower-dimensional space.\nIt helps in visualizing clusters and patterns by minimizing the divergence between the high-dimensional and\nlow-dimensional distributions.\n\nAttributes:\n    n_components (int): The target number of dimensions.\n    perplexity (float): The perplexity parameter balances attention between local and global aspects.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_components",
                                        "perplexity"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the t-SNE reducer with provided parameters.\n\nArgs:\n    n_components (int): The number of dimensions for the output space.\n    perplexity (float): The perplexity parameter affecting the balance of local and global structure."
                                },
                                {
                                    "name": "reduce",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "Union[pd.DataFrame, np.ndarray]",
                                    "docstring": "Reduce the dimensionality of input data using t-SNE.\n\nArgs:\n    data (pd.DataFrame or np.ndarray): The high-dimensional data to be reduced.\n\nReturns:\n    pd.DataFrame or np.ndarray: Low-dimensional representation of the input data."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/other_methods.py"
                    }
                ]
            },
            {
                "node": "topic distribution",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/topic distribution",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "LDATopicModeler",
                        "extra": {
                            "docstring": "LDATopicModeler performs topic modeling using Latent Dirichlet Allocation (LDA).\n\nThis class applies LDA to extract latent topics from a corpus or a feature data matrix and\nprovides functionality to retrieve both the latent topics and the topic distribution across\ndocuments/samples. It is primarily used for uncovering underlying topics in text or feature data.\n\nAttributes:\n    n_topics (int): The number of latent topics to extract.\n    random_state (Optional[int]): Seed used by the random number generator.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_topics",
                                        "random_state"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the LDA modeler with specified parameters.\n\nArgs:\n    n_topics (int): The number of topics to extract.\n    random_state (Optional[int]): Seed for reproducibility."
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "data"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Fit the LDA model on the given data.\n\nArgs:\n    data (pd.DataFrame or np.ndarray): Data matrix where rows correspond to samples/documents\n      and columns correspond to features (e.g., word counts or TF-IDF values)."
                                },
                                {
                                    "name": "get_latent_topics",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": "Union[pd.DataFrame, np.ndarray]",
                                    "docstring": "Retrieve the latent topic representation of the training data.\n\nReturns:\n    pd.DataFrame or np.ndarray: Matrix where each row corresponds to the topic distribution for a sample."
                                },
                                {
                                    "name": "get_topic_distribution",
                                    "args": [
                                        "self"
                                    ],
                                    "return_type": "Union[pd.DataFrame, np.ndarray]",
                                    "docstring": "Obtain the topic distribution over the vocabulary.\n\nReturns:\n    pd.DataFrame or np.ndarray: Matrix where each row contains the distribution of words for a topic."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/other_methods.py"
                    }
                ]
            },
            {
                "node": "umap",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/Other Reduction Methods/umap",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "UMAPReducer",
                        "extra": {
                            "docstring": "UMAPReducer performs dimensionality reduction using the UMAP algorithm.\n\nThis class implements the UMAP algorithm with configurable minimum distance and number of neighbors.\nIt is used to project high-dimensional data into a lower-dimensional space while preserving the\nunderlying structure according to the parameters provided.\n\nAttributes:\n    min_dist (float): The minimum distance between embedded points.\n    n_neighbors (int): The number of neighboring points used to balance local versus global structure.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "min_dist",
                                        "n_neighbors"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the UMAP reducer with specified parameters.\n\nArgs:\n    min_dist (float): The minimum distance between points in the embedded space.\n    n_neighbors (int): The number of neighbors to consider for each point."
                                },
                                {
                                    "name": "reduce",
                                    "args": [
                                        "self",
                                        "data",
                                        "n_components"
                                    ],
                                    "return_type": "Union[pd.DataFrame, np.ndarray]",
                                    "docstring": "Reduce the dimensionality of input data using UMAP.\n\nArgs:\n    data (pd.DataFrame or np.ndarray): The high-dimensional input data.\n    n_components (int): The target number of dimensions. Defaults to 2.\n\nReturns:\n    pd.DataFrame or np.ndarray: The dimensionally-reduced representation of the input data."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/other_methods.py"
                    }
                ]
            },
            {
                "node": "PCA Methods",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/PCA Methods",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py"
                    }
                ]
            },
            {
                "node": "dimensionality reduction",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/PCA Methods/dimensionality reduction",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "StandardPCA",
                        "extra": {
                            "docstring": "Perform standard Principal Component Analysis (PCA) using eigen or singular value decomposition.\n\nThis interface implements the classical PCA algorithm for dimensionality reduction. It computes the principal\ncomponents of the data via eigen decomposition or singular value decomposition, depending on the properties of\nthe input matrix. The resulting components can be used to reduce the dimensionality while preserving variance.\n\nMethods:\n    fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'StandardPCA':\n        Compute the principal components from the input data.\n    transform(X: pd.DataFrame) -> pd.DataFrame:\n        Project new data onto the principal component space.\n\nArgs:\n    n_components (int): The number of principal components to compute.\n    method (str): The decomposition method ('eigen' or 'svd') to use; the selection may affect numerical stability.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_components",
                                        "method"
                                    ],
                                    "return_type": null,
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'StandardPCA'",
                                    "docstring": "Fit the PCA model to the training data using the specified decomposition method.\n\nArgs:\n    X (pd.DataFrame): Training data for PCA.\n    y (Optional[pd.Series]): Not used; included for API consistency.\n\nReturns:\n    StandardPCA: The fitted PCA instance."
                                },
                                {
                                    "name": "transform",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Apply the PCA transformation to reduce the dimensionality of the input data.\n\nArgs:\n    X (pd.DataFrame): Data to transform.\n\nReturns:\n    pd.DataFrame: Data projected onto the principal components."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py"
                    }
                ]
            },
            {
                "node": "eigen decomposition",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/PCA Methods/eigen decomposition",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "StandardPCA",
                        "extra": {
                            "docstring": "Perform standard Principal Component Analysis (PCA) using eigen or singular value decomposition.\n\nThis interface implements the classical PCA algorithm for dimensionality reduction. It computes the principal\ncomponents of the data via eigen decomposition or singular value decomposition, depending on the properties of\nthe input matrix. The resulting components can be used to reduce the dimensionality while preserving variance.\n\nMethods:\n    fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'StandardPCA':\n        Compute the principal components from the input data.\n    transform(X: pd.DataFrame) -> pd.DataFrame:\n        Project new data onto the principal component space.\n\nArgs:\n    n_components (int): The number of principal components to compute.\n    method (str): The decomposition method ('eigen' or 'svd') to use; the selection may affect numerical stability.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_components",
                                        "method"
                                    ],
                                    "return_type": null,
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'StandardPCA'",
                                    "docstring": "Fit the PCA model to the training data using the specified decomposition method.\n\nArgs:\n    X (pd.DataFrame): Training data for PCA.\n    y (Optional[pd.Series]): Not used; included for API consistency.\n\nReturns:\n    StandardPCA: The fitted PCA instance."
                                },
                                {
                                    "name": "transform",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Apply the PCA transformation to reduce the dimensionality of the input data.\n\nArgs:\n    X (pd.DataFrame): Data to transform.\n\nReturns:\n    pd.DataFrame: Data projected onto the principal components."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py"
                    }
                ]
            },
            {
                "node": "incremental pca",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/PCA Methods/incremental pca",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "IncrementalPCA",
                        "extra": {
                            "docstring": "Perform Incremental Principal Component Analysis (Incremental PCA) for large datasets that cannot be processed in memory.\n\nThis interface implements Incremental PCA which allows partial fitting of the model on mini-batches of data,\nmaking it suitable for large datasets. The transformation yields principal components that approximate the standard PCA.\n\nMethods:\n    fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'IncrementalPCA':\n        Incrementally fit the PCA model on the input data.\n    partial_fit(X: pd.DataFrame) -> None:\n        Update the PCA model with a mini-batch of data.\n    transform(X: pd.DataFrame) -> pd.DataFrame:\n        Transform new data using the incrementally learned principal components.\n\nArgs:\n    n_components (int): The number of principal components to extract incrementally.\n    batch_size (int): Size of the mini-batches to use during incremental fitting.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_components",
                                        "batch_size"
                                    ],
                                    "return_type": null,
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'IncrementalPCA'",
                                    "docstring": "Fit the Incremental PCA model using the data, processing it in mini-batches.\n\nArgs:\n    X (pd.DataFrame): Input data for incremental fitting.\n    y (Optional[pd.Series]): Not used; provided for compatibility.\n\nReturns:\n    IncrementalPCA: The fitted IncrementalPCA model."
                                },
                                {
                                    "name": "partial_fit",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Update the PCA model with a mini-batch of data.\n\nArgs:\n    X (pd.DataFrame): A mini-batch of input data.\n\nReturns:\n    None"
                                },
                                {
                                    "name": "transform",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Transform the input data using the incrementally accumulated principal components.\n\nArgs:\n    X (pd.DataFrame): New input data to be transformed.\n\nReturns:\n    pd.DataFrame: Data represented in the reduced dimensional space."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py"
                    }
                ]
            },
            {
                "node": "kernel pca",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/PCA Methods/kernel pca",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "KernelPCA",
                        "extra": {
                            "docstring": "Perform Kernel Principal Component Analysis (Kernel PCA) for nonlinear dimensionality reduction.\n\nThis interface implements PCA using kernel methods to project data into a higher-dimensional feature space,\nallowing for the extraction of nonlinear patterns. The transform method applies the learned projection\non new data.\n\nMethods:\n    fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'KernelPCA':\n        Fit the Kernel PCA model to the input data.\n    transform(X: pd.DataFrame) -> pd.DataFrame:\n        Apply the kernel projection to reduce the dimensionality of X.\n\nArgs:\n    kernel (str): The kernel type to be used (e.g., 'rbf', 'poly', etc.).\n    n_components (int): Number of principal components to extract.\n    **kernel_params: Additional parameters for the kernel function.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "kernel",
                                        "n_components"
                                    ],
                                    "return_type": null,
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'KernelPCA'",
                                    "docstring": "Fit the Kernel PCA model using the given data.\n\nArgs:\n    X (pd.DataFrame): The input data for training.\n    y (Optional[pd.Series]): Not used, included for compatibility.\n\nReturns:\n    KernelPCA: The fitted KernelPCA instance."
                                },
                                {
                                    "name": "transform",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Transform the input data using the kernel PCA mapping.\n\nArgs:\n    X (pd.DataFrame): New input data to transform.\n\nReturns:\n    pd.DataFrame: The data represented in the reduced dimensional space."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py"
                    }
                ]
            },
            {
                "node": "singular value decomposition",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/PCA Methods/singular value decomposition",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "StandardPCA",
                        "extra": {
                            "docstring": "Perform standard Principal Component Analysis (PCA) using eigen or singular value decomposition.\n\nThis interface implements the classical PCA algorithm for dimensionality reduction. It computes the principal\ncomponents of the data via eigen decomposition or singular value decomposition, depending on the properties of\nthe input matrix. The resulting components can be used to reduce the dimensionality while preserving variance.\n\nMethods:\n    fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'StandardPCA':\n        Compute the principal components from the input data.\n    transform(X: pd.DataFrame) -> pd.DataFrame:\n        Project new data onto the principal component space.\n\nArgs:\n    n_components (int): The number of principal components to compute.\n    method (str): The decomposition method ('eigen' or 'svd') to use; the selection may affect numerical stability.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_components",
                                        "method"
                                    ],
                                    "return_type": null,
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'StandardPCA'",
                                    "docstring": "Fit the PCA model to the training data using the specified decomposition method.\n\nArgs:\n    X (pd.DataFrame): Training data for PCA.\n    y (Optional[pd.Series]): Not used; included for API consistency.\n\nReturns:\n    StandardPCA: The fitted PCA instance."
                                },
                                {
                                    "name": "transform",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Apply the PCA transformation to reduce the dimensionality of the input data.\n\nArgs:\n    X (pd.DataFrame): Data to transform.\n\nReturns:\n    pd.DataFrame: Data projected onto the principal components."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py"
                    }
                ]
            },
            {
                "node": "sparse pca",
                "feature_path": "Algorithms/Unsupervised Learning/Dimensionality Reduction/PCA Methods/sparse pca",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "SparsePCA",
                        "extra": {
                            "docstring": "Perform Sparse Principal Component Analysis (Sparse PCA) for dimensionality reduction with sparsity constraints.\n\nThis interface implements Sparse PCA where the principal components are obtained under a sparsity constraint,\nresulting in components with many zero loadings. This can enhance interpretability in high-dimensional data.\n\nMethods:\n    fit(X: pd.DataFrame, y: Optional[pd.Series]=None) -> 'SparsePCA':\n        Fit the Sparse PCA model using the input data.\n    transform(X: pd.DataFrame) -> pd.DataFrame:\n        Transform the data into a sparse principal component space.\n\nArgs:\n    n_components (int): The number of sparse principal components to compute.\n    alpha (float): Sparsity controlling parameter, where a higher value leads to sparser components.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "n_components",
                                        "alpha"
                                    ],
                                    "return_type": null,
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'SparsePCA'",
                                    "docstring": "Fit the Sparse PCA model to the data.\n\nArgs:\n    X (pd.DataFrame): Input training data.\n    y (Optional[pd.Series]): Not used, for interface compatibility only.\n\nReturns:\n    SparsePCA: The fitted SparsePCA instance."
                                },
                                {
                                    "name": "transform",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Transform input data into the space spanned by the sparse principal components.\n\nArgs:\n    X (pd.DataFrame): New data to transform.\n\nReturns:\n    pd.DataFrame: Data represented in the sparse principal component space."
                                }
                            ]
                        },
                        "path": "src/algorithms/unsupervised/dimensionality_reduction/pca_methods.py"
                    }
                ]
            }
        ],
        "Workflow": [
            {
                "node": "Evaluation",
                "feature_path": "Workflow/Evaluation",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/workflow/evaluation"
                    }
                ]
            },
            {
                "node": "Accuracy Metrics",
                "feature_path": "Workflow/Evaluation/Accuracy Metrics",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/workflow/evaluation"
                    }
                ]
            },
            {
                "node": "F1 Score",
                "feature_path": "Workflow/Evaluation/Accuracy Metrics/F1 Score",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/workflow/evaluation/accuracy.py"
                    }
                ]
            },
            {
                "node": "f1 score",
                "feature_path": "Workflow/Evaluation/Accuracy Metrics/F1 Score/f1 score",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "compute_f1_score",
                        "extra": {
                            "args": [
                                "y_true",
                                "y_pred",
                                "average"
                            ],
                            "return_type": "float",
                            "docstring": "Compute the F1 score, which is the harmonic mean of precision and recall.\n\nThe F1 score is a widely used accuracy metric for classification tasks,\ncombining the precision and recall into a single score by taking their harmonic mean.\nThis function serves as a public-facing interface for computing the F1 score\nfor a given set of true labels and predicted labels.\n\nArgs:\n    y_true (np.ndarray): Array of true class labels.\n    y_pred (np.ndarray): Array of predicted class labels.\n    average (str, optional): Defines the type of averaging performed on the data.\n         - 'binary': Only report results for the class specified by pos_label.\n         - 'micro', 'macro', 'weighted': Different averaging methods applicable to multi-class problems.\n         Defaults to 'binary'.\n\nReturns:\n    float: The computed F1 score as a floating-point value.\n\nEdge Cases:\n    - The function assumes that y_true and y_pred are valid numpy arrays of the same shape.\n    - For binary classification, it is assumed that the labels are encoded in a consistent manner.\n    - If the input arrays are empty or the metric is ill-defined, the behavior should be handled gracefully."
                        },
                        "path": "src/workflow/evaluation/accuracy.py"
                    }
                ]
            },
            {
                "node": "Performance Metrics",
                "feature_path": "Workflow/Evaluation/Performance Metrics",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/workflow/evaluation"
                    }
                ]
            },
            {
                "node": "Confusion Matrix",
                "feature_path": "Workflow/Evaluation/Performance Metrics/Confusion Matrix",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/workflow/evaluation/performance.py"
                    }
                ]
            },
            {
                "node": "confusion matrix",
                "feature_path": "Workflow/Evaluation/Performance Metrics/Confusion Matrix/confusion matrix",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "compute_confusion_matrix",
                        "extra": {
                            "args": [
                                "y_true",
                                "y_pred",
                                "labels"
                            ],
                            "return_type": "np.ndarray",
                            "docstring": "Compute the confusion matrix to evaluate the accuracy of a classification.\n\nThe confusion matrix is a summary table used to assess the performance of a classification\nalgorithm, by displaying the count of true positive, false positive, true negative, and false negative predictions.\nThis implementation compares the actual labels (y_true) to the predicted labels (y_pred).\n\nArgs:\n    y_true (np.ndarray): Array of true labels.\n    y_pred (np.ndarray): Array of predicted labels.\n    labels (Optional[List[Any]]): List of unique labels used in the classification to define the mapping\n        of matrix rows and columns. If None, this will be inferred from the input data.\n\nReturns:\n    np.ndarray: A 2D numpy array where the element at index [i, j] represents the count of samples \n    with true label corresponding to the i-th class and predicted label corresponding to the j-th class.\n\nEdge Cases:\n    - If the input arrays are empty or their lengths do not match, the behavior is undefined.\n    - If labels are not provided, they must be inferred correctly from y_true and y_pred to maintain consistency."
                        },
                        "path": "src/workflow/evaluation/performance.py"
                    }
                ]
            },
            {
                "node": "Preprocessing",
                "feature_path": "Workflow/Preprocessing",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/workflow/preprocessing/feature_engineering"
                    }
                ]
            },
            {
                "node": "Feature Engineering",
                "feature_path": "Workflow/Preprocessing/Feature Engineering",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/workflow/preprocessing/feature_engineering"
                    }
                ]
            },
            {
                "node": "Data Augmentation",
                "feature_path": "Workflow/Preprocessing/Feature Engineering/Data Augmentation",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/workflow/preprocessing/feature_engineering/data_augmentation.py"
                    }
                ]
            },
            {
                "node": "image augmentation",
                "feature_path": "Workflow/Preprocessing/Feature Engineering/Data Augmentation/image augmentation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "augment_image_data",
                        "extra": {
                            "args": [
                                "image_data",
                                "augmentation_params"
                            ],
                            "return_type": "np.ndarray",
                            "docstring": "Apply augmentation techniques to image data.\n\nThis function performs image augmentation operations such as rotation, scaling,\nflipping, and cropping on the input numpy array representing image(s). The optional \naugmentation parameters allow for configuring specific details of the augmentation \nprocess. This is particularly useful for expanding image datasets and improving\nmodel robustness in computer vision tasks.\n\nArgs:\n    image_data (np.ndarray): A numpy array representing one or more images.\n    augmentation_params (Optional[Dict[str, any]]): A dictionary containing augmentation \n        parameter settings (e.g., rotation angle, scaling factors, crop dimensions). If \n        None, default augmentation parameters should be applied.\n\nReturns:\n    np.ndarray: A numpy array of the augmented image data.\n\nEdge Cases:\n    - If the image_data array is empty, the function returns an empty array.\n    - If augmentation_params is None, default augmentation transformations are applied.\n\nAssumptions:\n    - The input numpy array conforms to the expected shape and data type for image data."
                        },
                        "path": "src/workflow/preprocessing/feature_engineering/data_augmentation.py"
                    }
                ]
            },
            {
                "node": "text augmentation",
                "feature_path": "Workflow/Preprocessing/Feature Engineering/Data Augmentation/text augmentation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "augment_text_data",
                        "extra": {
                            "args": [
                                "text_data",
                                "augmentation_methods"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Augment textual data using specified augmentation methods.\n\nThis function applies one or more text augmentation techniques to the input\nDataFrame containing textual data. Typical augmentation methods may include\nsynonym replacement, random insertion, random swap, and deletion. This interface\nenables users to enrich the dataset for downstream machine learning tasks,\nespecially in scenarios with limited text samples.\n\nArgs:\n    text_data (pd.DataFrame): A DataFrame containing text data to augment.\n    augmentation_methods (Optional[List[str]]): A list of augmentation method names \n        to apply (e.g., ['synonym_replacement', 'random_insertion']). If None, default \n        augmentation methods may be used.\n\nReturns:\n    pd.DataFrame: A DataFrame with the augmented text data.\n\nEdge Cases:\n    - If the input DataFrame is empty, the function returns an empty DataFrame.\n    - If augmentation_methods is None or empty, the function should handle it gracefully,\n      potentially applying a default method or returning the original DataFrame.\n\nAssumptions:\n    - The DataFrame has a column (or columns) that can be interpreted as textual data."
                        },
                        "path": "src/workflow/preprocessing/feature_engineering/data_augmentation.py"
                    }
                ]
            },
            {
                "node": "Data Transformation",
                "feature_path": "Workflow/Preprocessing/Feature Engineering/Data Transformation",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/workflow/preprocessing/feature_engineering/data_transformation.py"
                    }
                ]
            },
            {
                "node": "one-hot encoding",
                "feature_path": "Workflow/Preprocessing/Feature Engineering/Data Transformation/one-hot encoding",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "apply_onehot_encoding",
                        "extra": {
                            "args": [
                                "df",
                                "columns",
                                "sparse_output"
                            ],
                            "return_type": "Union[pd.DataFrame, sparse.csr_matrix]",
                            "docstring": "Apply one-hot encoding to the specified categorical columns of the input DataFrame.\n\nThis function transforms categorical columns into one-hot encoded representations.\nIf 'columns' are not specified, all columns with object or category dtypes will be encoded.\nThe output can be returned as a dense DataFrame or as a sparse CSR matrix based on the\n'sparse_output' flag.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing data to encode.\n    columns (Optional[List[str]]): List of column names to one-hot encode. If None, infer categorical columns.\n    sparse_output (bool): Flag indicating whether the output should be a sparse CSR matrix. Defaults to False.\n\nReturns:\n    Union[pd.DataFrame, sparse.csr_matrix]: One-hot encoded data in dense or sparse format.\n\nEdge Cases:\n    - If no categorical columns exist or the specified columns are not present, the function returns the original DataFrame.\n    - If an empty DataFrame is passed, an empty DataFrame (or corresponding sparse matrix) is returned.\n\nAssumptions:\n    - The input DataFrame is properly formatted."
                        },
                        "path": "src/workflow/preprocessing/feature_engineering/data_transformation.py"
                    }
                ]
            },
            {
                "node": "scaling and normalization",
                "feature_path": "Workflow/Preprocessing/Feature Engineering/Data Transformation/scaling and normalization",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "scale_and_normalize_features",
                        "extra": {
                            "args": [
                                "df",
                                "feature_columns",
                                "scaling_range",
                                "normalization_method"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Scale and normalize the specified features in the input DataFrame.\n\nThis function performs two transformations:\n  1. Scaling: Rescales numerical features to a specified range (default is [0, 1]).\n  2. Normalization: Applies normalization to standardize features. The default normalization\n     method is 'zscore' (transforming data to have zero mean and unit variance).\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing features to be transformed.\n    feature_columns (Optional[List[str]]): List of columns to be scaled and normalized. \n                                           If None, all numeric columns are processed.\n    scaling_range (Optional[tuple]): Desired range for scaling as (min, max). Defaults to (0, 1).\n    normalization_method (str): The normalization method to apply. Supported:\n                                'zscore' for standard normalization.\n                                Additional methods can be incorporated if needed.\n\nReturns:\n    pd.DataFrame: A new DataFrame with scaled and normalized feature values.\n\nEdge Cases:\n    - If the specified feature_columns list is empty or none of the columns are numeric,\n      the original DataFrame is returned.\n    - If an invalid normalization method is specified, behavior is undefined (implementation should handle errors).\n\nAssumptions:\n    - The input DataFrame is valid and contains numeric types for the scaling and normalization.\n    - The normalization_method adheres to supported methods which are validated elsewhere."
                        },
                        "path": "src/workflow/preprocessing/feature_engineering/data_transformation.py"
                    }
                ]
            },
            {
                "node": "Feature Extraction",
                "feature_path": "Workflow/Preprocessing/Feature Engineering/Feature Extraction",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/workflow/preprocessing/feature_engineering/feature_extraction.py"
                    }
                ]
            },
            {
                "node": "pca",
                "feature_path": "Workflow/Preprocessing/Feature Engineering/Feature Extraction/pca",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "extract_pca_features",
                        "extra": {
                            "args": [
                                "data",
                                "n_components",
                                "method"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Apply Principal Component Analysis (PCA) to extract principal components as features from the input data.\n\nThis function is designed for the preprocessing pipeline's feature extraction stage, enabling dimensionality\nreduction via PCA. It provides an interface for transforming the input DataFrame into a reduced set of features\nwhile preserving as much variance as possible from the original dataset.\n\nArgs:\n    data (pd.DataFrame): The input data containing the features to be transformed.\n    n_components (int, optional): The number of principal components to extract. Defaults to 2.\n    method (str, optional): The PCA computation method to use. Accepted values may include 'svd' for singular value decomposition,\n                            or other method identifiers as defined in the underlying PCA implementation. Defaults to 'svd'.\n\nReturns:\n    pd.DataFrame: A DataFrame containing the extracted principal components, with at most n_components columns.\n\nEdge Cases & Assumptions:\n    - It is assumed that the input DataFrame 'data' is preprocessed (e.g., missing values handled) prior to PCA.\n    - The function does not handle scaling; any required scaling should occur in an earlier pipeline stage.\n    - The 'method' parameter allows for future extension to different PCA computation algorithms."
                        },
                        "path": "src/workflow/preprocessing/feature_engineering/feature_extraction.py"
                    }
                ]
            },
            {
                "node": "Normalization",
                "feature_path": "Workflow/Preprocessing/Normalization",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/workflow/preprocessing/normalization"
                    }
                ]
            },
            {
                "node": "Z-score Normalization",
                "feature_path": "Workflow/Preprocessing/Normalization/Z-score Normalization",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/workflow/preprocessing/normalization/z_score.py"
                    }
                ]
            },
            {
                "node": "normalize to mean 0 and std 1",
                "feature_path": "Workflow/Preprocessing/Normalization/Z-score Normalization/normalize to mean 0 and std 1",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "normalize_z_score",
                        "extra": {
                            "args": [
                                "df",
                                "columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Normalize the input dataframe using Z-score normalization.\n\nThis function transforms specified numerical columns of the input dataframe so that they have a mean of 0 \nand a standard deviation of 1. If the 'columns' parameter is None, the function will apply normalization \nto all numeric columns present in the dataframe.\n\nArgs:\n    df (pd.DataFrame): The input dataframe containing the data to be normalized.\n    columns (Optional[List[str]]): A list of column names to normalize. If None, all numeric columns will be processed.\n\nReturns:\n    pd.DataFrame: A new dataframe with the selected columns normalized using Z-score scaling.\n\nNotes:\n    - The function assumes that the specified columns contain numeric data.\n    - It does not modify the original dataframe.\n    - Columns with zero standard deviation may lead to division by zero errors; handling such cases is left to the implementer."
                        },
                        "path": "src/workflow/preprocessing/normalization/z_score.py"
                    }
                ]
            }
        ],
        "Data Engineering": [
            {
                "node": "Data Encoding",
                "feature_path": "Data Engineering/Data Encoding",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_encoding"
                    }
                ]
            },
            {
                "node": "Binary Encoding",
                "feature_path": "Data Engineering/Data Encoding/Binary Encoding",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_encoding"
                    }
                ]
            },
            {
                "node": "Details",
                "feature_path": "Data Engineering/Data Encoding/Binary Encoding/Details",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_encoding/binary_encoding.py"
                    }
                ]
            },
            {
                "node": "compressed binary encoding",
                "feature_path": "Data Engineering/Data Encoding/Binary Encoding/Details/compressed binary encoding",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "compress_binary_encoding",
                        "extra": {
                            "args": [
                                "df",
                                "column"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Apply compressed binary encoding to the specified column of a DataFrame.\n\nThis function transforms the binary encoding of a given column in the DataFrame\nby applying compression techniques. It is intended to reduce storage overhead while\npreserving the binary nature of the column's data. The function assumes the input column \ncontains data that can be represented in a binary format and performs a compression\noperation that is tailored to binary encoded data.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing the binary-encoded column.\n    column (str): The name of the column in the DataFrame to be compressed.\n\nReturns:\n    pd.DataFrame: A new DataFrame with the specified column processed through compressed\n                  binary encoding. All other columns remain unchanged.\n\nEdge Cases:\n    - If the specified column does not exist, the function should handle the situation appropriately.\n    - The function assumes that the column's data is already in a binary encodable format.\n    - No in-place modification is performed; a new DataFrame is returned.\n\nNote:\n    This is a stub interface. Implementation logic is to be provided later."
                        },
                        "path": "src/data_engineering/data_encoding/binary_encoding.py"
                    }
                ]
            },
            {
                "node": "Label Encoding",
                "feature_path": "Data Engineering/Data Encoding/Label Encoding",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_encoding"
                    }
                ]
            },
            {
                "node": "Details",
                "feature_path": "Data Engineering/Data Encoding/Label Encoding/Details",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_encoding/label_encoding.py"
                    }
                ]
            },
            {
                "node": "frequency-based label encoding",
                "feature_path": "Data Engineering/Data Encoding/Label Encoding/Details/frequency-based label encoding",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "frequency_based_label_encoding",
                        "extra": {
                            "args": [
                                "df",
                                "column"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Perform frequency-based label encoding on a specified column of a DataFrame.\n\nThis function converts categorical labels in the provided column into numerical labels based\non the frequency of each unique category. Categories that appear more frequently are assigned lower\nnumerical values. This type of encoding is useful when the frequency order carries predictive significance.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing the target column.\n    column (str): The name of the column containing categorical labels to encode.\n\nReturns:\n    pd.DataFrame: A DataFrame with the specified column transformed into frequency-based numerical labels.\n\nNotes:\n    - The function assumes that the specified column contains categorical data.\n    - Handling of missing values or unseen categories must be considered in the implementation."
                        },
                        "path": "src/data_engineering/data_encoding/label_encoding.py"
                    }
                ]
            },
            {
                "node": "ordinal label encoding",
                "feature_path": "Data Engineering/Data Encoding/Label Encoding/Details/ordinal label encoding",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "ordinal_label_encoding",
                        "extra": {
                            "args": [
                                "df",
                                "column",
                                "categories"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Apply ordinal label encoding to a specified column of a DataFrame.\n\nThis function maps categorical labels in the given column to integer values that reflect a user-specified\norder. The order is provided by the 'categories' list, where the position of a category in the list\ndetermines its ordinal value. This encoding is appropriate when there is an inherent ranking among the categories.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing the target column.\n    column (str): The name of the column containing categorical labels to encode.\n    categories (list): A list of categories defining the desired order for ordinal encoding.\n\nReturns:\n    pd.DataFrame: A DataFrame with the specified column transformed using ordinal label encoding.\n\nNotes:\n    - The function assumes that the provided 'categories' list covers all possible labels in the column.\n    - Users must ensure that the order specified in the 'categories' list accurately reflects the intended ranking.\n    - Behavior for handling missing or unexpected labels should be defined as needed."
                        },
                        "path": "src/data_engineering/data_encoding/label_encoding.py"
                    }
                ]
            },
            {
                "node": "One-hot Encoding",
                "feature_path": "Data Engineering/Data Encoding/One-hot Encoding",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_encoding"
                    }
                ]
            },
            {
                "node": "Combined One-hot",
                "feature_path": "Data Engineering/Data Encoding/One-hot Encoding/Combined One-hot",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_encoding/onehot_encoding.py"
                    }
                ]
            },
            {
                "node": "dense one-hot encoding",
                "feature_path": "Data Engineering/Data Encoding/One-hot Encoding/Combined One-hot/dense one-hot encoding",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "CombinedOneHotEncoder",
                        "extra": {
                            "docstring": "A combined one-hot encoder that supports both dense one-hot encoding and sparse matrix optimization.\n\nThis class provides an interface for encoding categorical features into a one-hot representation.\nUsers can choose to produce a dense output (a pandas DataFrame) or an optimized sparse matrix,\ndepending on the requirements for memory and computational efficiency.\n\nAttributes:\n    sparse (bool): Determines the output format. If True, the transform method returns a sparse \n                   matrix optimized for performance. If False, it returns a dense pandas DataFrame.\n\nMethods:\n    fit(X: pd.DataFrame, columns: Optional[List[str]] = None) -> CombinedOneHotEncoder:\n        Fit the encoder on input data to learn the unique categories from specified columns.\n    \n    transform(X: pd.DataFrame) -> Union[pd.DataFrame, sparse.csr_matrix]:\n        Transform the input DataFrame into one-hot encoded format, producing either a dense DataFrame\n        or a sparse matrix based on the encoder configuration.\n    \n    fit_transform(X: pd.DataFrame, columns: Optional[List[str]] = None) -> Union[pd.DataFrame, sparse.csr_matrix]:\n        Fit the encoder and transform the data in one step.\n\nArgs:\n    sparse (bool): A flag to determine whether to use sparse matrix optimization (True) or dense \n                   representation (False). Default is False.\n\nReturns:\n    An instance of CombinedOneHotEncoder.\n\nEdge Cases and Constraints:\n    - The input DataFrame should contain the categorical columns to be encoded.\n    - If columns is None, the encoder may attempt to infer the categorical columns from the data.\n    - The transform method must handle cases where unknown categories are encountered gracefully.\n    - This interface does not implement the actual encoding logic; it provides the necessary structure.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "sparse"
                                    ],
                                    "return_type": null,
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "columns"
                                    ],
                                    "return_type": "'CombinedOneHotEncoder'",
                                    "docstring": "Fit the encoder to the input DataFrame by learning the unique categories from specified columns.\n\nArgs:\n    X (pd.DataFrame): The input data containing categorical features.\n    columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, the encoder\n                                   may attempt to infer categorical columns automatically.\n\nReturns:\n    CombinedOneHotEncoder: The fitted encoder instance."
                                },
                                {
                                    "name": "transform",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "Union[pd.DataFrame, sparse.csr_matrix]",
                                    "docstring": "Transform the input DataFrame into a one-hot encoded format.\n\nBased on the encoder's configuration, the output will either be a dense pandas DataFrame or \nan optimized sparse matrix.\n\nArgs:\n    X (pd.DataFrame): The data to transform.\n\nReturns:\n    Union[pd.DataFrame, sparse.csr_matrix]: The one-hot encoded representation of the input data."
                                },
                                {
                                    "name": "fit_transform",
                                    "args": [
                                        "self",
                                        "X",
                                        "columns"
                                    ],
                                    "return_type": "Union[pd.DataFrame, sparse.csr_matrix]",
                                    "docstring": "Fit the encoder to the data and then transform it into one-hot encoded format.\n\nArgs:\n    X (pd.DataFrame): The input data to fit and transform.\n    columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, it may infer columns.\n\nReturns:\n    Union[pd.DataFrame, sparse.csr_matrix]: The transformed one-hot encoded data."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_encoding/onehot_encoding.py"
                    }
                ]
            },
            {
                "node": "sparse matrix optimization",
                "feature_path": "Data Engineering/Data Encoding/One-hot Encoding/Combined One-hot/sparse matrix optimization",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "CombinedOneHotEncoder",
                        "extra": {
                            "docstring": "A combined one-hot encoder that supports both dense one-hot encoding and sparse matrix optimization.\n\nThis class provides an interface for encoding categorical features into a one-hot representation.\nUsers can choose to produce a dense output (a pandas DataFrame) or an optimized sparse matrix,\ndepending on the requirements for memory and computational efficiency.\n\nAttributes:\n    sparse (bool): Determines the output format. If True, the transform method returns a sparse \n                   matrix optimized for performance. If False, it returns a dense pandas DataFrame.\n\nMethods:\n    fit(X: pd.DataFrame, columns: Optional[List[str]] = None) -> CombinedOneHotEncoder:\n        Fit the encoder on input data to learn the unique categories from specified columns.\n    \n    transform(X: pd.DataFrame) -> Union[pd.DataFrame, sparse.csr_matrix]:\n        Transform the input DataFrame into one-hot encoded format, producing either a dense DataFrame\n        or a sparse matrix based on the encoder configuration.\n    \n    fit_transform(X: pd.DataFrame, columns: Optional[List[str]] = None) -> Union[pd.DataFrame, sparse.csr_matrix]:\n        Fit the encoder and transform the data in one step.\n\nArgs:\n    sparse (bool): A flag to determine whether to use sparse matrix optimization (True) or dense \n                   representation (False). Default is False.\n\nReturns:\n    An instance of CombinedOneHotEncoder.\n\nEdge Cases and Constraints:\n    - The input DataFrame should contain the categorical columns to be encoded.\n    - If columns is None, the encoder may attempt to infer the categorical columns from the data.\n    - The transform method must handle cases where unknown categories are encountered gracefully.\n    - This interface does not implement the actual encoding logic; it provides the necessary structure.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "sparse"
                                    ],
                                    "return_type": null,
                                    "docstring": null
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "columns"
                                    ],
                                    "return_type": "'CombinedOneHotEncoder'",
                                    "docstring": "Fit the encoder to the input DataFrame by learning the unique categories from specified columns.\n\nArgs:\n    X (pd.DataFrame): The input data containing categorical features.\n    columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, the encoder\n                                   may attempt to infer categorical columns automatically.\n\nReturns:\n    CombinedOneHotEncoder: The fitted encoder instance."
                                },
                                {
                                    "name": "transform",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "Union[pd.DataFrame, sparse.csr_matrix]",
                                    "docstring": "Transform the input DataFrame into a one-hot encoded format.\n\nBased on the encoder's configuration, the output will either be a dense pandas DataFrame or \nan optimized sparse matrix.\n\nArgs:\n    X (pd.DataFrame): The data to transform.\n\nReturns:\n    Union[pd.DataFrame, sparse.csr_matrix]: The one-hot encoded representation of the input data."
                                },
                                {
                                    "name": "fit_transform",
                                    "args": [
                                        "self",
                                        "X",
                                        "columns"
                                    ],
                                    "return_type": "Union[pd.DataFrame, sparse.csr_matrix]",
                                    "docstring": "Fit the encoder to the data and then transform it into one-hot encoded format.\n\nArgs:\n    X (pd.DataFrame): The input data to fit and transform.\n    columns (Optional[List[str]]): A list of columns to be one-hot encoded. If None, it may infer columns.\n\nReturns:\n    Union[pd.DataFrame, sparse.csr_matrix]: The transformed one-hot encoded data."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_encoding/onehot_encoding.py"
                    }
                ]
            },
            {
                "node": "Data Preparation",
                "feature_path": "Data Engineering/Data Preparation",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_preparation"
                    }
                ]
            },
            {
                "node": "Data Aggregation & Profiling",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_preparation/aggregation"
                    }
                ]
            },
            {
                "node": "Aggregation",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Aggregation",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/aggregation/aggregation.py"
                    }
                ]
            },
            {
                "node": "group and count",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Aggregation/group and count",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "group_and_count",
                        "extra": {
                            "args": [
                                "df",
                                "group_by"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Group the DataFrame by the specified columns and count the occurrences in each group.\n\nThe function performs a group by operation and returns the count of rows for each group.\nThis can be useful for data profiling purposes to identify the distribution of categorical\nor discrete feature occurrences within the dataset.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame to be grouped.\n    group_by (List[str]): A list of columns to group the DataFrame by.\n\nReturns:\n    pd.DataFrame: A DataFrame with the count of rows for each group defined by the group_by columns.\n\nEdge Cases:\n    - If no rows exist in the DataFrame, an empty DataFrame is returned.\n    - If group_by columns contain null values, the count might include these as a separate group."
                        },
                        "path": "src/data_engineering/data_preparation/aggregation/aggregation.py"
                    }
                ]
            },
            {
                "node": "mean",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Aggregation/mean",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "aggregate_mean",
                        "extra": {
                            "args": [
                                "df",
                                "columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Compute the mean value for specified columns in the given DataFrame.\n\nThis function calculates the arithmetic mean of the specified columns.\nIf no columns are provided, it computes the mean for all numerical columns.\nIt is designed to assist in data aggregation profiling by summarizing data trends.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing data to aggregate.\n    columns (List[str], optional): List of column names for which the mean should be computed.\n        If None, mean is computed on all numerical columns.\n\nReturns:\n    pd.DataFrame: A DataFrame containing the mean values for the specified columns.\n\nEdge Cases:\n    - If the DataFrame is empty, the function will return an empty DataFrame.\n    - If columns are specified but some are not present in the DataFrame, the behavior is undefined."
                        },
                        "path": "src/data_engineering/data_preparation/aggregation/aggregation.py"
                    }
                ]
            },
            {
                "node": "median",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Aggregation/median",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "aggregate_median",
                        "extra": {
                            "args": [
                                "df",
                                "columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Compute the median value for specified columns in the given DataFrame.\n\nThis function calculates the median of the provided columns.\nIf columns are not explicitly provided, it calculates the median for every numerical column in the DataFrame.\nThis aggregated insight is useful for understanding the central tendency of data distributions.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing data for aggregation.\n    columns (List[str], optional): List of column names for which the median should be computed.\n        Defaults to None, meaning all numerical columns will be considered.\n\nReturns:\n    pd.DataFrame: A DataFrame with median values for each specified column.\n\nEdge Cases:\n    - An empty DataFrame will return an empty DataFrame.\n    - Missing columns specified in 'columns' may lead to unexpected behavior."
                        },
                        "path": "src/data_engineering/data_preparation/aggregation/aggregation.py"
                    }
                ]
            },
            {
                "node": "Bucketization",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Bucketization",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/aggregation/bucketization.py"
                    }
                ]
            },
            {
                "node": "bucketize by custom bins",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Bucketization/bucketize by custom bins",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "bucketize_by_custom_bins",
                        "extra": {
                            "args": [
                                "df",
                                "bins"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Bucketize the DataFrame using custom bin edges.\n\nThis function applies user-defined bin edges to classify the data into discrete buckets.\nIt uses the provided list of bin boundaries to determine the bucket for each row in the DataFrame.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n    bins (List[float]): A list of floating-point numbers defining the custom bin edges.\n\nReturns:\n    pd.DataFrame: A new DataFrame with an added column that indicates the bucket assignment based on the custom bins.\n\nEdge Cases:\n    - An empty list of bins will result in no bucketization.\n    - Non-monotonic or overlapping bin values may lead to undefined bucket assignments.\n    - An empty DataFrame will simply return an empty DataFrame."
                        },
                        "path": "src/data_engineering/data_preparation/aggregation/bucketization.py"
                    }
                ]
            },
            {
                "node": "bucketize by quantiles",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Bucketization/bucketize by quantiles",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "bucketize_by_quantiles",
                        "extra": {
                            "args": [
                                "df",
                                "quantiles"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Bucketize the DataFrame by quantiles.\n\nThis function divides the data in a specified DataFrame column into buckets based on the provided quantile values.\nIt calculates the boundaries corresponding to the quantiles and assigns each row to a bucket derived from these boundaries.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n    quantiles (List[float]): A list of quantile values (ranging from 0 to 1) that determine the bucket boundaries.\n\nReturns:\n    pd.DataFrame: A new DataFrame with an added column indicating the quantile bucket for each row.\n\nEdge Cases:\n    - An empty DataFrame will return an empty DataFrame.\n    - An empty quantiles list will result in no bucketing.\n    - The function assumes that the appropriate data column will be managed externally."
                        },
                        "path": "src/data_engineering/data_preparation/aggregation/bucketization.py"
                    }
                ]
            },
            {
                "node": "bucketize by range",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Bucketization/bucketize by range",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "bucketize_by_range",
                        "extra": {
                            "args": [
                                "df",
                                "range_start",
                                "range_end",
                                "bucket_size"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Bucketize the DataFrame by a defined numerical range.\n\nThis function segments the data into buckets based on a continuous range defined by a start value, an end value,\nand a fixed bucket size. It creates equal intervals within the specified range and assigns each data row to its corresponding bucket.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing the values to be bucketized.\n    range_start (float): The beginning value of the range.\n    range_end (float): The ending value of the range.\n    bucket_size (float): The fixed width of each bucket within the range.\n\nReturns:\n    pd.DataFrame: A new DataFrame with an added column indicating the bucket assignment for each row.\n\nEdge Cases:\n    - If the range length is not an exact multiple of the bucket_size, the final bucket may be smaller.\n    - An empty DataFrame will return an empty DataFrame.\n    - Values outside the specified range may require special handling or may be excluded."
                        },
                        "path": "src/data_engineering/data_preparation/aggregation/bucketization.py"
                    }
                ]
            },
            {
                "node": "dynamic bucketization",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Bucketization/dynamic bucketization",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "dynamic_bucketization",
                        "extra": {
                            "args": [
                                "df",
                                "strategy",
                                "parameters"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Dynamically bucketize the DataFrame based on a specified strategy.\n\nThis function automatically determines bucket boundaries according to the distribution of the data.\nThe 'strategy' argument specifies the dynamic bucketing method to use (e.g., equal frequency or clustering-based),\nwhile the 'parameters' dictionary provides any additional configuration needed for that strategy.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing the data to be bucketized.\n    strategy (str): A string representing the dynamic bucketing strategy to apply.\n    parameters (dict): A dictionary of strategy-specific parameters for bucketization.\n\nReturns:\n    pd.DataFrame: A new DataFrame with an added column that denotes the dynamic bucket assignment for each row.\n\nEdge Cases:\n    - An empty DataFrame will result in an empty DataFrame.\n    - If an unrecognized strategy is provided, the function may default to a no-op bucketing process."
                        },
                        "path": "src/data_engineering/data_preparation/aggregation/bucketization.py"
                    }
                ]
            },
            {
                "node": "Profiling",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Profiling",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/aggregation/profiling.py"
                    }
                ]
            },
            {
                "node": "correlation analysis",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Profiling/correlation analysis",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "analyze_correlation",
                        "extra": {
                            "args": [
                                "df",
                                "method"
                            ],
                            "return_type": "Dict[str, Any]",
                            "docstring": "Perform a correlation analysis on the provided DataFrame.\n\nThis function computes the correlation matrix for the numerical features in the DataFrame using the specified\ncorrelation method (e.g., 'pearson', 'spearman', or 'kendall'). It returns a dictionary containing the correlation\nmatrix and additional insights such as pairs of features with high correlations. This analysis helps in data profiling\nby identifying relationships between features.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing numerical features to analyze.\n    method (str, optional): The correlation method to use; valid values include 'pearson', 'spearman', and 'kendall'.\n                            Defaults to 'pearson'.\n\nReturns:\n    Dict[str, Any]: A dictionary with the following keys:\n        - 'correlation_matrix': A DataFrame representing the computed correlation matrix.\n        - 'highly_correlated_pairs': A list of tuples indicating feature pairs with high correlation.\n        - 'summary': A summary of the correlation analysis providing further insights.\n\nEdge Cases:\n    - If the DataFrame is empty or lacks sufficient numerical data, the function should return an empty dictionary.\n    - The function assumes that inputs have appropriate numeric data types for correlation calculation."
                        },
                        "path": "src/data_engineering/data_preparation/aggregation/profiling.py"
                    }
                ]
            },
            {
                "node": "Zero Column Handling",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Zero Column Handling",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/aggregation/zero_column.py"
                    }
                ]
            },
            {
                "node": "drop columns with mostly zeros",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Zero Column Handling/drop columns with mostly zeros",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "drop_columns_with_mostly_zeros",
                        "extra": {
                            "args": [
                                "df",
                                "threshold"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Drop columns from the DataFrame that contain mostly zeros.\n\nThis function examines each column in the input DataFrame and drops those columns where\nthe proportion of zero values exceeds the specified threshold, indicating that the column\nis mostly populated with zeros. This is useful as a preprocessing step for cleaning data,\nparticularly in scenarios where such columns may not contribute meaningful information.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame to be processed.\n    threshold (float, optional): The proportion of zero values in a column above which\n        the column will be dropped. The value should be in the range (0, 1]. Default is 0.8,\n        meaning columns with more than 80% zeros will be removed.\n\nReturns:\n    pd.DataFrame: A DataFrame with columns that have mostly zeros removed.\n\nEdge Cases:\n    - If the DataFrame is empty, the function returns an empty DataFrame.\n    - If no column meets the dropping criteria, the original DataFrame is returned."
                        },
                        "path": "src/data_engineering/data_preparation/aggregation/zero_column.py"
                    }
                ]
            },
            {
                "node": "drop columns with zero variance",
                "feature_path": "Data Engineering/Data Preparation/Data Aggregation & Profiling/Zero Column Handling/drop columns with zero variance",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "drop_columns_with_zero_variance",
                        "extra": {
                            "args": [
                                "df"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Drop columns from the DataFrame that have zero variance.\n\nThis function identifies columns in the input DataFrame where all values are identical\n(i.e., the variance is zero) and removes them. Such columns typically do not provide\nuseful discriminatory information in further data analysis or modeling, and hence,\ntheir removal can help in reducing dimensionality.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing the data to be processed.\n\nReturns:\n    pd.DataFrame: A modified DataFrame with columns of zero variance removed.\n\nEdge Cases:\n    - If the DataFrame is empty, the function returns an empty DataFrame.\n    - If no columns have zero variance, the original DataFrame is returned unchanged."
                        },
                        "path": "src/data_engineering/data_preparation/aggregation/zero_column.py"
                    }
                ]
            },
            {
                "node": "Data Cleaning",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_preparation/cleaning"
                    }
                ]
            },
            {
                "node": "Initial Cleaning",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "clip values",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/clip values",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "clip_dataframe_values",
                        "extra": {
                            "args": [
                                "df",
                                "lower",
                                "upper"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Clip the values of a DataFrame to specified lower and/or upper bounds.\n\nThis function limits all numerical values in the DataFrame to be within\nthe specified range.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    lower (float, optional): Lower bound for clipping. Defaults to None.\n    upper (float, optional): Upper bound for clipping. Defaults to None.\n\nReturns:\n    pd.DataFrame: DataFrame with values clipped within the specified bounds.\n\nEdge Cases:\n    If both bounds are None, the original DataFrame is returned unchanged."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "convert text to number",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/convert text to number",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "convert_text_to_number",
                        "extra": {
                            "args": [
                                "df",
                                "columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Convert textual representations of numbers in specified DataFrame columns to numeric types.\n\nThis function attempts to cast string representations of numbers to numeric values.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    columns (list): List of column names (str) that contain textual numbers.\n\nReturns:\n    pd.DataFrame: DataFrame with the specified columns converted to numeric types.\n\nEdge Cases:\n    Improperly formatted strings that cannot be converted will remain unchanged or be set to NaN."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "convert to appropriate types",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/convert to appropriate types",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "convert_to_appropriate_types",
                        "extra": {
                            "args": [
                                "df",
                                "type_mapping"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Convert DataFrame columns to appropriate types.\n\nThis function casts DataFrame columns based on a provided mapping from column names\nto desired data types.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    type_mapping (dict): Dictionary mapping column names (str) to target data types.\n\nReturns:\n    pd.DataFrame: DataFrame with columns converted to the specified types.\n\nEdge Cases:\n    Columns not present in the mapping remain unchanged."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "convert units",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/convert units",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "convert_units",
                        "extra": {
                            "args": [
                                "df",
                                "conversion_mapping"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Convert units of measurement for specified columns in a DataFrame.\n\nThis function applies unit conversions to DataFrame columns based on a provided mapping,\nwhere each key is a column and the value is a conversion function or factor.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    conversion_mapping (dict): A dictionary mapping column names to conversion parameters.\n\nReturns:\n    pd.DataFrame: DataFrame with converted unit values.\n\nEdge Cases:\n    If a column is not present or conversion fails, the column remains unchanged."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "correct data inconsistencies",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/correct data inconsistencies",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "correct_data_inconsistencies",
                        "extra": {
                            "args": [
                                "df",
                                "correction_rules"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Correct data inconsistencies in a DataFrame based on predefined rules.\n\nThis function applies a set of rules to standardize and correct inconsistent data entries,\nensuring data quality and uniformity.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    correction_rules (dict): A mapping from column names to correction functions or rules.\n\nReturns:\n    pd.DataFrame: DataFrame with corrected data inconsistencies.\n\nEdge Cases:\n    If a column does not have a corresponding rule, it is left unchanged."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "detect using clustering",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/detect using clustering",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "detect_outliers_using_clustering",
                        "extra": {
                            "args": [
                                "df",
                                "clustering_params"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Detect outliers in a DataFrame using clustering techniques.\n\nThis function applies a clustering algorithm to the DataFrame to identify\ndata points that are considered outliers based on cluster assignments.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    clustering_params (dict): Parameters for the clustering algorithm (e.g., number of clusters, distance metrics).\n\nReturns:\n    pd.DataFrame: DataFrame with an additional indicator column for detected outliers.\n\nEdge Cases:\n    If the DataFrame is empty or clustering fails, an unmodified DataFrame may be returned."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "drop columns with missing",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/drop columns with missing",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "drop_columns_with_missing",
                        "extra": {
                            "args": [
                                "df",
                                "threshold"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Drop columns from a DataFrame that have a proportion of missing values above a threshold.\n\nThis function evaluates each column for the proportion of missing values\nand removes those columns that exceed the defined threshold.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    threshold (float, optional): Maximum allowed fraction of missing values per column. Defaults to 0.5.\n\nReturns:\n    pd.DataFrame: DataFrame after dropping columns with excessive missing data.\n\nEdge Cases:\n    If no columns meet the dropping criteria, the original DataFrame is returned intact."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "drop null columns",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/drop null columns",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "drop_null_columns",
                        "extra": {
                            "args": [
                                "df",
                                "threshold"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Drop columns from a DataFrame that consist entirely of null values.\n\nThis function examines each column and drops those that have a fraction of non-null entries\nlower than the specified threshold (typically 1.0 to indicate all values are null).\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    threshold (float, optional): Fraction of non-null values required to retain the column.\n                                 Defaults to 1.0.\n\nReturns:\n    pd.DataFrame: DataFrame with null-only columns removed.\n\nEdge Cases:\n    If no columns meet the criteria for dropping, the original DataFrame is returned."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "drop rows",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/drop rows",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "drop_rows",
                        "extra": {
                            "args": [
                                "df",
                                "condition"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Drop rows from a DataFrame that meet a specified condition.\n\nThis function removes rows based on a provided condition, which can be a boolean mask or a callable\nthat takes a row and returns a boolean value.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    condition: A boolean array, boolean Series, or callable that defines the drop condition.\n\nReturns:\n    pd.DataFrame: DataFrame with rows dropped based on the condition.\n\nEdge Cases:\n    If the condition does not match any rows, the original DataFrame is returned."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "dropna",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/dropna",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "dropna_values",
                        "extra": {
                            "args": [
                                "df",
                                "axis",
                                "how"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Drop NA/null values from a DataFrame.\n\nThis function removes missing values from the DataFrame along the specified axis.\nIt uses pandas' dropna functionality.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    axis (int, optional): The axis along which to drop NA values (0 for rows, 1 for columns). Defaults to 0.\n    how (str, optional): Determine if row or column is removed based on 'any' or 'all' NA values. Defaults to 'any'.\n\nReturns:\n    pd.DataFrame: DataFrame with NA values dropped.\n\nEdge Cases:\n    Returns the original DataFrame if there are no NA values."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "fill with constant",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/fill with constant",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "fill_with_constant",
                        "extra": {
                            "args": [
                                "df",
                                "value",
                                "columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Fill missing or undesirable values in a DataFrame with a constant value.\n\nThis function replaces missing values or outlier values with a specified constant across\nall or selected columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    value: The constant value to fill in.\n    columns (list, optional): List of column names (str) to apply the fill.\n                              If None, applies to entire DataFrame.\n\nReturns:\n    pd.DataFrame: DataFrame with specified columns filled with the constant value.\n\nEdge Cases:\n    If no columns are specified and the DataFrame has no missing values, the original DataFrame is returned."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "remove by standard deviation",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/remove by standard deviation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "remove_by_standard_deviation",
                        "extra": {
                            "args": [
                                "df",
                                "threshold"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Remove rows from a DataFrame that deviate from the mean by more than a specified number of standard deviations.\n\nThis function calculates the standard deviation for numerical columns and removes rows\nthat fall outside the defined threshold range.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    threshold (float, optional): Number of standard deviations from the mean to consider as the limit. Defaults to 3.0.\n\nReturns:\n    pd.DataFrame: DataFrame with extreme outlier rows removed.\n\nEdge Cases:\n    Returns the original DataFrame if no rows exceed the threshold."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "remove outlier rows",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/remove outlier rows",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "remove_outlier_rows",
                        "extra": {
                            "args": [
                                "df",
                                "outlier_indicator_column"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Remove rows identified as outliers from a DataFrame.\n\nThis function drops rows that have been flagged as outliers based on a specified indicator column.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    outlier_indicator_column (str, optional): Column name that indicates outlier status. Defaults to 'is_outlier'.\n\nReturns:\n    pd.DataFrame: DataFrame with outlier rows removed.\n\nEdge Cases:\n    If the indicator column does not exist, the original DataFrame is returned unchanged."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "remove top 1%",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/remove top 1%",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "remove_top_percent",
                        "extra": {
                            "args": [
                                "df",
                                "percent"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Remove the top percentage of extreme values from a DataFrame.\n\nThis function removes the rows corresponding to the top specified percentage \n(by value) of a target numerical column, often used to exclude extreme outliers.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    percent (float, optional): The percentage of top extreme values to remove. Defaults to 1.0.\n\nReturns:\n    pd.DataFrame: DataFrame with the top percent of extreme values removed.\n\nEdge Cases:\n    If the computed cutoff excludes no rows, the original DataFrame is returned."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "replace with default value",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/replace with default value",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "replace_with_default_value",
                        "extra": {
                            "args": [
                                "df",
                                "default_mapping"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Replace specified values in a DataFrame with default values.\n\nThis function uses a mapping of column names to default replacement values to\nsubstitute certain data entries in a DataFrame.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    default_mapping (dict): Dictionary where keys are column names and values are the default value to apply.\n\nReturns:\n    pd.DataFrame: DataFrame with specified values replaced by defaults.\n\nEdge Cases:\n    If a column in the mapping does not exist in the DataFrame, it is ignored."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "reset and drop old index",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/reset and drop old index",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "reset_and_drop_old_index",
                        "extra": {
                            "args": [
                                "df"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Reset the index of a DataFrame to default and drop the old index column if it exists.\n\nThis function reinitializes the DataFrame index and ensures any previous index column is discarded.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n\nReturns:\n    pd.DataFrame: DataFrame with a new default index and without the old index column.\n\nEdge Cases:\n    If there is no old index column to drop, the function behaves like a simple index reset."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "reset index to default",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/reset index to default",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "reset_index_to_default",
                        "extra": {
                            "args": [
                                "df",
                                "drop"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Reset the index of a DataFrame to the default integer index.\n\nThis function reinitializes the DataFrame index, optionally dropping the old index.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    drop (bool, optional): Whether to drop the existing index. Defaults to True.\n\nReturns:\n    pd.DataFrame: DataFrame with the index reset to the default integer index.\n\nEdge Cases:\n    If the index is already the default range index, the DataFrame is returned unchanged."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "standardize date formats",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/standardize date formats",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "standardize_date_formats",
                        "extra": {
                            "args": [
                                "df",
                                "date_columns",
                                "date_format"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Standardize the date formats in specified DataFrame columns.\n\nThis function converts dates in given columns to a uniform date format.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    date_columns (list): List of column names (str) containing date values.\n    date_format (str, optional): The target format for dates. Defaults to \"%Y-%m-%d\".\n\nReturns:\n    pd.DataFrame: DataFrame with dates in standardized format.\n\nEdge Cases:\n    If a date conversion fails, the original value is retained."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "standardize numeric formats",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/standardize numeric formats",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "standardize_numeric_formats",
                        "extra": {
                            "args": [
                                "df",
                                "columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Standardize the numeric formats in specified DataFrame columns.\n\nThis function ensures that numerical columns are formatted uniformly,\nwhich may include rounding, setting precision, or applying consistent scaling.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    columns (list): List of column names (str) to standardize numerically.\n\nReturns:\n    pd.DataFrame: DataFrame with standardized numeric columns.\n\nEdge Cases:\n    If a non-numeric column is included in the columns list, it is skipped."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "standardize string data",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/standardize string data",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "standardize_string_data",
                        "extra": {
                            "args": [
                                "df",
                                "columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Standardize string data in specified DataFrame columns.\n\nThis function ensures that string data in the selected columns is standardized,\nwhich may include trimming, lowercasing, or other normalization.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    columns (list): List of column names (str) to standardize.\n\nReturns:\n    pd.DataFrame: DataFrame with standardized string data in specified columns.\n\nEdge Cases:\n    If a specified column is not of string type, it will be skipped."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "tag outliers",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/tag outliers",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "tag_outliers",
                        "extra": {
                            "args": [
                                "df",
                                "method",
                                "factor"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Tag outliers in a DataFrame based on a specified statistical method.\n\nThis function identifies and tags outlier data points using either the IQR method\nor another specified method and adds a flag column indicating outlier status.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    method (str, optional): Statistical method to use ('IQR' or other). Defaults to 'IQR'.\n    factor (float, optional): Multiplicative factor for determining thresholds. Defaults to 1.5.\n\nReturns:\n    pd.DataFrame: DataFrame with an added column that flags outlier rows.\n\nEdge Cases:\n    If the method is unsupported or no outliers are found, a flag column with default values is added."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "use forward fill",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/use forward fill",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "use_forward_fill",
                        "extra": {
                            "args": [
                                "df",
                                "columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Fill missing values in a DataFrame using forward fill method.\n\nThis function applies a forward-fill strategy to impute missing values,\npropagating the last valid observation forward.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    columns (list, optional): List of column names (str) to forward fill. \n                              If None, applies to all columns.\n\nReturns:\n    pd.DataFrame: DataFrame with missing values filled using forward fill.\n\nEdge Cases:\n    If there are no missing values, the original DataFrame is returned."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "use iqr method",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/use iqr method",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "use_iqr_method",
                        "extra": {
                            "args": [
                                "df",
                                "factor"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Identify outliers in a DataFrame using the IQR method.\n\nThis function computes the interquartile range for numerical columns and flags rows\nthat lie outside the acceptable range defined by a multiplier factor.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    factor (float, optional): Multiplier for the IQR to set thresholds for outlier detection. Defaults to 1.5.\n\nReturns:\n    pd.DataFrame: DataFrame with an indicator marking rows considered outliers based on the IQR method.\n\nEdge Cases:\n    If the IQR cannot be computed (e.g., insufficient data), the original DataFrame is returned."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "use z-score method",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Initial Cleaning/use z-score method",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "use_z_score_method",
                        "extra": {
                            "args": [
                                "df",
                                "threshold"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Identify outliers in a DataFrame using the Z-score method.\n\nThis function calculates the Z-score for numerical columns and flags rows\nwhere the absolute Z-score exceeds the defined threshold.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    threshold (float, optional): Z-score threshold to identify outliers. Defaults to 3.0.\n\nReturns:\n    pd.DataFrame: DataFrame with an additional indicator for outliers based on the Z-score.\n\nEdge Cases:\n    If no outliers are present according to the threshold, the DataFrame is returned unmodified."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/initial_cleaning.py"
                    }
                ]
            },
            {
                "node": "Supplementary Adjustments",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "active learning strategies",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/active learning strategies",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "apply_active_learning_strategy",
                        "extra": {
                            "args": [
                                "df",
                                "strategy_params"
                            ],
                            "return_type": "'pd.DataFrame'",
                            "docstring": "Summary:\n    Applies an active learning strategy to modify the DataFrame for sample selection or re-weighting.\n\nExtended Explanation:\n    This function integrates active learning techniques into the data cleaning process. It processes the provided DataFrame \n    using parameters that define how samples should be selected or weighted. The resulting DataFrame is modified to better \n    suit active learning workflows.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame on which to apply the active learning strategy.\n    strategy_params (dict): A dictionary containing parameters for the active learning strategy, such as sampling \n                            criteria or re-weighting factors.\n\nReturns:\n    pd.DataFrame: A modified DataFrame adjusted according to the active learning strategy.\n\nRaises:\n    ValueError: If strategy_params is missing required keys or contains invalid values."
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "convert to integer",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/convert to integer",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "DataConversionScaler",
                        "extra": {
                            "docstring": "Provides conversion and scaling operations for numeric data in a DataFrame.\n\nThis class includes methods for converting data types to integers and ordinals,\napplying decimal scaling, and standardizing data using robust scaling techniques.",
                            "methods": [
                                {
                                    "name": "convert_to_integer",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Converts specified columns in a DataFrame to integer type.\n\nExtended Explanation:\n    This method is used to ensure that numeric data is stored as integers. It processes\n    the provided columns and converts any float or string representations of numbers to integers.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing the columns to convert.\n    columns (list): List of column names to be converted to integer type.\n\nReturns:\n    pd.DataFrame: A DataFrame with the specified columns converted to integer.\n\nRaises:\n    TypeError: If conversion cannot be performed due to incompatible data types."
                                },
                                {
                                    "name": "convert_to_ordinal",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "mapping"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Converts specified categorical columns to ordinal values.\n\nExtended Explanation:\n    This method applies a mapping dictionary to convert nominal or categorical data into ordinal form,\n    which can facilitate further numeric processing.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing categorical columns.\n    columns (list): List of column names to convert.\n    mapping (dict): Dictionary mapping original values to ordinal integers.\n\nReturns:\n    pd.DataFrame: DataFrame with specified columns converted to ordinal values.\n\nRaises:\n    KeyError: If a column value is missing from the mapping."
                                },
                                {
                                    "name": "apply_decimal_scaling",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Scales numeric columns using decimal scaling.\n\nExtended Explanation:\n    This method normalizes numeric data by shifting the decimal point. This is useful for adjusting the magnitude\n    of numbers without changing their distribution.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing numerical columns.\n    columns (list): List of columns to be scaled.\n\nReturns:\n    pd.DataFrame: DataFrame with scaled numeric columns.\n\nRaises:\n    ValueError: If columns contain non-numeric values."
                                },
                                {
                                    "name": "standardize_using_robust_scaling",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes data using techniques that are robust to outliers.\n\nExtended Explanation:\n    Rather than using mean and standard deviation, this method employs robust statistical measures\n    such as the median and IQR to reduce the influence of extreme values.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing columns to be standardized.\n    columns (list): List of column names to standardize.\n\nReturns:\n    pd.DataFrame: DataFrame with robust-scaled values.\n\nRaises:\n    ValueError: If provided columns contain non-numeric data."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "convert to ordinal",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/convert to ordinal",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "DataConversionScaler",
                        "extra": {
                            "docstring": "Provides conversion and scaling operations for numeric data in a DataFrame.\n\nThis class includes methods for converting data types to integers and ordinals,\napplying decimal scaling, and standardizing data using robust scaling techniques.",
                            "methods": [
                                {
                                    "name": "convert_to_integer",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Converts specified columns in a DataFrame to integer type.\n\nExtended Explanation:\n    This method is used to ensure that numeric data is stored as integers. It processes\n    the provided columns and converts any float or string representations of numbers to integers.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing the columns to convert.\n    columns (list): List of column names to be converted to integer type.\n\nReturns:\n    pd.DataFrame: A DataFrame with the specified columns converted to integer.\n\nRaises:\n    TypeError: If conversion cannot be performed due to incompatible data types."
                                },
                                {
                                    "name": "convert_to_ordinal",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "mapping"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Converts specified categorical columns to ordinal values.\n\nExtended Explanation:\n    This method applies a mapping dictionary to convert nominal or categorical data into ordinal form,\n    which can facilitate further numeric processing.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing categorical columns.\n    columns (list): List of column names to convert.\n    mapping (dict): Dictionary mapping original values to ordinal integers.\n\nReturns:\n    pd.DataFrame: DataFrame with specified columns converted to ordinal values.\n\nRaises:\n    KeyError: If a column value is missing from the mapping."
                                },
                                {
                                    "name": "apply_decimal_scaling",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Scales numeric columns using decimal scaling.\n\nExtended Explanation:\n    This method normalizes numeric data by shifting the decimal point. This is useful for adjusting the magnitude\n    of numbers without changing their distribution.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing numerical columns.\n    columns (list): List of columns to be scaled.\n\nReturns:\n    pd.DataFrame: DataFrame with scaled numeric columns.\n\nRaises:\n    ValueError: If columns contain non-numeric values."
                                },
                                {
                                    "name": "standardize_using_robust_scaling",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes data using techniques that are robust to outliers.\n\nExtended Explanation:\n    Rather than using mean and standard deviation, this method employs robust statistical measures\n    such as the median and IQR to reduce the influence of extreme values.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing columns to be standardized.\n    columns (list): List of column names to standardize.\n\nReturns:\n    pd.DataFrame: DataFrame with robust-scaled values.\n\nRaises:\n    ValueError: If provided columns contain non-numeric data."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "correct data entry errors",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/correct data entry errors",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ConsistencyCorrector",
                        "extra": {
                            "docstring": "Provides methods for correcting inconsistencies and errors in data formatting.\n\nThis class includes functions that adjust data to fixed formats,\ncorrect typographical errors, enforce consistency across entries, and resolve conflicts.",
                            "methods": [
                                {
                                    "name": "fix_inconsistent_data_formats",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "format_rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes data formats in specified columns based on provided rules.\n\nExtended Explanation:\n    This method applies a set of formatting rules to the designated columns to ensure the data\n    adheres to consistent standards.\n\nArgs:\n    df (pd.DataFrame): DataFrame to standardize.\n    columns (list): List of column names to adjust.\n    format_rules (dict): Mapping of column names to formatting rules.\n\nReturns:\n    pd.DataFrame: DataFrame with standardized formats.\n\nRaises:\n    KeyError: If a column is missing from the provided format_rules."
                                },
                                {
                                    "name": "correct_typos",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "typo_dict"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Corrects typographical errors in specified textual columns.\n\nExtended Explanation:\n    This method replaces common typos with their correct forms using a dictionary mapping.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    columns (list): List of columns to inspect for typos.\n    typo_dict (dict): Dictionary mapping incorrect spellings to correct ones.\n\nReturns:\n    pd.DataFrame: DataFrame with typos corrected.\n\nRaises:\n    ValueError: If typo_dict is empty or columns are missing."
                                },
                                {
                                    "name": "enforce_consistency_rules",
                                    "args": [
                                        "self",
                                        "df",
                                        "rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Enforces predefined consistency rules across the DataFrame.\n\nExtended Explanation:\n    This method applies a series of rules to ensure that data across columns adheres to\n    consistent formats and logical relationships.\n\nArgs:\n    df (pd.DataFrame): DataFrame to process.\n    rules (dict): Dictionary containing consistency rules to apply.\n\nReturns:\n    pd.DataFrame: DataFrame with consistency rules enforced.\n\nRaises:\n    ValueError: If rules are improperly defined."
                                },
                                {
                                    "name": "correct_data_entry_errors",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "error_corrections"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Corrects known data entry errors in specified columns.\n\nExtended Explanation:\n    This method uses an error correction mapping to replace incorrect entries due to data entry errors.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing errors.\n    columns (list): List of columns to correct.\n    error_corrections (dict): Dictionary mapping erroneous values to correct ones.\n\nReturns:\n    pd.DataFrame: DataFrame with data entry errors corrected.\n\nRaises:\n    KeyError: If an expected correction key is missing."
                                },
                                {
                                    "name": "resolve_conflicting_data",
                                    "args": [
                                        "self",
                                        "df",
                                        "conflict_resolution_rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Resolves conflicting data entries using specified rules.\n\nExtended Explanation:\n    This method applies conflict resolution strategies to deal with data contradictions,\n    ensuring that the resulting DataFrame has coherent and consistent data.\n\nArgs:\n    df (pd.DataFrame): DataFrame with conflicting entries.\n    conflict_resolution_rules (dict): Dictionary of rules to resolve conflicts.\n\nReturns:\n    pd.DataFrame: DataFrame with resolved conflicting data.\n\nRaises:\n    ValueError: If conflict_resolution_rules do not cover all conflicts."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "correct typos",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/correct typos",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ConsistencyCorrector",
                        "extra": {
                            "docstring": "Provides methods for correcting inconsistencies and errors in data formatting.\n\nThis class includes functions that adjust data to fixed formats,\ncorrect typographical errors, enforce consistency across entries, and resolve conflicts.",
                            "methods": [
                                {
                                    "name": "fix_inconsistent_data_formats",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "format_rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes data formats in specified columns based on provided rules.\n\nExtended Explanation:\n    This method applies a set of formatting rules to the designated columns to ensure the data\n    adheres to consistent standards.\n\nArgs:\n    df (pd.DataFrame): DataFrame to standardize.\n    columns (list): List of column names to adjust.\n    format_rules (dict): Mapping of column names to formatting rules.\n\nReturns:\n    pd.DataFrame: DataFrame with standardized formats.\n\nRaises:\n    KeyError: If a column is missing from the provided format_rules."
                                },
                                {
                                    "name": "correct_typos",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "typo_dict"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Corrects typographical errors in specified textual columns.\n\nExtended Explanation:\n    This method replaces common typos with their correct forms using a dictionary mapping.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    columns (list): List of columns to inspect for typos.\n    typo_dict (dict): Dictionary mapping incorrect spellings to correct ones.\n\nReturns:\n    pd.DataFrame: DataFrame with typos corrected.\n\nRaises:\n    ValueError: If typo_dict is empty or columns are missing."
                                },
                                {
                                    "name": "enforce_consistency_rules",
                                    "args": [
                                        "self",
                                        "df",
                                        "rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Enforces predefined consistency rules across the DataFrame.\n\nExtended Explanation:\n    This method applies a series of rules to ensure that data across columns adheres to\n    consistent formats and logical relationships.\n\nArgs:\n    df (pd.DataFrame): DataFrame to process.\n    rules (dict): Dictionary containing consistency rules to apply.\n\nReturns:\n    pd.DataFrame: DataFrame with consistency rules enforced.\n\nRaises:\n    ValueError: If rules are improperly defined."
                                },
                                {
                                    "name": "correct_data_entry_errors",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "error_corrections"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Corrects known data entry errors in specified columns.\n\nExtended Explanation:\n    This method uses an error correction mapping to replace incorrect entries due to data entry errors.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing errors.\n    columns (list): List of columns to correct.\n    error_corrections (dict): Dictionary mapping erroneous values to correct ones.\n\nReturns:\n    pd.DataFrame: DataFrame with data entry errors corrected.\n\nRaises:\n    KeyError: If an expected correction key is missing."
                                },
                                {
                                    "name": "resolve_conflicting_data",
                                    "args": [
                                        "self",
                                        "df",
                                        "conflict_resolution_rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Resolves conflicting data entries using specified rules.\n\nExtended Explanation:\n    This method applies conflict resolution strategies to deal with data contradictions,\n    ensuring that the resulting DataFrame has coherent and consistent data.\n\nArgs:\n    df (pd.DataFrame): DataFrame with conflicting entries.\n    conflict_resolution_rules (dict): Dictionary of rules to resolve conflicts.\n\nReturns:\n    pd.DataFrame: DataFrame with resolved conflicting data.\n\nRaises:\n    ValueError: If conflict_resolution_rules do not cover all conflicts."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "decimal scaling",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/decimal scaling",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "DataConversionScaler",
                        "extra": {
                            "docstring": "Provides conversion and scaling operations for numeric data in a DataFrame.\n\nThis class includes methods for converting data types to integers and ordinals,\napplying decimal scaling, and standardizing data using robust scaling techniques.",
                            "methods": [
                                {
                                    "name": "convert_to_integer",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Converts specified columns in a DataFrame to integer type.\n\nExtended Explanation:\n    This method is used to ensure that numeric data is stored as integers. It processes\n    the provided columns and converts any float or string representations of numbers to integers.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing the columns to convert.\n    columns (list): List of column names to be converted to integer type.\n\nReturns:\n    pd.DataFrame: A DataFrame with the specified columns converted to integer.\n\nRaises:\n    TypeError: If conversion cannot be performed due to incompatible data types."
                                },
                                {
                                    "name": "convert_to_ordinal",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "mapping"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Converts specified categorical columns to ordinal values.\n\nExtended Explanation:\n    This method applies a mapping dictionary to convert nominal or categorical data into ordinal form,\n    which can facilitate further numeric processing.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing categorical columns.\n    columns (list): List of column names to convert.\n    mapping (dict): Dictionary mapping original values to ordinal integers.\n\nReturns:\n    pd.DataFrame: DataFrame with specified columns converted to ordinal values.\n\nRaises:\n    KeyError: If a column value is missing from the mapping."
                                },
                                {
                                    "name": "apply_decimal_scaling",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Scales numeric columns using decimal scaling.\n\nExtended Explanation:\n    This method normalizes numeric data by shifting the decimal point. This is useful for adjusting the magnitude\n    of numbers without changing their distribution.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing numerical columns.\n    columns (list): List of columns to be scaled.\n\nReturns:\n    pd.DataFrame: DataFrame with scaled numeric columns.\n\nRaises:\n    ValueError: If columns contain non-numeric values."
                                },
                                {
                                    "name": "standardize_using_robust_scaling",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes data using techniques that are robust to outliers.\n\nExtended Explanation:\n    Rather than using mean and standard deviation, this method employs robust statistical measures\n    such as the median and IQR to reduce the influence of extreme values.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing columns to be standardized.\n    columns (list): List of column names to standardize.\n\nReturns:\n    pd.DataFrame: DataFrame with robust-scaled values.\n\nRaises:\n    ValueError: If provided columns contain non-numeric data."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "drop columns with all zeros",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/drop columns with all zeros",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ZeroColumnHandler",
                        "extra": {
                            "docstring": "Provides methods for identifying and handling columns composed entirely or predominantly of zeros.\n\nThis class includes methods to detect columns with a high proportion of zeroes and to remove columns which contain \nonly zero values.",
                            "methods": [
                                {
                                    "name": "identify_zero_columns",
                                    "args": [
                                        "self",
                                        "df",
                                        "threshold"
                                    ],
                                    "return_type": "list",
                                    "docstring": "Summary:\n    Identifies columns in the DataFrame where the proportion of zero values meets or exceeds the threshold.\n\nExtended Explanation:\n    This method evaluates each column in the DataFrame and returns a list of column names that have a zero-value\n    proportion greater than or equal to the provided threshold.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to inspect.\n    threshold (float, optional): The minimum proportion of zeros required to classify a column as zero-dominated.\n                                 Defaults to 0.0 (all zeros).\n\nReturns:\n    list: A list of column names that meet the zero column criteria.\n\nRaises:\n    ValueError: If the threshold is not between 0 and 1."
                                },
                                {
                                    "name": "drop_columns_with_all_zeros",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Drops columns from the DataFrame that contain only zeros.\n\nExtended Explanation:\n    This method scans each column and removes those where every element is zero. This is useful for cleaning\n    datasets that have redundant or non-informative zero-only columns.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n\nReturns:\n    pd.DataFrame: A DataFrame with columns containing only zeros removed.\n\nRaises:\n    ValueError: If no columns can be dropped due to incompatible data types."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "drop duplicate columns",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/drop duplicate columns",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "DuplicateCleaner",
                        "extra": {
                            "docstring": "Implements methods for dealing with duplicate data in a DataFrame.\n\nThis class covers keeping the first occurrence of duplicate records, dropping duplicate columns,\nremoving exact duplicate rows, and handling near-duplicate rows through fuzzy deduplication techniques.",
                            "methods": [
                                {
                                    "name": "keep_first",
                                    "args": [
                                        "self",
                                        "df",
                                        "subset"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Keeps the first occurrence of duplicate rows and removes subsequent duplicates.\n\nExtended Explanation:\n    This method checks for duplicate rows based on all columns or a specified subset of columns,\n    and retains only the first occurrence while removing others.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing possible duplicate rows.\n    subset (list, optional): List of column names to consider for identifying duplicates.\n\nReturns:\n    pd.DataFrame: DataFrame with duplicates removed, preserving the first occurrence.\n\nRaises:\n    ValueError: If 'df' is not a valid DataFrame."
                                },
                                {
                                    "name": "drop_duplicate_columns",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes columns that are exact duplicates of each other.\n\nExtended Explanation:\n    This method examines the DataFrame for columns that have identical content and drops the redundant ones.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n\nReturns:\n    pd.DataFrame: DataFrame with duplicate columns removed.\n\nRaises:\n    ValueError: If the DataFrame is empty or improperly formatted."
                                },
                                {
                                    "name": "remove_exact_duplicates",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes rows that are exactly duplicated.\n\nExtended Explanation:\n    This method identifies rows that are completely identical across all columns and removes the duplicates.\n\nArgs:\n    df (pd.DataFrame): DataFrame to be de-duplicated.\n\nReturns:\n    pd.DataFrame: DataFrame with exact duplicate rows removed.\n\nRaises:\n    ValueError: If 'df' has unexpected structure."
                                },
                                {
                                    "name": "fuzzy_deduplication",
                                    "args": [
                                        "self",
                                        "df",
                                        "similarity_threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Applies fuzzy matching to identify and remove near-duplicate rows.\n\nExtended Explanation:\n    This method uses a similarity threshold to determine which rows are sufficiently similar \n    to be considered duplicates and then removes or flags them.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame to process.\n    similarity_threshold (float): Similarity threshold (between 0 and 1); values closer to 1 denote stricter matching.\n\nReturns:\n    pd.DataFrame: DataFrame with near-duplicates handled.\n\nRaises:\n    ValueError: If similarity_threshold is outside the [0, 1] range."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "enforce consistency rules",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/enforce consistency rules",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ConsistencyCorrector",
                        "extra": {
                            "docstring": "Provides methods for correcting inconsistencies and errors in data formatting.\n\nThis class includes functions that adjust data to fixed formats,\ncorrect typographical errors, enforce consistency across entries, and resolve conflicts.",
                            "methods": [
                                {
                                    "name": "fix_inconsistent_data_formats",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "format_rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes data formats in specified columns based on provided rules.\n\nExtended Explanation:\n    This method applies a set of formatting rules to the designated columns to ensure the data\n    adheres to consistent standards.\n\nArgs:\n    df (pd.DataFrame): DataFrame to standardize.\n    columns (list): List of column names to adjust.\n    format_rules (dict): Mapping of column names to formatting rules.\n\nReturns:\n    pd.DataFrame: DataFrame with standardized formats.\n\nRaises:\n    KeyError: If a column is missing from the provided format_rules."
                                },
                                {
                                    "name": "correct_typos",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "typo_dict"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Corrects typographical errors in specified textual columns.\n\nExtended Explanation:\n    This method replaces common typos with their correct forms using a dictionary mapping.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    columns (list): List of columns to inspect for typos.\n    typo_dict (dict): Dictionary mapping incorrect spellings to correct ones.\n\nReturns:\n    pd.DataFrame: DataFrame with typos corrected.\n\nRaises:\n    ValueError: If typo_dict is empty or columns are missing."
                                },
                                {
                                    "name": "enforce_consistency_rules",
                                    "args": [
                                        "self",
                                        "df",
                                        "rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Enforces predefined consistency rules across the DataFrame.\n\nExtended Explanation:\n    This method applies a series of rules to ensure that data across columns adheres to\n    consistent formats and logical relationships.\n\nArgs:\n    df (pd.DataFrame): DataFrame to process.\n    rules (dict): Dictionary containing consistency rules to apply.\n\nReturns:\n    pd.DataFrame: DataFrame with consistency rules enforced.\n\nRaises:\n    ValueError: If rules are improperly defined."
                                },
                                {
                                    "name": "correct_data_entry_errors",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "error_corrections"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Corrects known data entry errors in specified columns.\n\nExtended Explanation:\n    This method uses an error correction mapping to replace incorrect entries due to data entry errors.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing errors.\n    columns (list): List of columns to correct.\n    error_corrections (dict): Dictionary mapping erroneous values to correct ones.\n\nReturns:\n    pd.DataFrame: DataFrame with data entry errors corrected.\n\nRaises:\n    KeyError: If an expected correction key is missing."
                                },
                                {
                                    "name": "resolve_conflicting_data",
                                    "args": [
                                        "self",
                                        "df",
                                        "conflict_resolution_rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Resolves conflicting data entries using specified rules.\n\nExtended Explanation:\n    This method applies conflict resolution strategies to deal with data contradictions,\n    ensuring that the resulting DataFrame has coherent and consistent data.\n\nArgs:\n    df (pd.DataFrame): DataFrame with conflicting entries.\n    conflict_resolution_rules (dict): Dictionary of rules to resolve conflicts.\n\nReturns:\n    pd.DataFrame: DataFrame with resolved conflicting data.\n\nRaises:\n    ValueError: If conflict_resolution_rules do not cover all conflicts."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "fill with custom value",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/fill with custom value",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "CategoricalCleaner",
                        "extra": {
                            "docstring": "Provides methods for cleaning and standardizing categorical or textual data in a DataFrame.\n\nThis includes operations to standardize categorical formats, remove special characters using regex,\nand fill missing or problematic entries with either custom values or zero.",
                            "methods": [
                                {
                                    "name": "standardize_categorical_data",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes the representation of categorical data in the DataFrame.\n\nExtended Explanation:\n    This method adjusts the string formats in the specified columns to a consistent standard, which\n    may involve trimming spaces, changing case, or other normalization procedures.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    columns (list): List of columns containing categorical data to standardize.\n\nReturns:\n    pd.DataFrame: DataFrame with standardized categorical data.\n\nRaises:\n    ValueError: If any of the specified columns are not of a string type."
                                },
                                {
                                    "name": "remove_special_characters",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "pattern"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes special characters from the specified string columns.\n\nExtended Explanation:\n    Using a regular expression pattern, this method cleans text data by removing characters that\n    do not match the allowed set defined by the pattern.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing textual data.\n    columns (list): List of column names to clean.\n    pattern (str): Regex pattern specifying allowed characters. Defaults to alphanumeric and whitespace.\n\nReturns:\n    pd.DataFrame: DataFrame with special characters removed from specified columns.\n\nRaises:\n    re.error: If the provided regex pattern is invalid."
                                },
                                {
                                    "name": "fill_with_custom_value",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "custom_value"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Fills missing or problematic entries in specified columns with a custom value.\n\nExtended Explanation:\n    This method replaces NaNs or erroneous entries within the given columns with a predetermined custom value.\n\nArgs:\n    df (pd.DataFrame): The DataFrame with missing or problematic entries.\n    columns (list): List of columns to fill.\n    custom_value: The value to use for filling missing entries.\n\nReturns:\n    pd.DataFrame: DataFrame with missing entries replaced by the custom value.\n\nRaises:\n    TypeError: If the custom_value is of the wrong type for the target column."
                                },
                                {
                                    "name": "fill_with_zero",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Fills missing or invalid values in designated columns with zero.\n\nExtended Explanation:\n    This method scans the specified columns for NaN or other placeholders for missing data and fills them with zero.\n\nArgs:\n    df (pd.DataFrame): DataFrame to process.\n    columns (list): List of column names where zeros will replace missing values.\n\nReturns:\n    pd.DataFrame: DataFrame with missing values filled with zero.\n\nRaises:\n    ValueError: If the operation fails due to type incompatibilities."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "fill with zero",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/fill with zero",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "CategoricalCleaner",
                        "extra": {
                            "docstring": "Provides methods for cleaning and standardizing categorical or textual data in a DataFrame.\n\nThis includes operations to standardize categorical formats, remove special characters using regex,\nand fill missing or problematic entries with either custom values or zero.",
                            "methods": [
                                {
                                    "name": "standardize_categorical_data",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes the representation of categorical data in the DataFrame.\n\nExtended Explanation:\n    This method adjusts the string formats in the specified columns to a consistent standard, which\n    may involve trimming spaces, changing case, or other normalization procedures.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    columns (list): List of columns containing categorical data to standardize.\n\nReturns:\n    pd.DataFrame: DataFrame with standardized categorical data.\n\nRaises:\n    ValueError: If any of the specified columns are not of a string type."
                                },
                                {
                                    "name": "remove_special_characters",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "pattern"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes special characters from the specified string columns.\n\nExtended Explanation:\n    Using a regular expression pattern, this method cleans text data by removing characters that\n    do not match the allowed set defined by the pattern.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing textual data.\n    columns (list): List of column names to clean.\n    pattern (str): Regex pattern specifying allowed characters. Defaults to alphanumeric and whitespace.\n\nReturns:\n    pd.DataFrame: DataFrame with special characters removed from specified columns.\n\nRaises:\n    re.error: If the provided regex pattern is invalid."
                                },
                                {
                                    "name": "fill_with_custom_value",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "custom_value"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Fills missing or problematic entries in specified columns with a custom value.\n\nExtended Explanation:\n    This method replaces NaNs or erroneous entries within the given columns with a predetermined custom value.\n\nArgs:\n    df (pd.DataFrame): The DataFrame with missing or problematic entries.\n    columns (list): List of columns to fill.\n    custom_value: The value to use for filling missing entries.\n\nReturns:\n    pd.DataFrame: DataFrame with missing entries replaced by the custom value.\n\nRaises:\n    TypeError: If the custom_value is of the wrong type for the target column."
                                },
                                {
                                    "name": "fill_with_zero",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Fills missing or invalid values in designated columns with zero.\n\nExtended Explanation:\n    This method scans the specified columns for NaN or other placeholders for missing data and fills them with zero.\n\nArgs:\n    df (pd.DataFrame): DataFrame to process.\n    columns (list): List of column names where zeros will replace missing values.\n\nReturns:\n    pd.DataFrame: DataFrame with missing values filled with zero.\n\nRaises:\n    ValueError: If the operation fails due to type incompatibilities."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "filter based on percentile",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/filter based on percentile",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "OutlierFilter",
                        "extra": {
                            "docstring": "Provides methods for identifying and handling outliers in a DataFrame.\n\nThis class supports multiple techniques including filtering by percentile, identifying high outliers,\nthreshold based filtering, IQR-based removal, and generalized outlier identification.",
                            "methods": [
                                {
                                    "name": "filter_by_percentile",
                                    "args": [
                                        "self",
                                        "df",
                                        "percentile"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters rows in the DataFrame based on a specified percentile threshold.\n\nExtended Explanation:\n    This method removes or marks rows in the DataFrame that fall below or above a given percentile value,\n    depending on the implementation of thresholding logic. It is useful for mitigating the influence of extreme values.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame to filter.\n    percentile (float): The percentile threshold (between 0 and 100) to use for filtering.\n\nReturns:\n    pd.DataFrame: A DataFrame with rows filtered based on the percentile.\n\nRaises:\n    ValueError: If the percentile is not between 0 and 100."
                                },
                                {
                                    "name": "filter_high_outliers",
                                    "args": [
                                        "self",
                                        "df",
                                        "high_threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters out rows that contain exceptionally high outlier values.\n\nExtended Explanation:\n    This method detects and filters rows where the values exceed a specified high threshold.\n    It can be applied to a specific column or across the DataFrame if aggregated.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    high_threshold (float): The threshold above which values are considered high outliers.\n\nReturns:\n    pd.DataFrame: DataFrame with high outliers removed or flagged.\n\nRaises:\n    ValueError: If high_threshold is not a positive number."
                                },
                                {
                                    "name": "filter_by_threshold",
                                    "args": [
                                        "self",
                                        "df",
                                        "threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters rows based on a fixed numeric threshold.\n\nExtended Explanation:\n    This method removes rows where specified numeric values exceed or fall short of a given threshold,\n    depending on business requirements.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    threshold (float): Numeric threshold for filtering.\n\nReturns:\n    pd.DataFrame: DataFrame filtered according to the threshold.\n\nRaises:\n    ValueError: If threshold is non-numeric."
                                },
                                {
                                    "name": "remove_based_on_iqr",
                                    "args": [
                                        "self",
                                        "df",
                                        "factor"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes outliers using the IQR method.\n\nExtended Explanation:\n    This method calculates the Interquartile Range (IQR) and filters out rows that\n    lie beyond the range defined by the specified factor multiplied by the IQR.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    factor (float): The multiplier for the IQR to define the acceptable range.\n\nReturns:\n    pd.DataFrame: A DataFrame with outliers removed based on the IQR method.\n\nRaises:\n    ValueError: If factor is not a positive number."
                                },
                                {
                                    "name": "identify_outliers",
                                    "args": [
                                        "self",
                                        "df",
                                        "method"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Identifies outliers in the DataFrame using a specified detection method.\n\nExtended Explanation:\n    This method labels or flags rows that are potential outliers following the specified \n    detection strategy (e.g., standard deviation, modified z-score, etc.). No rows are removed.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    method (str): The method used for detecting outliers. Defaults to 'standard'.\n\nReturns:\n    pd.DataFrame: DataFrame with an additional column flagging outliers.\n\nRaises:\n    ValueError: If the specified method is not supported."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "filter by threshold",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/filter by threshold",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "OutlierFilter",
                        "extra": {
                            "docstring": "Provides methods for identifying and handling outliers in a DataFrame.\n\nThis class supports multiple techniques including filtering by percentile, identifying high outliers,\nthreshold based filtering, IQR-based removal, and generalized outlier identification.",
                            "methods": [
                                {
                                    "name": "filter_by_percentile",
                                    "args": [
                                        "self",
                                        "df",
                                        "percentile"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters rows in the DataFrame based on a specified percentile threshold.\n\nExtended Explanation:\n    This method removes or marks rows in the DataFrame that fall below or above a given percentile value,\n    depending on the implementation of thresholding logic. It is useful for mitigating the influence of extreme values.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame to filter.\n    percentile (float): The percentile threshold (between 0 and 100) to use for filtering.\n\nReturns:\n    pd.DataFrame: A DataFrame with rows filtered based on the percentile.\n\nRaises:\n    ValueError: If the percentile is not between 0 and 100."
                                },
                                {
                                    "name": "filter_high_outliers",
                                    "args": [
                                        "self",
                                        "df",
                                        "high_threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters out rows that contain exceptionally high outlier values.\n\nExtended Explanation:\n    This method detects and filters rows where the values exceed a specified high threshold.\n    It can be applied to a specific column or across the DataFrame if aggregated.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    high_threshold (float): The threshold above which values are considered high outliers.\n\nReturns:\n    pd.DataFrame: DataFrame with high outliers removed or flagged.\n\nRaises:\n    ValueError: If high_threshold is not a positive number."
                                },
                                {
                                    "name": "filter_by_threshold",
                                    "args": [
                                        "self",
                                        "df",
                                        "threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters rows based on a fixed numeric threshold.\n\nExtended Explanation:\n    This method removes rows where specified numeric values exceed or fall short of a given threshold,\n    depending on business requirements.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    threshold (float): Numeric threshold for filtering.\n\nReturns:\n    pd.DataFrame: DataFrame filtered according to the threshold.\n\nRaises:\n    ValueError: If threshold is non-numeric."
                                },
                                {
                                    "name": "remove_based_on_iqr",
                                    "args": [
                                        "self",
                                        "df",
                                        "factor"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes outliers using the IQR method.\n\nExtended Explanation:\n    This method calculates the Interquartile Range (IQR) and filters out rows that\n    lie beyond the range defined by the specified factor multiplied by the IQR.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    factor (float): The multiplier for the IQR to define the acceptable range.\n\nReturns:\n    pd.DataFrame: A DataFrame with outliers removed based on the IQR method.\n\nRaises:\n    ValueError: If factor is not a positive number."
                                },
                                {
                                    "name": "identify_outliers",
                                    "args": [
                                        "self",
                                        "df",
                                        "method"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Identifies outliers in the DataFrame using a specified detection method.\n\nExtended Explanation:\n    This method labels or flags rows that are potential outliers following the specified \n    detection strategy (e.g., standard deviation, modified z-score, etc.). No rows are removed.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    method (str): The method used for detecting outliers. Defaults to 'standard'.\n\nReturns:\n    pd.DataFrame: DataFrame with an additional column flagging outliers.\n\nRaises:\n    ValueError: If the specified method is not supported."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "filter high outliers",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/filter high outliers",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "OutlierFilter",
                        "extra": {
                            "docstring": "Provides methods for identifying and handling outliers in a DataFrame.\n\nThis class supports multiple techniques including filtering by percentile, identifying high outliers,\nthreshold based filtering, IQR-based removal, and generalized outlier identification.",
                            "methods": [
                                {
                                    "name": "filter_by_percentile",
                                    "args": [
                                        "self",
                                        "df",
                                        "percentile"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters rows in the DataFrame based on a specified percentile threshold.\n\nExtended Explanation:\n    This method removes or marks rows in the DataFrame that fall below or above a given percentile value,\n    depending on the implementation of thresholding logic. It is useful for mitigating the influence of extreme values.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame to filter.\n    percentile (float): The percentile threshold (between 0 and 100) to use for filtering.\n\nReturns:\n    pd.DataFrame: A DataFrame with rows filtered based on the percentile.\n\nRaises:\n    ValueError: If the percentile is not between 0 and 100."
                                },
                                {
                                    "name": "filter_high_outliers",
                                    "args": [
                                        "self",
                                        "df",
                                        "high_threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters out rows that contain exceptionally high outlier values.\n\nExtended Explanation:\n    This method detects and filters rows where the values exceed a specified high threshold.\n    It can be applied to a specific column or across the DataFrame if aggregated.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    high_threshold (float): The threshold above which values are considered high outliers.\n\nReturns:\n    pd.DataFrame: DataFrame with high outliers removed or flagged.\n\nRaises:\n    ValueError: If high_threshold is not a positive number."
                                },
                                {
                                    "name": "filter_by_threshold",
                                    "args": [
                                        "self",
                                        "df",
                                        "threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters rows based on a fixed numeric threshold.\n\nExtended Explanation:\n    This method removes rows where specified numeric values exceed or fall short of a given threshold,\n    depending on business requirements.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    threshold (float): Numeric threshold for filtering.\n\nReturns:\n    pd.DataFrame: DataFrame filtered according to the threshold.\n\nRaises:\n    ValueError: If threshold is non-numeric."
                                },
                                {
                                    "name": "remove_based_on_iqr",
                                    "args": [
                                        "self",
                                        "df",
                                        "factor"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes outliers using the IQR method.\n\nExtended Explanation:\n    This method calculates the Interquartile Range (IQR) and filters out rows that\n    lie beyond the range defined by the specified factor multiplied by the IQR.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    factor (float): The multiplier for the IQR to define the acceptable range.\n\nReturns:\n    pd.DataFrame: A DataFrame with outliers removed based on the IQR method.\n\nRaises:\n    ValueError: If factor is not a positive number."
                                },
                                {
                                    "name": "identify_outliers",
                                    "args": [
                                        "self",
                                        "df",
                                        "method"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Identifies outliers in the DataFrame using a specified detection method.\n\nExtended Explanation:\n    This method labels or flags rows that are potential outliers following the specified \n    detection strategy (e.g., standard deviation, modified z-score, etc.). No rows are removed.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    method (str): The method used for detecting outliers. Defaults to 'standard'.\n\nReturns:\n    pd.DataFrame: DataFrame with an additional column flagging outliers.\n\nRaises:\n    ValueError: If the specified method is not supported."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "fix inconsistent data formats",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/fix inconsistent data formats",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ConsistencyCorrector",
                        "extra": {
                            "docstring": "Provides methods for correcting inconsistencies and errors in data formatting.\n\nThis class includes functions that adjust data to fixed formats,\ncorrect typographical errors, enforce consistency across entries, and resolve conflicts.",
                            "methods": [
                                {
                                    "name": "fix_inconsistent_data_formats",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "format_rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes data formats in specified columns based on provided rules.\n\nExtended Explanation:\n    This method applies a set of formatting rules to the designated columns to ensure the data\n    adheres to consistent standards.\n\nArgs:\n    df (pd.DataFrame): DataFrame to standardize.\n    columns (list): List of column names to adjust.\n    format_rules (dict): Mapping of column names to formatting rules.\n\nReturns:\n    pd.DataFrame: DataFrame with standardized formats.\n\nRaises:\n    KeyError: If a column is missing from the provided format_rules."
                                },
                                {
                                    "name": "correct_typos",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "typo_dict"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Corrects typographical errors in specified textual columns.\n\nExtended Explanation:\n    This method replaces common typos with their correct forms using a dictionary mapping.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    columns (list): List of columns to inspect for typos.\n    typo_dict (dict): Dictionary mapping incorrect spellings to correct ones.\n\nReturns:\n    pd.DataFrame: DataFrame with typos corrected.\n\nRaises:\n    ValueError: If typo_dict is empty or columns are missing."
                                },
                                {
                                    "name": "enforce_consistency_rules",
                                    "args": [
                                        "self",
                                        "df",
                                        "rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Enforces predefined consistency rules across the DataFrame.\n\nExtended Explanation:\n    This method applies a series of rules to ensure that data across columns adheres to\n    consistent formats and logical relationships.\n\nArgs:\n    df (pd.DataFrame): DataFrame to process.\n    rules (dict): Dictionary containing consistency rules to apply.\n\nReturns:\n    pd.DataFrame: DataFrame with consistency rules enforced.\n\nRaises:\n    ValueError: If rules are improperly defined."
                                },
                                {
                                    "name": "correct_data_entry_errors",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "error_corrections"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Corrects known data entry errors in specified columns.\n\nExtended Explanation:\n    This method uses an error correction mapping to replace incorrect entries due to data entry errors.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing errors.\n    columns (list): List of columns to correct.\n    error_corrections (dict): Dictionary mapping erroneous values to correct ones.\n\nReturns:\n    pd.DataFrame: DataFrame with data entry errors corrected.\n\nRaises:\n    KeyError: If an expected correction key is missing."
                                },
                                {
                                    "name": "resolve_conflicting_data",
                                    "args": [
                                        "self",
                                        "df",
                                        "conflict_resolution_rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Resolves conflicting data entries using specified rules.\n\nExtended Explanation:\n    This method applies conflict resolution strategies to deal with data contradictions,\n    ensuring that the resulting DataFrame has coherent and consistent data.\n\nArgs:\n    df (pd.DataFrame): DataFrame with conflicting entries.\n    conflict_resolution_rules (dict): Dictionary of rules to resolve conflicts.\n\nReturns:\n    pd.DataFrame: DataFrame with resolved conflicting data.\n\nRaises:\n    ValueError: If conflict_resolution_rules do not cover all conflicts."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "fuzzy deduplication",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/fuzzy deduplication",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "DuplicateCleaner",
                        "extra": {
                            "docstring": "Implements methods for dealing with duplicate data in a DataFrame.\n\nThis class covers keeping the first occurrence of duplicate records, dropping duplicate columns,\nremoving exact duplicate rows, and handling near-duplicate rows through fuzzy deduplication techniques.",
                            "methods": [
                                {
                                    "name": "keep_first",
                                    "args": [
                                        "self",
                                        "df",
                                        "subset"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Keeps the first occurrence of duplicate rows and removes subsequent duplicates.\n\nExtended Explanation:\n    This method checks for duplicate rows based on all columns or a specified subset of columns,\n    and retains only the first occurrence while removing others.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing possible duplicate rows.\n    subset (list, optional): List of column names to consider for identifying duplicates.\n\nReturns:\n    pd.DataFrame: DataFrame with duplicates removed, preserving the first occurrence.\n\nRaises:\n    ValueError: If 'df' is not a valid DataFrame."
                                },
                                {
                                    "name": "drop_duplicate_columns",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes columns that are exact duplicates of each other.\n\nExtended Explanation:\n    This method examines the DataFrame for columns that have identical content and drops the redundant ones.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n\nReturns:\n    pd.DataFrame: DataFrame with duplicate columns removed.\n\nRaises:\n    ValueError: If the DataFrame is empty or improperly formatted."
                                },
                                {
                                    "name": "remove_exact_duplicates",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes rows that are exactly duplicated.\n\nExtended Explanation:\n    This method identifies rows that are completely identical across all columns and removes the duplicates.\n\nArgs:\n    df (pd.DataFrame): DataFrame to be de-duplicated.\n\nReturns:\n    pd.DataFrame: DataFrame with exact duplicate rows removed.\n\nRaises:\n    ValueError: If 'df' has unexpected structure."
                                },
                                {
                                    "name": "fuzzy_deduplication",
                                    "args": [
                                        "self",
                                        "df",
                                        "similarity_threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Applies fuzzy matching to identify and remove near-duplicate rows.\n\nExtended Explanation:\n    This method uses a similarity threshold to determine which rows are sufficiently similar \n    to be considered duplicates and then removes or flags them.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame to process.\n    similarity_threshold (float): Similarity threshold (between 0 and 1); values closer to 1 denote stricter matching.\n\nReturns:\n    pd.DataFrame: DataFrame with near-duplicates handled.\n\nRaises:\n    ValueError: If similarity_threshold is outside the [0, 1] range."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "identify outliers",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/identify outliers",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "OutlierFilter",
                        "extra": {
                            "docstring": "Provides methods for identifying and handling outliers in a DataFrame.\n\nThis class supports multiple techniques including filtering by percentile, identifying high outliers,\nthreshold based filtering, IQR-based removal, and generalized outlier identification.",
                            "methods": [
                                {
                                    "name": "filter_by_percentile",
                                    "args": [
                                        "self",
                                        "df",
                                        "percentile"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters rows in the DataFrame based on a specified percentile threshold.\n\nExtended Explanation:\n    This method removes or marks rows in the DataFrame that fall below or above a given percentile value,\n    depending on the implementation of thresholding logic. It is useful for mitigating the influence of extreme values.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame to filter.\n    percentile (float): The percentile threshold (between 0 and 100) to use for filtering.\n\nReturns:\n    pd.DataFrame: A DataFrame with rows filtered based on the percentile.\n\nRaises:\n    ValueError: If the percentile is not between 0 and 100."
                                },
                                {
                                    "name": "filter_high_outliers",
                                    "args": [
                                        "self",
                                        "df",
                                        "high_threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters out rows that contain exceptionally high outlier values.\n\nExtended Explanation:\n    This method detects and filters rows where the values exceed a specified high threshold.\n    It can be applied to a specific column or across the DataFrame if aggregated.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    high_threshold (float): The threshold above which values are considered high outliers.\n\nReturns:\n    pd.DataFrame: DataFrame with high outliers removed or flagged.\n\nRaises:\n    ValueError: If high_threshold is not a positive number."
                                },
                                {
                                    "name": "filter_by_threshold",
                                    "args": [
                                        "self",
                                        "df",
                                        "threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters rows based on a fixed numeric threshold.\n\nExtended Explanation:\n    This method removes rows where specified numeric values exceed or fall short of a given threshold,\n    depending on business requirements.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    threshold (float): Numeric threshold for filtering.\n\nReturns:\n    pd.DataFrame: DataFrame filtered according to the threshold.\n\nRaises:\n    ValueError: If threshold is non-numeric."
                                },
                                {
                                    "name": "remove_based_on_iqr",
                                    "args": [
                                        "self",
                                        "df",
                                        "factor"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes outliers using the IQR method.\n\nExtended Explanation:\n    This method calculates the Interquartile Range (IQR) and filters out rows that\n    lie beyond the range defined by the specified factor multiplied by the IQR.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    factor (float): The multiplier for the IQR to define the acceptable range.\n\nReturns:\n    pd.DataFrame: A DataFrame with outliers removed based on the IQR method.\n\nRaises:\n    ValueError: If factor is not a positive number."
                                },
                                {
                                    "name": "identify_outliers",
                                    "args": [
                                        "self",
                                        "df",
                                        "method"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Identifies outliers in the DataFrame using a specified detection method.\n\nExtended Explanation:\n    This method labels or flags rows that are potential outliers following the specified \n    detection strategy (e.g., standard deviation, modified z-score, etc.). No rows are removed.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    method (str): The method used for detecting outliers. Defaults to 'standard'.\n\nReturns:\n    pd.DataFrame: DataFrame with an additional column flagging outliers.\n\nRaises:\n    ValueError: If the specified method is not supported."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "identify zero columns",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/identify zero columns",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ZeroColumnHandler",
                        "extra": {
                            "docstring": "Provides methods for identifying and handling columns composed entirely or predominantly of zeros.\n\nThis class includes methods to detect columns with a high proportion of zeroes and to remove columns which contain \nonly zero values.",
                            "methods": [
                                {
                                    "name": "identify_zero_columns",
                                    "args": [
                                        "self",
                                        "df",
                                        "threshold"
                                    ],
                                    "return_type": "list",
                                    "docstring": "Summary:\n    Identifies columns in the DataFrame where the proportion of zero values meets or exceeds the threshold.\n\nExtended Explanation:\n    This method evaluates each column in the DataFrame and returns a list of column names that have a zero-value\n    proportion greater than or equal to the provided threshold.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to inspect.\n    threshold (float, optional): The minimum proportion of zeros required to classify a column as zero-dominated.\n                                 Defaults to 0.0 (all zeros).\n\nReturns:\n    list: A list of column names that meet the zero column criteria.\n\nRaises:\n    ValueError: If the threshold is not between 0 and 1."
                                },
                                {
                                    "name": "drop_columns_with_all_zeros",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Drops columns from the DataFrame that contain only zeros.\n\nExtended Explanation:\n    This method scans each column and removes those where every element is zero. This is useful for cleaning\n    datasets that have redundant or non-informative zero-only columns.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n\nReturns:\n    pd.DataFrame: A DataFrame with columns containing only zeros removed.\n\nRaises:\n    ValueError: If no columns can be dropped due to incompatible data types."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "keep first",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/keep first",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "DuplicateCleaner",
                        "extra": {
                            "docstring": "Implements methods for dealing with duplicate data in a DataFrame.\n\nThis class covers keeping the first occurrence of duplicate records, dropping duplicate columns,\nremoving exact duplicate rows, and handling near-duplicate rows through fuzzy deduplication techniques.",
                            "methods": [
                                {
                                    "name": "keep_first",
                                    "args": [
                                        "self",
                                        "df",
                                        "subset"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Keeps the first occurrence of duplicate rows and removes subsequent duplicates.\n\nExtended Explanation:\n    This method checks for duplicate rows based on all columns or a specified subset of columns,\n    and retains only the first occurrence while removing others.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing possible duplicate rows.\n    subset (list, optional): List of column names to consider for identifying duplicates.\n\nReturns:\n    pd.DataFrame: DataFrame with duplicates removed, preserving the first occurrence.\n\nRaises:\n    ValueError: If 'df' is not a valid DataFrame."
                                },
                                {
                                    "name": "drop_duplicate_columns",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes columns that are exact duplicates of each other.\n\nExtended Explanation:\n    This method examines the DataFrame for columns that have identical content and drops the redundant ones.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n\nReturns:\n    pd.DataFrame: DataFrame with duplicate columns removed.\n\nRaises:\n    ValueError: If the DataFrame is empty or improperly formatted."
                                },
                                {
                                    "name": "remove_exact_duplicates",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes rows that are exactly duplicated.\n\nExtended Explanation:\n    This method identifies rows that are completely identical across all columns and removes the duplicates.\n\nArgs:\n    df (pd.DataFrame): DataFrame to be de-duplicated.\n\nReturns:\n    pd.DataFrame: DataFrame with exact duplicate rows removed.\n\nRaises:\n    ValueError: If 'df' has unexpected structure."
                                },
                                {
                                    "name": "fuzzy_deduplication",
                                    "args": [
                                        "self",
                                        "df",
                                        "similarity_threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Applies fuzzy matching to identify and remove near-duplicate rows.\n\nExtended Explanation:\n    This method uses a similarity threshold to determine which rows are sufficiently similar \n    to be considered duplicates and then removes or flags them.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame to process.\n    similarity_threshold (float): Similarity threshold (between 0 and 1); values closer to 1 denote stricter matching.\n\nReturns:\n    pd.DataFrame: DataFrame with near-duplicates handled.\n\nRaises:\n    ValueError: If similarity_threshold is outside the [0, 1] range."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "remove based on iqr",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/remove based on iqr",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "OutlierFilter",
                        "extra": {
                            "docstring": "Provides methods for identifying and handling outliers in a DataFrame.\n\nThis class supports multiple techniques including filtering by percentile, identifying high outliers,\nthreshold based filtering, IQR-based removal, and generalized outlier identification.",
                            "methods": [
                                {
                                    "name": "filter_by_percentile",
                                    "args": [
                                        "self",
                                        "df",
                                        "percentile"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters rows in the DataFrame based on a specified percentile threshold.\n\nExtended Explanation:\n    This method removes or marks rows in the DataFrame that fall below or above a given percentile value,\n    depending on the implementation of thresholding logic. It is useful for mitigating the influence of extreme values.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame to filter.\n    percentile (float): The percentile threshold (between 0 and 100) to use for filtering.\n\nReturns:\n    pd.DataFrame: A DataFrame with rows filtered based on the percentile.\n\nRaises:\n    ValueError: If the percentile is not between 0 and 100."
                                },
                                {
                                    "name": "filter_high_outliers",
                                    "args": [
                                        "self",
                                        "df",
                                        "high_threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters out rows that contain exceptionally high outlier values.\n\nExtended Explanation:\n    This method detects and filters rows where the values exceed a specified high threshold.\n    It can be applied to a specific column or across the DataFrame if aggregated.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    high_threshold (float): The threshold above which values are considered high outliers.\n\nReturns:\n    pd.DataFrame: DataFrame with high outliers removed or flagged.\n\nRaises:\n    ValueError: If high_threshold is not a positive number."
                                },
                                {
                                    "name": "filter_by_threshold",
                                    "args": [
                                        "self",
                                        "df",
                                        "threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Filters rows based on a fixed numeric threshold.\n\nExtended Explanation:\n    This method removes rows where specified numeric values exceed or fall short of a given threshold,\n    depending on business requirements.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    threshold (float): Numeric threshold for filtering.\n\nReturns:\n    pd.DataFrame: DataFrame filtered according to the threshold.\n\nRaises:\n    ValueError: If threshold is non-numeric."
                                },
                                {
                                    "name": "remove_based_on_iqr",
                                    "args": [
                                        "self",
                                        "df",
                                        "factor"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes outliers using the IQR method.\n\nExtended Explanation:\n    This method calculates the Interquartile Range (IQR) and filters out rows that\n    lie beyond the range defined by the specified factor multiplied by the IQR.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    factor (float): The multiplier for the IQR to define the acceptable range.\n\nReturns:\n    pd.DataFrame: A DataFrame with outliers removed based on the IQR method.\n\nRaises:\n    ValueError: If factor is not a positive number."
                                },
                                {
                                    "name": "identify_outliers",
                                    "args": [
                                        "self",
                                        "df",
                                        "method"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Identifies outliers in the DataFrame using a specified detection method.\n\nExtended Explanation:\n    This method labels or flags rows that are potential outliers following the specified \n    detection strategy (e.g., standard deviation, modified z-score, etc.). No rows are removed.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    method (str): The method used for detecting outliers. Defaults to 'standard'.\n\nReturns:\n    pd.DataFrame: DataFrame with an additional column flagging outliers.\n\nRaises:\n    ValueError: If the specified method is not supported."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "remove exact duplicates",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/remove exact duplicates",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "DuplicateCleaner",
                        "extra": {
                            "docstring": "Implements methods for dealing with duplicate data in a DataFrame.\n\nThis class covers keeping the first occurrence of duplicate records, dropping duplicate columns,\nremoving exact duplicate rows, and handling near-duplicate rows through fuzzy deduplication techniques.",
                            "methods": [
                                {
                                    "name": "keep_first",
                                    "args": [
                                        "self",
                                        "df",
                                        "subset"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Keeps the first occurrence of duplicate rows and removes subsequent duplicates.\n\nExtended Explanation:\n    This method checks for duplicate rows based on all columns or a specified subset of columns,\n    and retains only the first occurrence while removing others.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing possible duplicate rows.\n    subset (list, optional): List of column names to consider for identifying duplicates.\n\nReturns:\n    pd.DataFrame: DataFrame with duplicates removed, preserving the first occurrence.\n\nRaises:\n    ValueError: If 'df' is not a valid DataFrame."
                                },
                                {
                                    "name": "drop_duplicate_columns",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes columns that are exact duplicates of each other.\n\nExtended Explanation:\n    This method examines the DataFrame for columns that have identical content and drops the redundant ones.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n\nReturns:\n    pd.DataFrame: DataFrame with duplicate columns removed.\n\nRaises:\n    ValueError: If the DataFrame is empty or improperly formatted."
                                },
                                {
                                    "name": "remove_exact_duplicates",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes rows that are exactly duplicated.\n\nExtended Explanation:\n    This method identifies rows that are completely identical across all columns and removes the duplicates.\n\nArgs:\n    df (pd.DataFrame): DataFrame to be de-duplicated.\n\nReturns:\n    pd.DataFrame: DataFrame with exact duplicate rows removed.\n\nRaises:\n    ValueError: If 'df' has unexpected structure."
                                },
                                {
                                    "name": "fuzzy_deduplication",
                                    "args": [
                                        "self",
                                        "df",
                                        "similarity_threshold"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Applies fuzzy matching to identify and remove near-duplicate rows.\n\nExtended Explanation:\n    This method uses a similarity threshold to determine which rows are sufficiently similar \n    to be considered duplicates and then removes or flags them.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame to process.\n    similarity_threshold (float): Similarity threshold (between 0 and 1); values closer to 1 denote stricter matching.\n\nReturns:\n    pd.DataFrame: DataFrame with near-duplicates handled.\n\nRaises:\n    ValueError: If similarity_threshold is outside the [0, 1] range."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "remove special characters",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/remove special characters",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "CategoricalCleaner",
                        "extra": {
                            "docstring": "Provides methods for cleaning and standardizing categorical or textual data in a DataFrame.\n\nThis includes operations to standardize categorical formats, remove special characters using regex,\nand fill missing or problematic entries with either custom values or zero.",
                            "methods": [
                                {
                                    "name": "standardize_categorical_data",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes the representation of categorical data in the DataFrame.\n\nExtended Explanation:\n    This method adjusts the string formats in the specified columns to a consistent standard, which\n    may involve trimming spaces, changing case, or other normalization procedures.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    columns (list): List of columns containing categorical data to standardize.\n\nReturns:\n    pd.DataFrame: DataFrame with standardized categorical data.\n\nRaises:\n    ValueError: If any of the specified columns are not of a string type."
                                },
                                {
                                    "name": "remove_special_characters",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "pattern"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes special characters from the specified string columns.\n\nExtended Explanation:\n    Using a regular expression pattern, this method cleans text data by removing characters that\n    do not match the allowed set defined by the pattern.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing textual data.\n    columns (list): List of column names to clean.\n    pattern (str): Regex pattern specifying allowed characters. Defaults to alphanumeric and whitespace.\n\nReturns:\n    pd.DataFrame: DataFrame with special characters removed from specified columns.\n\nRaises:\n    re.error: If the provided regex pattern is invalid."
                                },
                                {
                                    "name": "fill_with_custom_value",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "custom_value"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Fills missing or problematic entries in specified columns with a custom value.\n\nExtended Explanation:\n    This method replaces NaNs or erroneous entries within the given columns with a predetermined custom value.\n\nArgs:\n    df (pd.DataFrame): The DataFrame with missing or problematic entries.\n    columns (list): List of columns to fill.\n    custom_value: The value to use for filling missing entries.\n\nReturns:\n    pd.DataFrame: DataFrame with missing entries replaced by the custom value.\n\nRaises:\n    TypeError: If the custom_value is of the wrong type for the target column."
                                },
                                {
                                    "name": "fill_with_zero",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Fills missing or invalid values in designated columns with zero.\n\nExtended Explanation:\n    This method scans the specified columns for NaN or other placeholders for missing data and fills them with zero.\n\nArgs:\n    df (pd.DataFrame): DataFrame to process.\n    columns (list): List of column names where zeros will replace missing values.\n\nReturns:\n    pd.DataFrame: DataFrame with missing values filled with zero.\n\nRaises:\n    ValueError: If the operation fails due to type incompatibilities."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "resolve conflicting data",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/resolve conflicting data",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ConsistencyCorrector",
                        "extra": {
                            "docstring": "Provides methods for correcting inconsistencies and errors in data formatting.\n\nThis class includes functions that adjust data to fixed formats,\ncorrect typographical errors, enforce consistency across entries, and resolve conflicts.",
                            "methods": [
                                {
                                    "name": "fix_inconsistent_data_formats",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "format_rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes data formats in specified columns based on provided rules.\n\nExtended Explanation:\n    This method applies a set of formatting rules to the designated columns to ensure the data\n    adheres to consistent standards.\n\nArgs:\n    df (pd.DataFrame): DataFrame to standardize.\n    columns (list): List of column names to adjust.\n    format_rules (dict): Mapping of column names to formatting rules.\n\nReturns:\n    pd.DataFrame: DataFrame with standardized formats.\n\nRaises:\n    KeyError: If a column is missing from the provided format_rules."
                                },
                                {
                                    "name": "correct_typos",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "typo_dict"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Corrects typographical errors in specified textual columns.\n\nExtended Explanation:\n    This method replaces common typos with their correct forms using a dictionary mapping.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame.\n    columns (list): List of columns to inspect for typos.\n    typo_dict (dict): Dictionary mapping incorrect spellings to correct ones.\n\nReturns:\n    pd.DataFrame: DataFrame with typos corrected.\n\nRaises:\n    ValueError: If typo_dict is empty or columns are missing."
                                },
                                {
                                    "name": "enforce_consistency_rules",
                                    "args": [
                                        "self",
                                        "df",
                                        "rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Enforces predefined consistency rules across the DataFrame.\n\nExtended Explanation:\n    This method applies a series of rules to ensure that data across columns adheres to\n    consistent formats and logical relationships.\n\nArgs:\n    df (pd.DataFrame): DataFrame to process.\n    rules (dict): Dictionary containing consistency rules to apply.\n\nReturns:\n    pd.DataFrame: DataFrame with consistency rules enforced.\n\nRaises:\n    ValueError: If rules are improperly defined."
                                },
                                {
                                    "name": "correct_data_entry_errors",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "error_corrections"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Corrects known data entry errors in specified columns.\n\nExtended Explanation:\n    This method uses an error correction mapping to replace incorrect entries due to data entry errors.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing errors.\n    columns (list): List of columns to correct.\n    error_corrections (dict): Dictionary mapping erroneous values to correct ones.\n\nReturns:\n    pd.DataFrame: DataFrame with data entry errors corrected.\n\nRaises:\n    KeyError: If an expected correction key is missing."
                                },
                                {
                                    "name": "resolve_conflicting_data",
                                    "args": [
                                        "self",
                                        "df",
                                        "conflict_resolution_rules"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Resolves conflicting data entries using specified rules.\n\nExtended Explanation:\n    This method applies conflict resolution strategies to deal with data contradictions,\n    ensuring that the resulting DataFrame has coherent and consistent data.\n\nArgs:\n    df (pd.DataFrame): DataFrame with conflicting entries.\n    conflict_resolution_rules (dict): Dictionary of rules to resolve conflicts.\n\nReturns:\n    pd.DataFrame: DataFrame with resolved conflicting data.\n\nRaises:\n    ValueError: If conflict_resolution_rules do not cover all conflicts."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "standardize categorical data",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/standardize categorical data",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "CategoricalCleaner",
                        "extra": {
                            "docstring": "Provides methods for cleaning and standardizing categorical or textual data in a DataFrame.\n\nThis includes operations to standardize categorical formats, remove special characters using regex,\nand fill missing or problematic entries with either custom values or zero.",
                            "methods": [
                                {
                                    "name": "standardize_categorical_data",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes the representation of categorical data in the DataFrame.\n\nExtended Explanation:\n    This method adjusts the string formats in the specified columns to a consistent standard, which\n    may involve trimming spaces, changing case, or other normalization procedures.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to process.\n    columns (list): List of columns containing categorical data to standardize.\n\nReturns:\n    pd.DataFrame: DataFrame with standardized categorical data.\n\nRaises:\n    ValueError: If any of the specified columns are not of a string type."
                                },
                                {
                                    "name": "remove_special_characters",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "pattern"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Removes special characters from the specified string columns.\n\nExtended Explanation:\n    Using a regular expression pattern, this method cleans text data by removing characters that\n    do not match the allowed set defined by the pattern.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing textual data.\n    columns (list): List of column names to clean.\n    pattern (str): Regex pattern specifying allowed characters. Defaults to alphanumeric and whitespace.\n\nReturns:\n    pd.DataFrame: DataFrame with special characters removed from specified columns.\n\nRaises:\n    re.error: If the provided regex pattern is invalid."
                                },
                                {
                                    "name": "fill_with_custom_value",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "custom_value"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Fills missing or problematic entries in specified columns with a custom value.\n\nExtended Explanation:\n    This method replaces NaNs or erroneous entries within the given columns with a predetermined custom value.\n\nArgs:\n    df (pd.DataFrame): The DataFrame with missing or problematic entries.\n    columns (list): List of columns to fill.\n    custom_value: The value to use for filling missing entries.\n\nReturns:\n    pd.DataFrame: DataFrame with missing entries replaced by the custom value.\n\nRaises:\n    TypeError: If the custom_value is of the wrong type for the target column."
                                },
                                {
                                    "name": "fill_with_zero",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Fills missing or invalid values in designated columns with zero.\n\nExtended Explanation:\n    This method scans the specified columns for NaN or other placeholders for missing data and fills them with zero.\n\nArgs:\n    df (pd.DataFrame): DataFrame to process.\n    columns (list): List of column names where zeros will replace missing values.\n\nReturns:\n    pd.DataFrame: DataFrame with missing values filled with zero.\n\nRaises:\n    ValueError: If the operation fails due to type incompatibilities."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "standardize using robust scaling",
                "feature_path": "Data Engineering/Data Preparation/Data Cleaning/Supplementary Adjustments/standardize using robust scaling",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "DataConversionScaler",
                        "extra": {
                            "docstring": "Provides conversion and scaling operations for numeric data in a DataFrame.\n\nThis class includes methods for converting data types to integers and ordinals,\napplying decimal scaling, and standardizing data using robust scaling techniques.",
                            "methods": [
                                {
                                    "name": "convert_to_integer",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Converts specified columns in a DataFrame to integer type.\n\nExtended Explanation:\n    This method is used to ensure that numeric data is stored as integers. It processes\n    the provided columns and converts any float or string representations of numbers to integers.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing the columns to convert.\n    columns (list): List of column names to be converted to integer type.\n\nReturns:\n    pd.DataFrame: A DataFrame with the specified columns converted to integer.\n\nRaises:\n    TypeError: If conversion cannot be performed due to incompatible data types."
                                },
                                {
                                    "name": "convert_to_ordinal",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns",
                                        "mapping"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Converts specified categorical columns to ordinal values.\n\nExtended Explanation:\n    This method applies a mapping dictionary to convert nominal or categorical data into ordinal form,\n    which can facilitate further numeric processing.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing categorical columns.\n    columns (list): List of column names to convert.\n    mapping (dict): Dictionary mapping original values to ordinal integers.\n\nReturns:\n    pd.DataFrame: DataFrame with specified columns converted to ordinal values.\n\nRaises:\n    KeyError: If a column value is missing from the mapping."
                                },
                                {
                                    "name": "apply_decimal_scaling",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Scales numeric columns using decimal scaling.\n\nExtended Explanation:\n    This method normalizes numeric data by shifting the decimal point. This is useful for adjusting the magnitude\n    of numbers without changing their distribution.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing numerical columns.\n    columns (list): List of columns to be scaled.\n\nReturns:\n    pd.DataFrame: DataFrame with scaled numeric columns.\n\nRaises:\n    ValueError: If columns contain non-numeric values."
                                },
                                {
                                    "name": "standardize_using_robust_scaling",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Summary:\n    Standardizes data using techniques that are robust to outliers.\n\nExtended Explanation:\n    Rather than using mean and standard deviation, this method employs robust statistical measures\n    such as the median and IQR to reduce the influence of extreme values.\n\nArgs:\n    df (pd.DataFrame): DataFrame containing columns to be standardized.\n    columns (list): List of column names to standardize.\n\nReturns:\n    pd.DataFrame: DataFrame with robust-scaled values.\n\nRaises:\n    ValueError: If provided columns contain non-numeric data."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/cleaning/supplementary_cleaning.py"
                    }
                ]
            },
            {
                "node": "Feature Engineering",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_preparation/feature_engineering"
                    }
                ]
            },
            {
                "node": "Combine Features",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Combine Features",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/feature_engineering/combine_features.py"
                    }
                ]
            },
            {
                "node": "aggregate features",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Combine Features/aggregate features",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "aggregate_features",
                        "extra": {
                            "args": [
                                "df",
                                "group_by",
                                "agg_functions"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Aggregate features in a DataFrame based on specified grouping keys and aggregation functions.\n\nThis function performs aggregation on a DataFrame by grouping the data using the provided key(s) and \nthen applying a set of aggregation functions. It is useful for summarizing feature values and deriving \naggregated representations of data for modeling.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing features to be aggregated.\n    group_by (Optional[List[str]]): Column names to group by; if None, aggregation is performed on the entire DataFrame.\n    agg_functions (Dict[str, Callable]): A dictionary mapping column names to aggregation functions (e.g., sum, mean).\n\nReturns:\n    pd.DataFrame: A DataFrame containing the aggregated features.\n\nEdge Cases:\n    - If group_by is None, aggregation is applied across the entire DataFrame.\n    - An empty DataFrame is returned if the input DataFrame is empty."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/combine_features.py"
                    }
                ]
            },
            {
                "node": "combine categorical features",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Combine Features/combine categorical features",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "combine_categorical_features",
                        "extra": {
                            "args": [
                                "categorical_dfs"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Combine multiple categorical feature DataFrames into a single DataFrame.\n\nThis function merges several DataFrames that contain categorical features. It ensures that the data\ntypes remain consistent and that the categorical features are properly combined for further processing.\n\nArgs:\n    categorical_dfs (List[pd.DataFrame]): A list of DataFrames, each with categorical feature columns.\n\nReturns:\n    pd.DataFrame: A DataFrame containing the combined categorical features.\n\nEdge Cases:\n    - Returns an empty DataFrame if the input list is empty.\n    - Assumes that the DataFrames have a common index or merging logic that allows them to be combined."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/combine_features.py"
                    }
                ]
            },
            {
                "node": "combine numerical features",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Combine Features/combine numerical features",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "combine_numerical_features",
                        "extra": {
                            "args": [
                                "numerical_dfs"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Combine multiple numerical feature DataFrames into a single DataFrame containing numerical features.\n\nThis function takes a list of DataFrames that each contain numerical feature columns and merges them\nby aligning their indices. It is designed to consolidate numerical features into one DataFrame for\nsubsequent modeling steps.\n\nArgs:\n    numerical_dfs (List[pd.DataFrame]): A list of DataFrames, each containing numerical feature columns.\n\nReturns:\n    pd.DataFrame: A DataFrame resulting from the combination of the numerical feature DataFrames.\n\nEdge Cases:\n    - Returns an empty DataFrame if the input list is empty.\n    - Assumes all DataFrames have compatible indices for merging."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/combine_features.py"
                    }
                ]
            },
            {
                "node": "concatenate features",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Combine Features/concatenate features",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "concatenate_features",
                        "extra": {
                            "args": [
                                "feature_dfs",
                                "axis"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Concatenate multiple feature DataFrames along a specified axis.\n\nThis function concatenates a list of DataFrames, which contain features that need to be combined into\na single DataFrame. It supports concatenation along either rows or columns based on the provided axis.\n\nArgs:\n    feature_dfs (List[pd.DataFrame]): A list of DataFrames containing feature columns.\n    axis (int, optional): The axis along which to concatenate. Default is 1 (columns).\n\nReturns:\n    pd.DataFrame: A concatenated DataFrame containing all combined features.\n\nEdge Cases:\n    - Returns an empty DataFrame if the input list is empty.\n    - Assumes that the DataFrames are dimensionally compatible along the non-concatenation axis."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/combine_features.py"
                    }
                ]
            },
            {
                "node": "merge feature sets",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Combine Features/merge feature sets",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "merge_feature_sets",
                        "extra": {
                            "args": [
                                "feature_dfs",
                                "on"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Merge multiple feature DataFrames based on a common key or set of keys.\n\nThis function executes a merge (join) operation on a list of DataFrames containing features using the specified \nkey(s). It is aimed at integrating feature sets from different sources into a single cohesive DataFrame.\n\nArgs:\n    feature_dfs (List[pd.DataFrame]): A list of DataFrames with features to be merged.\n    on (Union[str, List[str]]): A column name or list of column names to be used as join keys.\n\nReturns:\n    pd.DataFrame: A DataFrame resulting from the merge of the input feature sets.\n\nEdge Cases:\n    - Returns an empty DataFrame if the input list is empty.\n    - Assumes all DataFrames in the list contain the specified join key(s)."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/combine_features.py"
                    }
                ]
            },
            {
                "node": "Create New Features",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Create New Features",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/feature_engineering/create_features.py"
                    }
                ]
            },
            {
                "node": "create features from date",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Create New Features/create features from date",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "create_date_features",
                        "extra": {
                            "args": [
                                "data",
                                "date_column",
                                "date_format"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Create new features by extracting elements from a date field in the DataFrame.\n\nThis function converts the given date_column into various derived features such as year, month, day,\nweekday, and other date-related components based on the specified date_format. These new features can \nbe utilized in downstream analyses.\n\nArgs:\n    data (pd.DataFrame): The DataFrame containing the date information.\n    date_column (str): The name of the column containing date strings or datetime objects.\n    date_format (str, optional): The format in which dates are structured if they are in string form.\n                                 Defaults to '%Y-%m-%d'.\n\nReturns:\n    pd.DataFrame: A DataFrame with newly created features derived from the date column.\n\nEdge Cases:\n    - If date_column does not exist or dates are not parsable using the given format, the function\n      should report an error or skip feature creation."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/create_features.py"
                    }
                ]
            },
            {
                "node": "create features from existing data",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Create New Features/create features from existing data",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "create_existing_data_features",
                        "extra": {
                            "args": [
                                "data",
                                "feature_instructions"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Create new features by transforming or combining existing features within the DataFrame.\n\nThis function applies a set of specified transformations (outlined in feature_instructions) to the \nDataFrame in order to generate new, derived features from the existing ones. These instructions \ncould include arithmetic operations, concatenations, or other custom transformations.\n\nArgs:\n    data (pd.DataFrame): The input DataFrame containing the original features.\n    feature_instructions (dict): A dictionary defining how to transform or combine existing features.\n                                 The keys might refer to new feature names, and the values indicate the\n                                 operations or source columns involved.\n\nReturns:\n    pd.DataFrame: A DataFrame with the newly created features from existing data added.\n\nEdge Cases:\n    - If feature_instructions is empty, the function could return the original DataFrame."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/create_features.py"
                    }
                ]
            },
            {
                "node": "create features from external data",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Create New Features/create features from external data",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "create_external_data_features",
                        "extra": {
                            "args": [
                                "main_data",
                                "external_data",
                                "join_keys"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Create new features by integrating external data with the main DataFrame.\n\nThis function merges the main_data with external_data based on the specified join_keys to augment the \nfeature set with external information. This can be used to incorporate additional context or metadata \ninto the main dataset.\n\nArgs:\n    main_data (pd.DataFrame): The primary DataFrame containing the original features.\n    external_data (pd.DataFrame): The external DataFrame that contains supplementary features.\n    join_keys (List[str]): A list of column names used to join the two DataFrames.\n\nReturns:\n    pd.DataFrame: A DataFrame resulting from the merge process, enriched with external features.\n\nEdge Cases:\n    - If the join_keys are not present in both DataFrames, the function should handle the error."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/create_features.py"
                    }
                ]
            },
            {
                "node": "derive new metrics",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Create New Features/derive new metrics",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "derive_new_metrics",
                        "extra": {
                            "args": [
                                "feature_data",
                                "metric_params"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Derive new metric features from the provided data based on specified parameters.\n\nThis function computes new metrics using the input feature_data and a dictionary of parameters\ndefining the metric calculations. This might include aggregations or transformations to derive\nsummary or performance measures not originally present in the data.\n\nArgs:\n    feature_data (pd.DataFrame): The input DataFrame containing features.\n    metric_params (dict): A dictionary specifying how to compute new metrics. Keys could be the\n                          new metric names and values could be functions or specifications of the\n                          operations to perform.\n\nReturns:\n    pd.DataFrame: A DataFrame with the new derived metric features added.\n\nEdge Cases:\n    - If metric_params is empty, the function may return the original DataFrame.\n    - Assumes feature_data is a valid DataFrame."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/create_features.py"
                    }
                ]
            },
            {
                "node": "generate ratios",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Create New Features/generate ratios",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "generate_ratios",
                        "extra": {
                            "args": [
                                "feature_data",
                                "ratio_definitions"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Generate ratio features based on specified numerator-denominator pairs.\n\nThis function calculates new features by computing ratios for selected columns in the\nfeature_data. The ratio_definitions dictionary maps new feature names to a tuple of two strings,\neach representing the column names for the numerator and denominator.\n\nArgs:\n    feature_data (pd.DataFrame): The input DataFrame containing the features.\n    ratio_definitions (dict): A dictionary where each key is the name of the new ratio feature and\n                              the value is a tuple (numerator_column, denominator_column).\n\nReturns:\n    pd.DataFrame: A DataFrame with the newly generated ratio features added.\n\nEdge Cases:\n    - If any denominator is zero or missing, the function should handle division errors.\n    - If ratio_definitions is empty, the original DataFrame is returned."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/create_features.py"
                    }
                ]
            },
            {
                "node": "location-based features",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Create New Features/location-based features",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "create_location_based_features",
                        "extra": {
                            "args": [
                                "data",
                                "location_column",
                                "additional_params"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Create location-based features from a specified location column in the DataFrame.\n\nThis function extracts and/or transforms the location data (e.g., geographical coordinates, \naddresses, regions) found in the location_column and generates new features such as distance measures, \nregion flags, or spatial clusters. Additional parameters can guide specific feature extraction methods.\n\nArgs:\n    data (pd.DataFrame): The input DataFrame containing location data.\n    location_column (str): The name of the column that holds location information.\n    additional_params (dict, optional): A dictionary with extra options to guide the feature \n                                        extraction process. Defaults to None.\n\nReturns:\n    pd.DataFrame: A DataFrame with the newly created location-based features appended.\n\nEdge Cases:\n    - If location_column is missing or contains invalid entries, the function\n      should handle such cases gracefully."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/create_features.py"
                    }
                ]
            },
            {
                "node": "time-based features",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Create New Features/time-based features",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "create_time_based_features",
                        "extra": {
                            "args": [
                                "data",
                                "time_column"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Create time-based features from a specified time column in the DataFrame.\n\nThis function extracts various temporal features (such as hour, day, month, weekday, etc.)\nfrom the provided time_column and appends them as new features in the output DataFrame.\nAdditional keyword arguments may define specific extraction rules or configurations.\n\nArgs:\n    data (pd.DataFrame): The input DataFrame containing the time column.\n    time_column (str): The name of the column in the DataFrame that holds datetime information.\n    **kwargs: Additional parameters such as timezone adjustments or custom extraction options.\n\nReturns:\n    pd.DataFrame: A DataFrame with the new time-based features appended.\n\nEdge Cases:\n    - If time_column does not exist in data, the function should handle it appropriately.\n    - Assumes the time_column is in a parseable datetime format."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/create_features.py"
                    }
                ]
            },
            {
                "node": "Feature Extraction",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Extraction",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_extraction.py"
                    }
                ]
            },
            {
                "node": "extract categorical features",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Extraction/extract categorical features",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "extract_categorical_features",
                        "extra": {
                            "args": [
                                "data",
                                "categorical_columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Extract categorical features from the input DataFrame.\n\nThis function isolates categorical features from the provided DataFrame for use in downstream tasks such as encoding\nor modeling. It either extracts user-specified columns or automatically identifies categorical types if none are provided.\n\nArgs:\n    data (pd.DataFrame): The input DataFrame containing the data from which to extract categorical features.\n    categorical_columns (Optional[List[str]]): A list of column names to extract as categorical features. \n        If None, the function will attempt to infer categorical columns automatically.\n\nReturns:\n    pd.DataFrame: A DataFrame containing only the extracted categorical features.\n\nRaises:\n    ValueError: If the input DataFrame is empty or if the specified 'categorical_columns' are not found in the DataFrame."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_extraction.py"
                    }
                ]
            },
            {
                "node": "extract features from text",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Extraction/extract features from text",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "extract_text_features",
                        "extra": {
                            "args": [
                                "data",
                                "text_columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Extract features from text data in the input DataFrame.\n\nThis function processes the DataFrame to extract textual features, applying common text processing techniques such as \ntokenization or embedding extraction. Users may supply a list of text_columns, or the function can attempt to infer text data.\n\nArgs:\n    data (pd.DataFrame): The input DataFrame containing textual data.\n    text_columns (Optional[List[str]]): A list of column names that contain text data. \n        If None, the function will attempt to automatically identify text-containing columns.\n\nReturns:\n    pd.DataFrame: A DataFrame containing features extracted from the text data.\n\nRaises:\n    ValueError: If the input DataFrame is empty, or if specified text columns are not present in the DataFrame."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_extraction.py"
                    }
                ]
            },
            {
                "node": "extract numerical features",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Extraction/extract numerical features",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "extract_numerical_features",
                        "extra": {
                            "args": [
                                "data",
                                "numerical_columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Extract numerical features from the input DataFrame.\n\nThis function identifies and extracts numerical features from a DataFrame, which may then be utilized for scaling,\nstatistical analysis, or machine learning model training. It supports both automatic detection and explicit column specification.\n\nArgs:\n    data (pd.DataFrame): The input DataFrame containing a mix of feature types.\n    numerical_columns (Optional[List[str]]): A list of column names representing numerical features to extract.\n        If None, the function will automatically detect numerical columns.\n\nReturns:\n    pd.DataFrame: A DataFrame containing only the extracted numerical features.\n\nRaises:\n    ValueError: If the input DataFrame is empty or if no numerical columns are found based on the given criteria."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_extraction.py"
                    }
                ]
            },
            {
                "node": "image feature extraction",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Extraction/image feature extraction",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "extract_image_features",
                        "extra": {
                            "args": [
                                "image_array",
                                "config"
                            ],
                            "return_type": "np.ndarray",
                            "docstring": "Extract features from image data represented as a numpy array.\n\nThis function processes an image (represented as a numpy array) to extract meaningful features such as descriptors or embeddings.\nAn optional configuration dictionary allows for customization of the feature extraction process.\n\nArgs:\n    image_array (np.ndarray): The input image data in numpy array format.\n    config (Optional[dict]): A dictionary containing configuration parameters to customize the feature extraction.\n        If None, default extraction parameters are used.\n\nReturns:\n    np.ndarray: A numpy array containing the extracted image features.\n\nRaises:\n    ValueError: If the input image array is empty or does not conform to expected dimensions, or if the configuration \n                is missing required keys for processing."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_extraction.py"
                    }
                ]
            },
            {
                "node": "Feature Scaling",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Scaling",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_scaling.py"
                    }
                ]
            },
            {
                "node": "min-max scaling",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Scaling/min-max scaling",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "scale_features_to_range",
                        "extra": {
                            "args": [
                                "df",
                                "feature_columns",
                                "min_value",
                                "max_value"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Scale features in a DataFrame to a specified range using min-max scaling.\n\nThis function implements the min-max scaling algorithm which rescales the data\nin the specified feature columns to fit within the provided [min_value, max_value] range.\nIf 'feature_columns' is not provided, all numeric columns in the DataFrame will be scaled.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing the features to scale.\n    feature_columns (Optional[List[str]]): A list of column names to be scaled.\n        If None, all numeric columns are considered.\n    min_value (Union[int, float]): The minimum value of the desired scale range. Default is 0.\n    max_value (Union[int, float]): The maximum value of the desired scale range. Default is 1.\n\nReturns:\n    pd.DataFrame: A new DataFrame with the specified features scaled to the given range.\n\nEdge Cases and Assumptions:\n    - The function assumes that 'df' contains numeric data for the selected columns.\n    - If feature_columns is provided but some columns are non-numeric, the behavior is undefined.\n    - The function does not modify the input DataFrame in-place; it returns a new DataFrame."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_scaling.py"
                    }
                ]
            },
            {
                "node": "scale features to range",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Scaling/scale features to range",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "scale_features_to_range",
                        "extra": {
                            "args": [
                                "df",
                                "feature_columns",
                                "min_value",
                                "max_value"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Scale features in a DataFrame to a specified range using min-max scaling.\n\nThis function implements the min-max scaling algorithm which rescales the data\nin the specified feature columns to fit within the provided [min_value, max_value] range.\nIf 'feature_columns' is not provided, all numeric columns in the DataFrame will be scaled.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing the features to scale.\n    feature_columns (Optional[List[str]]): A list of column names to be scaled.\n        If None, all numeric columns are considered.\n    min_value (Union[int, float]): The minimum value of the desired scale range. Default is 0.\n    max_value (Union[int, float]): The maximum value of the desired scale range. Default is 1.\n\nReturns:\n    pd.DataFrame: A new DataFrame with the specified features scaled to the given range.\n\nEdge Cases and Assumptions:\n    - The function assumes that 'df' contains numeric data for the selected columns.\n    - If feature_columns is provided but some columns are non-numeric, the behavior is undefined.\n    - The function does not modify the input DataFrame in-place; it returns a new DataFrame."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_scaling.py"
                    }
                ]
            },
            {
                "node": "standard scaling",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Scaling/standard scaling",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "standard_scale_features",
                        "extra": {
                            "args": [
                                "df",
                                "feature_columns"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Standardize features in a DataFrame using standard scaling (z-score normalization).\n\nThis function applies standard scaling to the selected feature columns by subtracting\nthe mean and dividing by the standard deviation for each feature. If 'feature_columns' is\nnot provided, all numeric columns will be standardized.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing the features to standardize.\n    feature_columns (Optional[List[str]]): A list of column names to be standardized.\n        If None, standardization will be applied to all numeric columns.\n\nReturns:\n    pd.DataFrame: A new DataFrame with the specified features standardized.\n\nEdge Cases and Assumptions:\n    - The function assumes that 'df' contains numeric data for the selected columns.\n    - Columns with zero variance may result in division by zero; these cases are not explicitly handled.\n    - The input DataFrame remains unchanged; a new DataFrame with standardized values is returned."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_scaling.py"
                    }
                ]
            },
            {
                "node": "Feature Selection & Reduction",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Selection & Reduction",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_selection.py"
                    }
                ]
            },
            {
                "node": "feature importance ranking",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Selection & Reduction/feature importance ranking",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "FilterFeatureSelector",
                        "extra": {
                            "docstring": "A collection of filter-based feature selection methods.\n\nThis class implements multiple filtering techniques for feature selection such as:\n  - Select with chi-squared tests.\n  - Select with mutual information scores.\n  - Select features with high correlation to the target.\n  - Filter features based on low variance.\n  - Rank features by their computed importance.\n\nEach method is designed to work with input pandas DataFrames and aligns with the\ndata flow expectations of the system.",
                            "methods": [
                                {
                                    "name": "select_with_chi_squared",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on chi-squared test scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with non-negative features.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of selected feature names based on chi-squared results.\n\nRaises:\n    ValueError: If X contains negative values or if k is out of valid range."
                                },
                                {
                                    "name": "select_with_mutual_information",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on mutual information scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of feature names selected based on mutual information.\n\nRaises:\n    ValueError: If the mutual information calculation fails."
                                },
                                {
                                    "name": "select_high_correlation",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select features that have a correlation with the target variable above a given threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with named columns.\n    y (Any): The target variable used for correlation computation.\n    threshold (float): The minimum correlation coefficient to consider. \n                       Should be between 0 and 1.\n\nReturns:\n    List[str]: A list of feature names that exhibit high correlation with the target.\n\nRaises:\n    ValueError: If threshold is not between 0 and 1."
                                },
                                {
                                    "name": "select_low_variance",
                                    "args": [
                                        "self",
                                        "X",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Filter out features with variance lower than the specified threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    threshold (float): The minimum variance required for a feature to be retained.\n\nReturns:\n    List[str]: A list of feature names with variance exceeding the threshold.\n\nRaises:\n    ValueError: If the threshold is negative."
                                },
                                {
                                    "name": "rank_features_by_importance",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "List[Tuple[str, float]]",
                                    "docstring": "Rank features based on their importance scores derived from a model-based approach.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    \nReturns:\n    List[Tuple[str, float]]: A list of tuples where each tuple contains a feature name and \n                             its corresponding importance score, sorted in descending order.\n\nRaises:\n    Exception: If feature importance cannot be computed."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_selection.py"
                    }
                ]
            },
            {
                "node": "filter methods",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Selection & Reduction/filter methods",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "FilterFeatureSelector",
                        "extra": {
                            "docstring": "A collection of filter-based feature selection methods.\n\nThis class implements multiple filtering techniques for feature selection such as:\n  - Select with chi-squared tests.\n  - Select with mutual information scores.\n  - Select features with high correlation to the target.\n  - Filter features based on low variance.\n  - Rank features by their computed importance.\n\nEach method is designed to work with input pandas DataFrames and aligns with the\ndata flow expectations of the system.",
                            "methods": [
                                {
                                    "name": "select_with_chi_squared",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on chi-squared test scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with non-negative features.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of selected feature names based on chi-squared results.\n\nRaises:\n    ValueError: If X contains negative values or if k is out of valid range."
                                },
                                {
                                    "name": "select_with_mutual_information",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on mutual information scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of feature names selected based on mutual information.\n\nRaises:\n    ValueError: If the mutual information calculation fails."
                                },
                                {
                                    "name": "select_high_correlation",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select features that have a correlation with the target variable above a given threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with named columns.\n    y (Any): The target variable used for correlation computation.\n    threshold (float): The minimum correlation coefficient to consider. \n                       Should be between 0 and 1.\n\nReturns:\n    List[str]: A list of feature names that exhibit high correlation with the target.\n\nRaises:\n    ValueError: If threshold is not between 0 and 1."
                                },
                                {
                                    "name": "select_low_variance",
                                    "args": [
                                        "self",
                                        "X",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Filter out features with variance lower than the specified threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    threshold (float): The minimum variance required for a feature to be retained.\n\nReturns:\n    List[str]: A list of feature names with variance exceeding the threshold.\n\nRaises:\n    ValueError: If the threshold is negative."
                                },
                                {
                                    "name": "rank_features_by_importance",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "List[Tuple[str, float]]",
                                    "docstring": "Rank features based on their importance scores derived from a model-based approach.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    \nReturns:\n    List[Tuple[str, float]]: A list of tuples where each tuple contains a feature name and \n                             its corresponding importance score, sorted in descending order.\n\nRaises:\n    Exception: If feature importance cannot be computed."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_selection.py"
                    }
                ]
            },
            {
                "node": "recursive feature elimination",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Selection & Reduction/recursive feature elimination",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "recursive_feature_elimination",
                        "extra": {
                            "args": [
                                "X",
                                "y",
                                "estimator",
                                "min_features_to_select"
                            ],
                            "return_type": "List[str]",
                            "docstring": "Perform recursive feature elimination to select the most important features.\n\nThis function iteratively removes features from the input dataset based on the performance\nof the provided estimator until the specified minimum number of features is reached.\nIt helps in reducing model complexity and enhancing model generalization.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with named columns.\n    y (Any): The target variable.\n    estimator (Any): A machine learning estimator that provides feature importance or coefficient attributes.\n    min_features_to_select (int, optional): The minimum number of features to retain. Defaults to 1.\n\nReturns:\n    List[str]: A list of selected feature names.\n\nEdge Cases:\n    - The estimator must support feature importance or provide coefficients.\n    - X is expected to have unique column names."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_selection.py"
                    }
                ]
            },
            {
                "node": "select features with high correlation",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Selection & Reduction/select features with high correlation",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "FilterFeatureSelector",
                        "extra": {
                            "docstring": "A collection of filter-based feature selection methods.\n\nThis class implements multiple filtering techniques for feature selection such as:\n  - Select with chi-squared tests.\n  - Select with mutual information scores.\n  - Select features with high correlation to the target.\n  - Filter features based on low variance.\n  - Rank features by their computed importance.\n\nEach method is designed to work with input pandas DataFrames and aligns with the\ndata flow expectations of the system.",
                            "methods": [
                                {
                                    "name": "select_with_chi_squared",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on chi-squared test scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with non-negative features.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of selected feature names based on chi-squared results.\n\nRaises:\n    ValueError: If X contains negative values or if k is out of valid range."
                                },
                                {
                                    "name": "select_with_mutual_information",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on mutual information scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of feature names selected based on mutual information.\n\nRaises:\n    ValueError: If the mutual information calculation fails."
                                },
                                {
                                    "name": "select_high_correlation",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select features that have a correlation with the target variable above a given threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with named columns.\n    y (Any): The target variable used for correlation computation.\n    threshold (float): The minimum correlation coefficient to consider. \n                       Should be between 0 and 1.\n\nReturns:\n    List[str]: A list of feature names that exhibit high correlation with the target.\n\nRaises:\n    ValueError: If threshold is not between 0 and 1."
                                },
                                {
                                    "name": "select_low_variance",
                                    "args": [
                                        "self",
                                        "X",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Filter out features with variance lower than the specified threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    threshold (float): The minimum variance required for a feature to be retained.\n\nReturns:\n    List[str]: A list of feature names with variance exceeding the threshold.\n\nRaises:\n    ValueError: If the threshold is negative."
                                },
                                {
                                    "name": "rank_features_by_importance",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "List[Tuple[str, float]]",
                                    "docstring": "Rank features based on their importance scores derived from a model-based approach.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    \nReturns:\n    List[Tuple[str, float]]: A list of tuples where each tuple contains a feature name and \n                             its corresponding importance score, sorted in descending order.\n\nRaises:\n    Exception: If feature importance cannot be computed."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_selection.py"
                    }
                ]
            },
            {
                "node": "select features with low variance",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Selection & Reduction/select features with low variance",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "FilterFeatureSelector",
                        "extra": {
                            "docstring": "A collection of filter-based feature selection methods.\n\nThis class implements multiple filtering techniques for feature selection such as:\n  - Select with chi-squared tests.\n  - Select with mutual information scores.\n  - Select features with high correlation to the target.\n  - Filter features based on low variance.\n  - Rank features by their computed importance.\n\nEach method is designed to work with input pandas DataFrames and aligns with the\ndata flow expectations of the system.",
                            "methods": [
                                {
                                    "name": "select_with_chi_squared",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on chi-squared test scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with non-negative features.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of selected feature names based on chi-squared results.\n\nRaises:\n    ValueError: If X contains negative values or if k is out of valid range."
                                },
                                {
                                    "name": "select_with_mutual_information",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on mutual information scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of feature names selected based on mutual information.\n\nRaises:\n    ValueError: If the mutual information calculation fails."
                                },
                                {
                                    "name": "select_high_correlation",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select features that have a correlation with the target variable above a given threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with named columns.\n    y (Any): The target variable used for correlation computation.\n    threshold (float): The minimum correlation coefficient to consider. \n                       Should be between 0 and 1.\n\nReturns:\n    List[str]: A list of feature names that exhibit high correlation with the target.\n\nRaises:\n    ValueError: If threshold is not between 0 and 1."
                                },
                                {
                                    "name": "select_low_variance",
                                    "args": [
                                        "self",
                                        "X",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Filter out features with variance lower than the specified threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    threshold (float): The minimum variance required for a feature to be retained.\n\nReturns:\n    List[str]: A list of feature names with variance exceeding the threshold.\n\nRaises:\n    ValueError: If the threshold is negative."
                                },
                                {
                                    "name": "rank_features_by_importance",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "List[Tuple[str, float]]",
                                    "docstring": "Rank features based on their importance scores derived from a model-based approach.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    \nReturns:\n    List[Tuple[str, float]]: A list of tuples where each tuple contains a feature name and \n                             its corresponding importance score, sorted in descending order.\n\nRaises:\n    Exception: If feature importance cannot be computed."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_selection.py"
                    }
                ]
            },
            {
                "node": "select with chi-squared",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Selection & Reduction/select with chi-squared",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "FilterFeatureSelector",
                        "extra": {
                            "docstring": "A collection of filter-based feature selection methods.\n\nThis class implements multiple filtering techniques for feature selection such as:\n  - Select with chi-squared tests.\n  - Select with mutual information scores.\n  - Select features with high correlation to the target.\n  - Filter features based on low variance.\n  - Rank features by their computed importance.\n\nEach method is designed to work with input pandas DataFrames and aligns with the\ndata flow expectations of the system.",
                            "methods": [
                                {
                                    "name": "select_with_chi_squared",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on chi-squared test scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with non-negative features.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of selected feature names based on chi-squared results.\n\nRaises:\n    ValueError: If X contains negative values or if k is out of valid range."
                                },
                                {
                                    "name": "select_with_mutual_information",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on mutual information scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of feature names selected based on mutual information.\n\nRaises:\n    ValueError: If the mutual information calculation fails."
                                },
                                {
                                    "name": "select_high_correlation",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select features that have a correlation with the target variable above a given threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with named columns.\n    y (Any): The target variable used for correlation computation.\n    threshold (float): The minimum correlation coefficient to consider. \n                       Should be between 0 and 1.\n\nReturns:\n    List[str]: A list of feature names that exhibit high correlation with the target.\n\nRaises:\n    ValueError: If threshold is not between 0 and 1."
                                },
                                {
                                    "name": "select_low_variance",
                                    "args": [
                                        "self",
                                        "X",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Filter out features with variance lower than the specified threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    threshold (float): The minimum variance required for a feature to be retained.\n\nReturns:\n    List[str]: A list of feature names with variance exceeding the threshold.\n\nRaises:\n    ValueError: If the threshold is negative."
                                },
                                {
                                    "name": "rank_features_by_importance",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "List[Tuple[str, float]]",
                                    "docstring": "Rank features based on their importance scores derived from a model-based approach.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    \nReturns:\n    List[Tuple[str, float]]: A list of tuples where each tuple contains a feature name and \n                             its corresponding importance score, sorted in descending order.\n\nRaises:\n    Exception: If feature importance cannot be computed."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_selection.py"
                    }
                ]
            },
            {
                "node": "select with mutual information",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Selection & Reduction/select with mutual information",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "FilterFeatureSelector",
                        "extra": {
                            "docstring": "A collection of filter-based feature selection methods.\n\nThis class implements multiple filtering techniques for feature selection such as:\n  - Select with chi-squared tests.\n  - Select with mutual information scores.\n  - Select features with high correlation to the target.\n  - Filter features based on low variance.\n  - Rank features by their computed importance.\n\nEach method is designed to work with input pandas DataFrames and aligns with the\ndata flow expectations of the system.",
                            "methods": [
                                {
                                    "name": "select_with_chi_squared",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on chi-squared test scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with non-negative features.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of selected feature names based on chi-squared results.\n\nRaises:\n    ValueError: If X contains negative values or if k is out of valid range."
                                },
                                {
                                    "name": "select_with_mutual_information",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "k"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select the top 'k' features based on mutual information scores.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    k (int): The number of top features to select.\n\nReturns:\n    List[str]: A list of feature names selected based on mutual information.\n\nRaises:\n    ValueError: If the mutual information calculation fails."
                                },
                                {
                                    "name": "select_high_correlation",
                                    "args": [
                                        "self",
                                        "X",
                                        "y",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Select features that have a correlation with the target variable above a given threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix with named columns.\n    y (Any): The target variable used for correlation computation.\n    threshold (float): The minimum correlation coefficient to consider. \n                       Should be between 0 and 1.\n\nReturns:\n    List[str]: A list of feature names that exhibit high correlation with the target.\n\nRaises:\n    ValueError: If threshold is not between 0 and 1."
                                },
                                {
                                    "name": "select_low_variance",
                                    "args": [
                                        "self",
                                        "X",
                                        "threshold"
                                    ],
                                    "return_type": "List[str]",
                                    "docstring": "Filter out features with variance lower than the specified threshold.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    threshold (float): The minimum variance required for a feature to be retained.\n\nReturns:\n    List[str]: A list of feature names with variance exceeding the threshold.\n\nRaises:\n    ValueError: If the threshold is negative."
                                },
                                {
                                    "name": "rank_features_by_importance",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "List[Tuple[str, float]]",
                                    "docstring": "Rank features based on their importance scores derived from a model-based approach.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    \nReturns:\n    List[Tuple[str, float]]: A list of tuples where each tuple contains a feature name and \n                             its corresponding importance score, sorted in descending order.\n\nRaises:\n    Exception: If feature importance cannot be computed."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_selection.py"
                    }
                ]
            },
            {
                "node": "wrapper methods",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Feature Selection & Reduction/wrapper methods",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "wrapper_feature_selection",
                        "extra": {
                            "args": [
                                "X",
                                "y",
                                "estimator",
                                "cv"
                            ],
                            "return_type": "List[str]",
                            "docstring": "Perform feature selection using a wrapper method based on model performance.\n\nThis function evaluates different subsets of features using cross-validation with the provided\nestimator and selects the combination that optimizes the performance metric.\n\nArgs:\n    X (pd.DataFrame): The input feature matrix.\n    y (Any): The target variable.\n    estimator (Any): A machine learning estimator used to evaluate feature subsets.\n    cv (int, optional): The number of cross-validation folds to use. Defaults to 5.\n\nReturns:\n    List[str]: A list of feature names selected by the wrapper method.\n\nEdge Cases:\n    - The estimator must be compatible with cross-validation.\n    - Computational cost may be high for large feature sets."
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/feature_selection.py"
                    }
                ]
            },
            {
                "node": "Interaction Terms",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Interaction Terms",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/feature_engineering/interaction_terms.py"
                    }
                ]
            },
            {
                "node": "categorical interaction terms",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Interaction Terms/categorical interaction terms",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "InteractionTermsGenerator",
                        "extra": {
                            "docstring": "InteractionTermsGenerator is a utility class that provides methods to generate interaction features\nfrom a given DataFrame. It supports creating categorical interaction terms, polynomial terms,\nmultiplicative terms, and ratio terms. These transformations are commonly used in feature engineering \nto capture non-linear relationships and interactions between features.\n\nMethods:\n    create_categorical_interaction_terms(df, columns) -> pd.DataFrame:\n        Generates interaction terms between specified categorical columns.\n    \n    create_polynomial_terms(df, degree, include_bias) -> pd.DataFrame:\n        Generates polynomial features of a given degree based on numerical columns.\n        \n    create_multiplicative_terms(df, columns) -> pd.DataFrame:\n        Generates new features by computing the product of values across the given columns.\n        \n    create_ratio_terms(df, numerator_columns, denominator_columns, epsilon) -> pd.DataFrame:\n        Generates new features by computing the ratio between specified numerator and denominator columns.",
                            "methods": [
                                {
                                    "name": "create_categorical_interaction_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate categorical interaction terms by combining specified categorical columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing the categorical features.\n    columns (List[str]): List of column names to combine for creating interaction terms.\n    \nReturns:\n    pd.DataFrame: A DataFrame with new columns representing the categorical interaction terms.\n\nEdge Cases:\n    - Returns the original DataFrame if the list of columns is empty.\n    - Assumes that the provided columns exist in the DataFrame."
                                },
                                {
                                    "name": "create_polynomial_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "degree",
                                        "include_bias"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate polynomial terms for numerical features up to the specified degree.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame with numerical features.\n    degree (int, optional): Degree of the polynomial features to be generated. Defaults to 2.\n    include_bias (bool, optional): If True, include a bias (intercept) column. Defaults to False.\n    \nReturns:\n    pd.DataFrame: A DataFrame with additional polynomial features.\n\nEdge Cases:\n    - Assumes that the DataFrame contains only numerical columns for transformation.\n    - If degree is less than 2, no additional interaction is created."
                                },
                                {
                                    "name": "create_multiplicative_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Create multiplicative interaction terms by multiplying the values of specified columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing features.\n    columns (List[str]): List of columns whose element-wise products are to be computed.\n    \nReturns:\n    pd.DataFrame: A DataFrame enriched with multiplicative interaction features.\n\nEdge Cases:\n    - If fewer than two columns are provided, the function returns the DataFrame unmodified.\n    - Assumes numerical data for valid multiplication."
                                },
                                {
                                    "name": "create_ratio_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "numerator_columns",
                                        "denominator_columns",
                                        "epsilon"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Create ratio terms by computing the element-wise ratios of the specified numerator and denominator columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical data.\n    numerator_columns (List[str]): List of column names to be used as numerators.\n    denominator_columns (List[str]): List of column names to be used as denominators.\n    epsilon (float, optional): A small number added to denominators to avoid division by zero. Defaults to 1e-8.\n    \nReturns:\n    pd.DataFrame: A DataFrame with new columns representing the ratio terms.\n\nEdge Cases:\n    - Assumes that for each ratio, corresponding numerator and denominator columns exist.\n    - If denominator values are zero, the epsilon value prevents division errors."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/interaction_terms.py"
                    }
                ]
            },
            {
                "node": "create multiplicative terms",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Interaction Terms/create multiplicative terms",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "InteractionTermsGenerator",
                        "extra": {
                            "docstring": "InteractionTermsGenerator is a utility class that provides methods to generate interaction features\nfrom a given DataFrame. It supports creating categorical interaction terms, polynomial terms,\nmultiplicative terms, and ratio terms. These transformations are commonly used in feature engineering \nto capture non-linear relationships and interactions between features.\n\nMethods:\n    create_categorical_interaction_terms(df, columns) -> pd.DataFrame:\n        Generates interaction terms between specified categorical columns.\n    \n    create_polynomial_terms(df, degree, include_bias) -> pd.DataFrame:\n        Generates polynomial features of a given degree based on numerical columns.\n        \n    create_multiplicative_terms(df, columns) -> pd.DataFrame:\n        Generates new features by computing the product of values across the given columns.\n        \n    create_ratio_terms(df, numerator_columns, denominator_columns, epsilon) -> pd.DataFrame:\n        Generates new features by computing the ratio between specified numerator and denominator columns.",
                            "methods": [
                                {
                                    "name": "create_categorical_interaction_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate categorical interaction terms by combining specified categorical columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing the categorical features.\n    columns (List[str]): List of column names to combine for creating interaction terms.\n    \nReturns:\n    pd.DataFrame: A DataFrame with new columns representing the categorical interaction terms.\n\nEdge Cases:\n    - Returns the original DataFrame if the list of columns is empty.\n    - Assumes that the provided columns exist in the DataFrame."
                                },
                                {
                                    "name": "create_polynomial_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "degree",
                                        "include_bias"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate polynomial terms for numerical features up to the specified degree.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame with numerical features.\n    degree (int, optional): Degree of the polynomial features to be generated. Defaults to 2.\n    include_bias (bool, optional): If True, include a bias (intercept) column. Defaults to False.\n    \nReturns:\n    pd.DataFrame: A DataFrame with additional polynomial features.\n\nEdge Cases:\n    - Assumes that the DataFrame contains only numerical columns for transformation.\n    - If degree is less than 2, no additional interaction is created."
                                },
                                {
                                    "name": "create_multiplicative_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Create multiplicative interaction terms by multiplying the values of specified columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing features.\n    columns (List[str]): List of columns whose element-wise products are to be computed.\n    \nReturns:\n    pd.DataFrame: A DataFrame enriched with multiplicative interaction features.\n\nEdge Cases:\n    - If fewer than two columns are provided, the function returns the DataFrame unmodified.\n    - Assumes numerical data for valid multiplication."
                                },
                                {
                                    "name": "create_ratio_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "numerator_columns",
                                        "denominator_columns",
                                        "epsilon"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Create ratio terms by computing the element-wise ratios of the specified numerator and denominator columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical data.\n    numerator_columns (List[str]): List of column names to be used as numerators.\n    denominator_columns (List[str]): List of column names to be used as denominators.\n    epsilon (float, optional): A small number added to denominators to avoid division by zero. Defaults to 1e-8.\n    \nReturns:\n    pd.DataFrame: A DataFrame with new columns representing the ratio terms.\n\nEdge Cases:\n    - Assumes that for each ratio, corresponding numerator and denominator columns exist.\n    - If denominator values are zero, the epsilon value prevents division errors."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/interaction_terms.py"
                    }
                ]
            },
            {
                "node": "create polynomial terms",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Interaction Terms/create polynomial terms",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "InteractionTermsGenerator",
                        "extra": {
                            "docstring": "InteractionTermsGenerator is a utility class that provides methods to generate interaction features\nfrom a given DataFrame. It supports creating categorical interaction terms, polynomial terms,\nmultiplicative terms, and ratio terms. These transformations are commonly used in feature engineering \nto capture non-linear relationships and interactions between features.\n\nMethods:\n    create_categorical_interaction_terms(df, columns) -> pd.DataFrame:\n        Generates interaction terms between specified categorical columns.\n    \n    create_polynomial_terms(df, degree, include_bias) -> pd.DataFrame:\n        Generates polynomial features of a given degree based on numerical columns.\n        \n    create_multiplicative_terms(df, columns) -> pd.DataFrame:\n        Generates new features by computing the product of values across the given columns.\n        \n    create_ratio_terms(df, numerator_columns, denominator_columns, epsilon) -> pd.DataFrame:\n        Generates new features by computing the ratio between specified numerator and denominator columns.",
                            "methods": [
                                {
                                    "name": "create_categorical_interaction_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate categorical interaction terms by combining specified categorical columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing the categorical features.\n    columns (List[str]): List of column names to combine for creating interaction terms.\n    \nReturns:\n    pd.DataFrame: A DataFrame with new columns representing the categorical interaction terms.\n\nEdge Cases:\n    - Returns the original DataFrame if the list of columns is empty.\n    - Assumes that the provided columns exist in the DataFrame."
                                },
                                {
                                    "name": "create_polynomial_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "degree",
                                        "include_bias"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate polynomial terms for numerical features up to the specified degree.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame with numerical features.\n    degree (int, optional): Degree of the polynomial features to be generated. Defaults to 2.\n    include_bias (bool, optional): If True, include a bias (intercept) column. Defaults to False.\n    \nReturns:\n    pd.DataFrame: A DataFrame with additional polynomial features.\n\nEdge Cases:\n    - Assumes that the DataFrame contains only numerical columns for transformation.\n    - If degree is less than 2, no additional interaction is created."
                                },
                                {
                                    "name": "create_multiplicative_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Create multiplicative interaction terms by multiplying the values of specified columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing features.\n    columns (List[str]): List of columns whose element-wise products are to be computed.\n    \nReturns:\n    pd.DataFrame: A DataFrame enriched with multiplicative interaction features.\n\nEdge Cases:\n    - If fewer than two columns are provided, the function returns the DataFrame unmodified.\n    - Assumes numerical data for valid multiplication."
                                },
                                {
                                    "name": "create_ratio_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "numerator_columns",
                                        "denominator_columns",
                                        "epsilon"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Create ratio terms by computing the element-wise ratios of the specified numerator and denominator columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical data.\n    numerator_columns (List[str]): List of column names to be used as numerators.\n    denominator_columns (List[str]): List of column names to be used as denominators.\n    epsilon (float, optional): A small number added to denominators to avoid division by zero. Defaults to 1e-8.\n    \nReturns:\n    pd.DataFrame: A DataFrame with new columns representing the ratio terms.\n\nEdge Cases:\n    - Assumes that for each ratio, corresponding numerator and denominator columns exist.\n    - If denominator values are zero, the epsilon value prevents division errors."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/interaction_terms.py"
                    }
                ]
            },
            {
                "node": "create ratio terms",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Interaction Terms/create ratio terms",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "InteractionTermsGenerator",
                        "extra": {
                            "docstring": "InteractionTermsGenerator is a utility class that provides methods to generate interaction features\nfrom a given DataFrame. It supports creating categorical interaction terms, polynomial terms,\nmultiplicative terms, and ratio terms. These transformations are commonly used in feature engineering \nto capture non-linear relationships and interactions between features.\n\nMethods:\n    create_categorical_interaction_terms(df, columns) -> pd.DataFrame:\n        Generates interaction terms between specified categorical columns.\n    \n    create_polynomial_terms(df, degree, include_bias) -> pd.DataFrame:\n        Generates polynomial features of a given degree based on numerical columns.\n        \n    create_multiplicative_terms(df, columns) -> pd.DataFrame:\n        Generates new features by computing the product of values across the given columns.\n        \n    create_ratio_terms(df, numerator_columns, denominator_columns, epsilon) -> pd.DataFrame:\n        Generates new features by computing the ratio between specified numerator and denominator columns.",
                            "methods": [
                                {
                                    "name": "create_categorical_interaction_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate categorical interaction terms by combining specified categorical columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing the categorical features.\n    columns (List[str]): List of column names to combine for creating interaction terms.\n    \nReturns:\n    pd.DataFrame: A DataFrame with new columns representing the categorical interaction terms.\n\nEdge Cases:\n    - Returns the original DataFrame if the list of columns is empty.\n    - Assumes that the provided columns exist in the DataFrame."
                                },
                                {
                                    "name": "create_polynomial_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "degree",
                                        "include_bias"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate polynomial terms for numerical features up to the specified degree.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame with numerical features.\n    degree (int, optional): Degree of the polynomial features to be generated. Defaults to 2.\n    include_bias (bool, optional): If True, include a bias (intercept) column. Defaults to False.\n    \nReturns:\n    pd.DataFrame: A DataFrame with additional polynomial features.\n\nEdge Cases:\n    - Assumes that the DataFrame contains only numerical columns for transformation.\n    - If degree is less than 2, no additional interaction is created."
                                },
                                {
                                    "name": "create_multiplicative_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "columns"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Create multiplicative interaction terms by multiplying the values of specified columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing features.\n    columns (List[str]): List of columns whose element-wise products are to be computed.\n    \nReturns:\n    pd.DataFrame: A DataFrame enriched with multiplicative interaction features.\n\nEdge Cases:\n    - If fewer than two columns are provided, the function returns the DataFrame unmodified.\n    - Assumes numerical data for valid multiplication."
                                },
                                {
                                    "name": "create_ratio_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "numerator_columns",
                                        "denominator_columns",
                                        "epsilon"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Create ratio terms by computing the element-wise ratios of the specified numerator and denominator columns.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical data.\n    numerator_columns (List[str]): List of column names to be used as numerators.\n    denominator_columns (List[str]): List of column names to be used as denominators.\n    epsilon (float, optional): A small number added to denominators to avoid division by zero. Defaults to 1e-8.\n    \nReturns:\n    pd.DataFrame: A DataFrame with new columns representing the ratio terms.\n\nEdge Cases:\n    - Assumes that for each ratio, corresponding numerator and denominator columns exist.\n    - If denominator values are zero, the epsilon value prevents division errors."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/interaction_terms.py"
                    }
                ]
            },
            {
                "node": "Polynomial Features",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Polynomial Features",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/feature_engineering/polynomial_features.py"
                    }
                ]
            },
            {
                "node": "create polynomial features of degree 2",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Polynomial Features/create polynomial features of degree 2",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "PolynomialFeaturesGenerator",
                        "extra": {
                            "docstring": "A generator for creating polynomial features from numerical data.\n\nThis class provides methods to generate higher-order polynomial terms\nfrom a given DataFrame. It includes a generic method to create polynomial terms \nup to any specified degree, a method specifically to create third-order terms,\nand a method to generate second-order (degree 2) polynomial features.\n\nMethods:\n    generate_higher_order_terms(df: pd.DataFrame, degree: int) -> pd.DataFrame:\n        Generates polynomial features up to the specified degree.\n    create_third_order_terms(df: pd.DataFrame) -> pd.DataFrame:\n        Generates polynomial features of exactly third order.\n    create_degree2_polynomial_features(df: pd.DataFrame) -> pd.DataFrame:\n        Generates polynomial features of degree 2.\n\nNote:\n    - It is assumed that the input DataFrame contains only numerical data.\n    - Preprocessing to handle non-numerical data should be done beforehand.",
                            "methods": [
                                {
                                    "name": "generate_higher_order_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "degree"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate polynomial features up to the specified degree.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical features.\n    degree (int): Maximum power for the polynomial expansion (>= 2).\n\nReturns:\n    pd.DataFrame: DataFrame with the generated polynomial features appended."
                                },
                                {
                                    "name": "create_third_order_terms",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Create polynomial features of exactly third order.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical features.\n\nReturns:\n    pd.DataFrame: DataFrame with third-order polynomial features added."
                                },
                                {
                                    "name": "create_degree2_polynomial_features",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate polynomial features of degree 2.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical features.\n\nReturns:\n    pd.DataFrame: DataFrame with second-order (degree 2) polynomial features appended."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/polynomial_features.py"
                    }
                ]
            },
            {
                "node": "create third-order terms",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Polynomial Features/create third-order terms",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "PolynomialFeaturesGenerator",
                        "extra": {
                            "docstring": "A generator for creating polynomial features from numerical data.\n\nThis class provides methods to generate higher-order polynomial terms\nfrom a given DataFrame. It includes a generic method to create polynomial terms \nup to any specified degree, a method specifically to create third-order terms,\nand a method to generate second-order (degree 2) polynomial features.\n\nMethods:\n    generate_higher_order_terms(df: pd.DataFrame, degree: int) -> pd.DataFrame:\n        Generates polynomial features up to the specified degree.\n    create_third_order_terms(df: pd.DataFrame) -> pd.DataFrame:\n        Generates polynomial features of exactly third order.\n    create_degree2_polynomial_features(df: pd.DataFrame) -> pd.DataFrame:\n        Generates polynomial features of degree 2.\n\nNote:\n    - It is assumed that the input DataFrame contains only numerical data.\n    - Preprocessing to handle non-numerical data should be done beforehand.",
                            "methods": [
                                {
                                    "name": "generate_higher_order_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "degree"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate polynomial features up to the specified degree.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical features.\n    degree (int): Maximum power for the polynomial expansion (>= 2).\n\nReturns:\n    pd.DataFrame: DataFrame with the generated polynomial features appended."
                                },
                                {
                                    "name": "create_third_order_terms",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Create polynomial features of exactly third order.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical features.\n\nReturns:\n    pd.DataFrame: DataFrame with third-order polynomial features added."
                                },
                                {
                                    "name": "create_degree2_polynomial_features",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate polynomial features of degree 2.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical features.\n\nReturns:\n    pd.DataFrame: DataFrame with second-order (degree 2) polynomial features appended."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/polynomial_features.py"
                    }
                ]
            },
            {
                "node": "generate higher-order terms",
                "feature_path": "Data Engineering/Data Preparation/Feature Engineering/Polynomial Features/generate higher-order terms",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "PolynomialFeaturesGenerator",
                        "extra": {
                            "docstring": "A generator for creating polynomial features from numerical data.\n\nThis class provides methods to generate higher-order polynomial terms\nfrom a given DataFrame. It includes a generic method to create polynomial terms \nup to any specified degree, a method specifically to create third-order terms,\nand a method to generate second-order (degree 2) polynomial features.\n\nMethods:\n    generate_higher_order_terms(df: pd.DataFrame, degree: int) -> pd.DataFrame:\n        Generates polynomial features up to the specified degree.\n    create_third_order_terms(df: pd.DataFrame) -> pd.DataFrame:\n        Generates polynomial features of exactly third order.\n    create_degree2_polynomial_features(df: pd.DataFrame) -> pd.DataFrame:\n        Generates polynomial features of degree 2.\n\nNote:\n    - It is assumed that the input DataFrame contains only numerical data.\n    - Preprocessing to handle non-numerical data should be done beforehand.",
                            "methods": [
                                {
                                    "name": "generate_higher_order_terms",
                                    "args": [
                                        "self",
                                        "df",
                                        "degree"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate polynomial features up to the specified degree.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical features.\n    degree (int): Maximum power for the polynomial expansion (>= 2).\n\nReturns:\n    pd.DataFrame: DataFrame with the generated polynomial features appended."
                                },
                                {
                                    "name": "create_third_order_terms",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Create polynomial features of exactly third order.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical features.\n\nReturns:\n    pd.DataFrame: DataFrame with third-order polynomial features added."
                                },
                                {
                                    "name": "create_degree2_polynomial_features",
                                    "args": [
                                        "self",
                                        "df"
                                    ],
                                    "return_type": "pd.DataFrame",
                                    "docstring": "Generate polynomial features of degree 2.\n\nArgs:\n    df (pd.DataFrame): Input DataFrame containing numerical features.\n\nReturns:\n    pd.DataFrame: DataFrame with second-order (degree 2) polynomial features appended."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_preparation/feature_engineering/polynomial_features.py"
                    }
                ]
            },
            {
                "node": "Imputation Techniques",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_preparation/imputation"
                    }
                ]
            },
            {
                "node": "Imputation Methods",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "Weighted mean",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/Weighted mean",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "weighted_mean_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "weight_col"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column using a weighted mean computed from a specified weight column.\n\nThe weighted mean is calculated considering the weights provided, and missing values in the target column\nare replaced with the computed weighted mean.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing missing values.\n    target_col (str): The column in which to impute missing values.\n    weight_col (str): The column containing weights for calculating the weighted mean.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed using the weighted mean.\n    \nAssumptions:\n    - Both target_col and weight_col exist in the DataFrame and contain numeric data."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "conditional mean imputation",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/conditional mean imputation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "conditional_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "condition",
                                "strategy"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Perform conditional imputation on the target column based on provided conditions.\n\nDepending on the chosen strategy, either the mean or median is computed on subsets of data that satisfy\nthe condition specified by a dictionary of column-value pairs. Missing values are imputed accordingly.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing values.\n    target_col (str): The column in which missing values will be imputed.\n    condition (dict): A dictionary where keys are column names and values define the condition to filter the DataFrame.\n    strategy (str): The imputation strategy. Expected values are \"mean\" or \"median\". Defaults to \"mean\".\n    \nReturns:\n    pd.DataFrame: A DataFrame with conditionally imputed values in the target column.\n    \nAssumptions:\n    - The condition provided effectively segments the DataFrame into meaningful subsets."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "conditional median imputation",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/conditional median imputation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "conditional_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "condition",
                                "strategy"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Perform conditional imputation on the target column based on provided conditions.\n\nDepending on the chosen strategy, either the mean or median is computed on subsets of data that satisfy\nthe condition specified by a dictionary of column-value pairs. Missing values are imputed accordingly.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing values.\n    target_col (str): The column in which missing values will be imputed.\n    condition (dict): A dictionary where keys are column names and values define the condition to filter the DataFrame.\n    strategy (str): The imputation strategy. Expected values are \"mean\" or \"median\". Defaults to \"mean\".\n    \nReturns:\n    pd.DataFrame: A DataFrame with conditionally imputed values in the target column.\n    \nAssumptions:\n    - The condition provided effectively segments the DataFrame into meaningful subsets."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "delete columns",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/delete columns",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "delete_columns",
                        "extra": {
                            "args": [
                                "df",
                                "missing_threshold"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Delete columns from the DataFrame based on a missing data threshold.\n\nThis function drops columns where the proportion of missing values exceeds the specified threshold.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with potential missing data.\n    missing_threshold (float): The maximum allowed fraction of missing data for a column to be retained.\n                              Columns with a higher fraction of missing data will be deleted. Defaults to 0.5.\n    \nReturns:\n    pd.DataFrame: A DataFrame with columns removed if they exceeded the missing data threshold.\n    \nEdge Cases:\n    - If missing_threshold is set to a value outside [0,1], behavior is undefined."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "delete incomplete records",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/delete incomplete records",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "delete_incomplete_records",
                        "extra": {
                            "args": [
                                "df"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Delete records (rows) from the DataFrame that contain any missing values.\n\nThis function removes all rows where any column has a missing entry.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing values.\n    \nReturns:\n    pd.DataFrame: A DataFrame with all rows containing missing values removed.\n    \nEdge Cases:\n    - If the DataFrame is entirely missing values or empty, the result should be an empty DataFrame."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "drop based on percentage missing",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/drop based on percentage missing",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "drop_columns_based_on_percentage",
                        "extra": {
                            "args": [
                                "df",
                                "percentage"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Drop columns from the DataFrame where the percentage of missing values exceeds a specified limit.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame.\n    percentage (float): The maximum allowed percentage (between 0 and 100) of missing values in a column.\n    \nReturns:\n    pd.DataFrame: A DataFrame with columns dropped if their missing data percentage exceeds the specified limit.\n    \nConstraints:\n    - The value of percentage should be within the range 0 to 100."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "flag outliers",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/flag outliers",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "flag_outliers",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "method",
                                "factor"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Flag outlier values in the target column using a specified method.\n\nThis function marks outliers in the DataFrame based on the specified statistical method (e.g., IQR).\nThe flagged outliers can be used for further inspection or processing.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame.\n    target_col (str): The column in which to flag outliers.\n    method (str): The method to detect outliers (e.g., \"IQR\"). Defaults to \"IQR\".\n    factor (float): The multiplier applied in the outlier detection method. Defaults to 1.5.\n    \nReturns:\n    pd.DataFrame: A DataFrame with an additional boolean column (or modified target column) indicating outlier status.\n    \nRemarks:\n    - The implementation should clearly document how outliers are flagged."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "flag rows with missing values",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/flag rows with missing values",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "flag_missing_rows",
                        "extra": {
                            "args": [
                                "df",
                                "threshold"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Flag rows with a high proportion of missing values.\n\nThis function adds a boolean indicator to each row in the DataFrame informing whether the row has \nmissing data exceeding a specified threshold.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame.\n    threshold (float): The fraction of missing values in a row required to flag it as problematic. Defaults to 0.2.\n    \nReturns:\n    pd.DataFrame: The original DataFrame augmented with an additional column indicating rows with excessive missing values.\n    \nConstraints:\n    - Assumes the threshold is a value between 0 and 1."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "handle large datasets for mode imputation",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/handle large datasets for mode imputation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "optimized_mode_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Perform mode imputation on large datasets with optimization considerations.\n\nThis function imputes missing values in the target column by computing the mode in a manner optimized for large datasets,\npotentially using chunk processing or memory-efficient algorithms.\n\nArgs:\n    df (pd.DataFrame): The large input DataFrame with missing values.\n    target_col (str): The column on which to perform mode imputation.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values imputed using an optimized mode computation.\n    \nConsiderations:\n    - Designed for performance and memory efficiency on very large DataFrames."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "identify missing",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/identify missing",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "identify_missing",
                        "extra": {
                            "args": [
                                "df"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Identify missing values in the DataFrame and return a DataFrame indicating the positions of missing data.\n\nThis function scans the entire DataFrame to detect locations (rows and columns) with missing data.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame to be analyzed for missing values.\n    \nReturns:\n    pd.DataFrame: A boolean DataFrame of the same shape as input, where True indicates a missing value.\n    \nEdge Cases:\n    - An empty DataFrame is returned if no data is present."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "impute by group mean",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/impute by group mean",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "group_based_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "group_col",
                                "target_col",
                                "strategy"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column by computing grouped statistics.\n\nThis function imputes missing values in a specified target column within each group defined by 'group_col'.\nThe imputation strategy can be set to either 'mean' or 'median' to perform the corresponding imputation.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing missing values.\n    group_col (str): Column name to group the data by.\n    target_col (str): The target column in which missing values are to be imputed.\n    strategy (str): The imputation strategy. Expected values are \"mean\" or \"median\". Defaults to \"mean\".\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed using the group-based strategy.\n    \nEdge Cases:\n    - If the group or target columns do not exist, processing should be handled upstream.\n    - Assumes that the groups are non-empty and the target column is numeric."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "impute by group median",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/impute by group median",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "group_based_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "group_col",
                                "target_col",
                                "strategy"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column by computing grouped statistics.\n\nThis function imputes missing values in a specified target column within each group defined by 'group_col'.\nThe imputation strategy can be set to either 'mean' or 'median' to perform the corresponding imputation.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing missing values.\n    group_col (str): Column name to group the data by.\n    target_col (str): The target column in which missing values are to be imputed.\n    strategy (str): The imputation strategy. Expected values are \"mean\" or \"median\". Defaults to \"mean\".\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed using the group-based strategy.\n    \nEdge Cases:\n    - If the group or target columns do not exist, processing should be handled upstream.\n    - Assumes that the groups are non-empty and the target column is numeric."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "impute categorical columns",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/impute categorical columns",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "categorical_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "strategy"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in categorical columns using a specified strategy.\n\nThis function imputes missing values for categorical data. By default, it uses the mode, but other strategies\ncan be specified if needed.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing categorical data.\n    target_col (str): The categorical column to impute.\n    strategy (str): The imputation strategy to use (e.g., \"mode\", \"constant\"). Defaults to \"mode\".\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the categorical column imputed.\n    \nAssumptions:\n    - The target column contains categorical data."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "impute missing binary data with mode",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/impute missing binary data with mode",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "binary_mode_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing binary data in the target column using the mode.\n\nThis function determines the most frequent binary value in the target column and uses it to fill missing entries.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing binary data.\n    target_col (str): The name of the binary column to be imputed.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the binary target column imputed with its mode.\n    \nConstraints:\n    - Assumes that the target column only contains two distinct values aside from missing values."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "impute using local median",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/impute using local median",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "local_median_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "neighborhood"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column by computing the median over a local neighborhood.\n\nThis function replaces missing values with the median computed from a specified number of neighboring entries.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing values.\n    target_col (str): The column in which missing data is to be imputed.\n    neighborhood (int): The number of adjacent data points to consider for computing the local median. Defaults to 5.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed using a local median.\n    \nAssumptions:\n    - The DataFrame is sorted appropriately such that a local neighborhood is meaningful."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "impute with column median",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/impute with column median",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "column_median_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column using the median computed for that specific column.\n\nThis function calculates the median value of the target column (ignoring missing values)\nand fills missing entries with that median.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing data.\n    target_col (str): The name of the column to be imputed.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column filled using its median.\n    \nConstraints:\n    - The target column is assumed to be numeric."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "impute with conditional mode",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/impute with conditional mode",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "conditional_mode_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "condition"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column conditionally using the mode.\n\nThe function segments the DataFrame based on given conditions and fills missing values with the most frequent value\nwithin each segment.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing data.\n    target_col (str): The column to be imputed.\n    condition (dict): A dictionary specifying column names and their desired values to filter the DataFrame.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values conditionally imputed using the mode.\n    \nEdge Cases:\n    - If the condition does not segregate the data, the global mode might be used."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "impute with global mean",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/impute with global mean",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "global_mean_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column using the global mean.\n\nThis function computes the mean of the target column over the entire DataFrame (ignoring missing data)\nand fills missing values with this mean.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing missing values.\n    target_col (str): The column to be imputed.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed using the global mean.\n    \nConstraints:\n    - Assumes that the target column contains numeric data."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "impute with global median",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/impute with global median",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "global_median_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column using the global median.\n\nThis function computes the median of the entire column (ignoring missing entries) and replaces missing values with it.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing missing data.\n    target_col (str): The name of the column where missing values will be imputed.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed with its global median.\n    \nConstraints:\n    - Assumes that the target column contains numeric data."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "impute with mode",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/impute with mode",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "mode_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "subset"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column using mode imputation.\n\nThis function computes the mode either for the entire column or for a specified subset of data\n(if 'subset' is provided) and fills missing values with the most frequent value.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing data.\n    target_col (str): The column in which to perform mode imputation.\n    subset (list, optional): A list of column names to consider when computing the mode. If None, the mode is\n                             computed using the entire column. Defaults to None.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed using the mode.\n    \nAssumptions:\n    - The target column contains categorical or numerical data for which mode is an appropriate statistic."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "knn imputation",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/knn imputation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "knn_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "n_neighbors"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in a target column using the K-Nearest Neighbors algorithm.\n\nThis function identifies the k-nearest rows with non-missing values in the target column and imputes\nmissing values by aggregating these neighbors (e.g., by taking their mean).\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing values.\n    target_col (str): The column in which to impute missing values.\n    n_neighbors (int): The number of neighbors to consider for imputation. Defaults to 5.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed using KNN.\n    \nAssumptions:\n    - The DataFrame is appropriately scaled so that the distance metric in KNN is meaningful."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "linear interpolation",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/linear interpolation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "linear_interpolation_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column using linear interpolation.\n\nThe function linearly interpolates missing values based on surrounding data points in the target column.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing missing values.\n    target_col (str): The name of the column for linear interpolation.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed via linear interpolation.\n    \nConstraints:\n    - Assumes the data is ordered so that linear interpolation is meaningful."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "mode imputation for numbers",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/mode imputation for numbers",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "mode_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "subset"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column using mode imputation.\n\nThis function computes the mode either for the entire column or for a specified subset of data\n(if 'subset' is provided) and fills missing values with the most frequent value.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing data.\n    target_col (str): The column in which to perform mode imputation.\n    subset (list, optional): A list of column names to consider when computing the mode. If None, the mode is\n                             computed using the entire column. Defaults to None.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed using the mode.\n    \nAssumptions:\n    - The target column contains categorical or numerical data for which mode is an appropriate statistic."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "mode of subset of dataset",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/mode of subset of dataset",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "mode_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "subset"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column using mode imputation.\n\nThis function computes the mode either for the entire column or for a specified subset of data\n(if 'subset' is provided) and fills missing values with the most frequent value.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with missing data.\n    target_col (str): The column in which to perform mode imputation.\n    subset (list, optional): A list of column names to consider when computing the mode. If None, the mode is\n                             computed using the entire column. Defaults to None.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed using the mode.\n    \nAssumptions:\n    - The target column contains categorical or numerical data for which mode is an appropriate statistic."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "model-based imputation",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/model-based imputation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "model_based_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "model",
                                "features"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values using a predictive model.\n\nThis function leverages a given model to predict missing values in the target column based on other related features.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing missing values.\n    target_col (str): The column where imputation is required.\n    model: A predictive model instance that implements fit/predict.\n    features (list): A list of column names to be used as predictors for the model.\n    \nReturns:\n    pd.DataFrame: A DataFrame with the target column imputed using model predictions.\n    \nAssumptions:\n    - The provided model follows a standard interface (fit and predict methods)."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "pairwise deletion",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/pairwise deletion",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "pairwise_deletion",
                        "extra": {
                            "args": [
                                "df",
                                "target_cols"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Apply pairwise deletion to handle missing data across specified columns.\n\nThis function removes rows based on the presence of missing values in a pairwise manner from the specified columns.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing missing data.\n    target_cols (list): List of column names on which the pairwise deletion should be based.\n    \nReturns:\n    pd.DataFrame: A DataFrame where rows with missing data in specified column pairs are removed.\n    \nEdge Cases:\n    - If target_cols is empty, the function will return the original DataFrame."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "polynomial interpolation",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/polynomial interpolation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "polynomial_interpolation_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "degree"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Impute missing values in the target column using polynomial interpolation.\n\nThis function fits a polynomial of a specified degree to the non-missing data and uses it to estimate missing values.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with gaps in the target column.\n    target_col (str): The column to interpolate.\n    degree (int): The degree of the polynomial to be used for interpolation. Defaults to 2.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column imputed by polynomial interpolation.\n    \nRemarks:\n    - Suitable for time series or sequential data where polynomial trends are expected."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "simple imputation",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/simple imputation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "simple_imputation",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "fill_value"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Perform a simple imputation on the target column using a constant fill value.\n\nThis function replaces missing values in a specified column with a provided constant.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing missing values.\n    target_col (str): The name of the column in which to impute missing data.\n    fill_value: The constant value to use for imputing missing entries.\n    \nReturns:\n    pd.DataFrame: A DataFrame with missing values in the target column replaced by the fill_value.\n    \nAssumptions:\n    - The fill_value is appropriate for the data type of the target column."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "threshold-based removal",
                "feature_path": "Data Engineering/Data Preparation/Imputation Techniques/Imputation Methods/threshold-based removal",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "threshold_based_removal",
                        "extra": {
                            "args": [
                                "df",
                                "target_col",
                                "threshold"
                            ],
                            "return_type": "pd.DataFrame",
                            "docstring": "Remove entries from the target column if their value exceeds a specified threshold.\n\nThis function deletes rows or marks values for removal if they do not meet the threshold criterion.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame.\n    target_col (str): The column to check against the threshold.\n    threshold (float): The threshold value that determines if an entry should be removed.\n    \nReturns:\n    pd.DataFrame: A DataFrame with rows removed or flagged where the target column exceeds the threshold.\n    \nEdge Cases:\n    - Behavior when no rows meet the threshold should be clearly documented."
                        },
                        "path": "src/data_engineering/data_preparation/imputation/imputation_methods.py"
                    }
                ]
            },
            {
                "node": "Data Splitting",
                "feature_path": "Data Engineering/Data Splitting",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_splitting"
                    }
                ]
            },
            {
                "node": "Temporal Split",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_splitting"
                    }
                ]
            },
            {
                "node": "Cross-validation Temporal Split",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Cross-validation Temporal Split",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_splitting/cv_temporal.py"
                    }
                ]
            },
            {
                "node": "cross-validation with event windows",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Cross-validation Temporal Split/cross-validation with event windows",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "CVTemporalSplitter",
                        "extra": {
                            "docstring": "Provides cross-validation splitting strategies for temporal data with various approaches.\n\nThis class implements three methods to generate cross-validation splits that respect the temporal \nstructure of the data. It supports:\n  - Time-series cross-validation: Splitting based on chronological order.\n  - Cross-validation with event windows: Creating splits based on event-based windows.\n  - Cross-validation with time windows: Dividing data into splits based on predefined time intervals.\n\nEach method returns a list of tuples, where each tuple contains two lists:\n  - The first list contains indices for the training set.\n  - The second list contains indices for the testing set.\n  \nMethods:\n    time_series_cv: Generates splits for time-series cross-validation.\n    event_windows_cv: Generates splits based on event windows.\n    time_windows_cv: Generates splits based on time windows.",
                            "methods": [
                                {
                                    "name": "time_series_cv",
                                    "args": [
                                        "self",
                                        "X",
                                        "n_splits"
                                    ],
                                    "return_type": "List[Tuple[List[int], List[int]]]",
                                    "docstring": "Perform time-series cross-validation by splitting data sequentially.\n\nThis method assumes that the input DataFrame X is ordered by time. The function generates\nn_splits pairs of training and testing indices, where each successive split uses later portions \nof the data for testing.\n\nArgs:\n    X (pd.DataFrame): The input data ordered chronologically.\n    n_splits (int): The number of splits to generate. Default is 5.\n    \nReturns:\n    List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices."
                                },
                                {
                                    "name": "event_windows_cv",
                                    "args": [
                                        "self",
                                        "X",
                                        "event_column",
                                        "window_size"
                                    ],
                                    "return_type": "List[Tuple[List[int], List[int]]]",
                                    "docstring": "Perform cross-validation using event windows.\n\nThis method splits the DataFrame based on event occurrences specified in the event_column.\nEach window is determined by a fixed number of events (window_size), allowing the cross-validation\nprocess to account for clustered or bursty events and their implications on data distribution.\n\nArgs:\n    X (pd.DataFrame): The input data that includes an event indicator column.\n    event_column (str): The name of the column that specifies events.\n    window_size (int): The number of events to include in each window.\n    \nReturns:\n    List[Tuple[List[int], List[int]]]: A list of tuples, each containing indices for training and testing sets."
                                },
                                {
                                    "name": "time_windows_cv",
                                    "args": [
                                        "self",
                                        "X",
                                        "time_column",
                                        "window_duration"
                                    ],
                                    "return_type": "List[Tuple[List[int], List[int]]]",
                                    "docstring": "Perform cross-validation using predefined time windows.\n\nThis method splits the DataFrame based on a time column. Each split is created by segmenting the data \ninto intervals determined by the window_duration parameter. This allows for evaluation across \nconsistent time intervals in the data.\n\nArgs:\n    X (pd.DataFrame): The input data containing a time-based column.\n    time_column (str): The name of the column representing time.\n    window_duration: The duration of each time window (e.g., an integer representing days or a pandas offset).\n    \nReturns:\n    List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/cv_temporal.py"
                    }
                ]
            },
            {
                "node": "cross-validation with time windows",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Cross-validation Temporal Split/cross-validation with time windows",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "CVTemporalSplitter",
                        "extra": {
                            "docstring": "Provides cross-validation splitting strategies for temporal data with various approaches.\n\nThis class implements three methods to generate cross-validation splits that respect the temporal \nstructure of the data. It supports:\n  - Time-series cross-validation: Splitting based on chronological order.\n  - Cross-validation with event windows: Creating splits based on event-based windows.\n  - Cross-validation with time windows: Dividing data into splits based on predefined time intervals.\n\nEach method returns a list of tuples, where each tuple contains two lists:\n  - The first list contains indices for the training set.\n  - The second list contains indices for the testing set.\n  \nMethods:\n    time_series_cv: Generates splits for time-series cross-validation.\n    event_windows_cv: Generates splits based on event windows.\n    time_windows_cv: Generates splits based on time windows.",
                            "methods": [
                                {
                                    "name": "time_series_cv",
                                    "args": [
                                        "self",
                                        "X",
                                        "n_splits"
                                    ],
                                    "return_type": "List[Tuple[List[int], List[int]]]",
                                    "docstring": "Perform time-series cross-validation by splitting data sequentially.\n\nThis method assumes that the input DataFrame X is ordered by time. The function generates\nn_splits pairs of training and testing indices, where each successive split uses later portions \nof the data for testing.\n\nArgs:\n    X (pd.DataFrame): The input data ordered chronologically.\n    n_splits (int): The number of splits to generate. Default is 5.\n    \nReturns:\n    List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices."
                                },
                                {
                                    "name": "event_windows_cv",
                                    "args": [
                                        "self",
                                        "X",
                                        "event_column",
                                        "window_size"
                                    ],
                                    "return_type": "List[Tuple[List[int], List[int]]]",
                                    "docstring": "Perform cross-validation using event windows.\n\nThis method splits the DataFrame based on event occurrences specified in the event_column.\nEach window is determined by a fixed number of events (window_size), allowing the cross-validation\nprocess to account for clustered or bursty events and their implications on data distribution.\n\nArgs:\n    X (pd.DataFrame): The input data that includes an event indicator column.\n    event_column (str): The name of the column that specifies events.\n    window_size (int): The number of events to include in each window.\n    \nReturns:\n    List[Tuple[List[int], List[int]]]: A list of tuples, each containing indices for training and testing sets."
                                },
                                {
                                    "name": "time_windows_cv",
                                    "args": [
                                        "self",
                                        "X",
                                        "time_column",
                                        "window_duration"
                                    ],
                                    "return_type": "List[Tuple[List[int], List[int]]]",
                                    "docstring": "Perform cross-validation using predefined time windows.\n\nThis method splits the DataFrame based on a time column. Each split is created by segmenting the data \ninto intervals determined by the window_duration parameter. This allows for evaluation across \nconsistent time intervals in the data.\n\nArgs:\n    X (pd.DataFrame): The input data containing a time-based column.\n    time_column (str): The name of the column representing time.\n    window_duration: The duration of each time window (e.g., an integer representing days or a pandas offset).\n    \nReturns:\n    List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/cv_temporal.py"
                    }
                ]
            },
            {
                "node": "time-series cross-validation",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Cross-validation Temporal Split/time-series cross-validation",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "CVTemporalSplitter",
                        "extra": {
                            "docstring": "Provides cross-validation splitting strategies for temporal data with various approaches.\n\nThis class implements three methods to generate cross-validation splits that respect the temporal \nstructure of the data. It supports:\n  - Time-series cross-validation: Splitting based on chronological order.\n  - Cross-validation with event windows: Creating splits based on event-based windows.\n  - Cross-validation with time windows: Dividing data into splits based on predefined time intervals.\n\nEach method returns a list of tuples, where each tuple contains two lists:\n  - The first list contains indices for the training set.\n  - The second list contains indices for the testing set.\n  \nMethods:\n    time_series_cv: Generates splits for time-series cross-validation.\n    event_windows_cv: Generates splits based on event windows.\n    time_windows_cv: Generates splits based on time windows.",
                            "methods": [
                                {
                                    "name": "time_series_cv",
                                    "args": [
                                        "self",
                                        "X",
                                        "n_splits"
                                    ],
                                    "return_type": "List[Tuple[List[int], List[int]]]",
                                    "docstring": "Perform time-series cross-validation by splitting data sequentially.\n\nThis method assumes that the input DataFrame X is ordered by time. The function generates\nn_splits pairs of training and testing indices, where each successive split uses later portions \nof the data for testing.\n\nArgs:\n    X (pd.DataFrame): The input data ordered chronologically.\n    n_splits (int): The number of splits to generate. Default is 5.\n    \nReturns:\n    List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices."
                                },
                                {
                                    "name": "event_windows_cv",
                                    "args": [
                                        "self",
                                        "X",
                                        "event_column",
                                        "window_size"
                                    ],
                                    "return_type": "List[Tuple[List[int], List[int]]]",
                                    "docstring": "Perform cross-validation using event windows.\n\nThis method splits the DataFrame based on event occurrences specified in the event_column.\nEach window is determined by a fixed number of events (window_size), allowing the cross-validation\nprocess to account for clustered or bursty events and their implications on data distribution.\n\nArgs:\n    X (pd.DataFrame): The input data that includes an event indicator column.\n    event_column (str): The name of the column that specifies events.\n    window_size (int): The number of events to include in each window.\n    \nReturns:\n    List[Tuple[List[int], List[int]]]: A list of tuples, each containing indices for training and testing sets."
                                },
                                {
                                    "name": "time_windows_cv",
                                    "args": [
                                        "self",
                                        "X",
                                        "time_column",
                                        "window_duration"
                                    ],
                                    "return_type": "List[Tuple[List[int], List[int]]]",
                                    "docstring": "Perform cross-validation using predefined time windows.\n\nThis method splits the DataFrame based on a time column. Each split is created by segmenting the data \ninto intervals determined by the window_duration parameter. This allows for evaluation across \nconsistent time intervals in the data.\n\nArgs:\n    X (pd.DataFrame): The input data containing a time-based column.\n    time_column (str): The name of the column representing time.\n    window_duration: The duration of each time window (e.g., an integer representing days or a pandas offset).\n    \nReturns:\n    List[Tuple[List[int], List[int]]]: A list of tuples, each containing training and testing indices."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/cv_temporal.py"
                    }
                ]
            },
            {
                "node": "Event-based Split",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Event-based Split",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_splitting/event_based.py"
                    }
                ]
            },
            {
                "node": "split by event count",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Event-based Split/split by event count",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "EventBasedSplitter",
                        "extra": {
                            "docstring": "A class that provides various methods for splitting a DataFrame based on event-related criteria.\n\nThis class encapsulates three event-based splitting strategies:\n1. Splitting the dataset based on event count.\n2. Splitting the dataset based on event frequency.\n3. Splitting the dataset based on event types.\n\nEach method takes a DataFrame and relevant parameters to determine how the data is segmented.\n\nMethods:\n    split_by_event_count: Splits the DataFrame based on the count of events.\n    split_by_event_frequency: Splits the DataFrame based on the frequency of events.\n    split_by_event_types: Splits the DataFrame based on event type categorization.",
                            "methods": [
                                {
                                    "name": "split_by_event_count",
                                    "args": [
                                        "self",
                                        "df",
                                        "event_count_column",
                                        "count_threshold"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Splits the DataFrame based on the count of events recorded in a specific column.\n\nThis method divides the input DataFrame into multiple segments based on a threshold value applied \nto the event count column. Different segments may reflect varying event participation quantities.\n\nArgs:\n    df (pd.DataFrame): The input data containing events.\n    event_count_column (str): The column name that holds the event count.\n    count_threshold (int): The threshold value used to determine splitting logic.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary where keys represent split segment identifiers and values \n    are the corresponding DataFrame segments."
                                },
                                {
                                    "name": "split_by_event_frequency",
                                    "args": [
                                        "self",
                                        "df",
                                        "frequency_column",
                                        "frequency_threshold"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Splits the DataFrame based on the frequency of events observed in a given column.\n\nThe method segments the DataFrame into multiple groups based on whether the event frequency \nexceeds or falls below a given threshold. This is useful for categorizing events based on how \noften they occur.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing event frequency information.\n    frequency_column (str): The column name that contains the event frequency data.\n    frequency_threshold (float): The threshold frequency to determine how the dataset is split.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary mapping segment names to their corresponding DataFrame splits."
                                },
                                {
                                    "name": "split_by_event_types",
                                    "args": [
                                        "self",
                                        "df",
                                        "event_type_column"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Splits the DataFrame based on distinct event types as specified in a given column.\n\nThis method categorizes the input data into multiple segments, each corresponding to a unique\nevent type found in the specified column. It facilitates analysis per event category.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing event data.\n    event_type_column (str): The column name that classifies the types of events.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary where each key is an event type and the value is the \n    subset of the DataFrame corresponding to that event type."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/event_based.py"
                    }
                ]
            },
            {
                "node": "split by event frequency",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Event-based Split/split by event frequency",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "EventBasedSplitter",
                        "extra": {
                            "docstring": "A class that provides various methods for splitting a DataFrame based on event-related criteria.\n\nThis class encapsulates three event-based splitting strategies:\n1. Splitting the dataset based on event count.\n2. Splitting the dataset based on event frequency.\n3. Splitting the dataset based on event types.\n\nEach method takes a DataFrame and relevant parameters to determine how the data is segmented.\n\nMethods:\n    split_by_event_count: Splits the DataFrame based on the count of events.\n    split_by_event_frequency: Splits the DataFrame based on the frequency of events.\n    split_by_event_types: Splits the DataFrame based on event type categorization.",
                            "methods": [
                                {
                                    "name": "split_by_event_count",
                                    "args": [
                                        "self",
                                        "df",
                                        "event_count_column",
                                        "count_threshold"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Splits the DataFrame based on the count of events recorded in a specific column.\n\nThis method divides the input DataFrame into multiple segments based on a threshold value applied \nto the event count column. Different segments may reflect varying event participation quantities.\n\nArgs:\n    df (pd.DataFrame): The input data containing events.\n    event_count_column (str): The column name that holds the event count.\n    count_threshold (int): The threshold value used to determine splitting logic.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary where keys represent split segment identifiers and values \n    are the corresponding DataFrame segments."
                                },
                                {
                                    "name": "split_by_event_frequency",
                                    "args": [
                                        "self",
                                        "df",
                                        "frequency_column",
                                        "frequency_threshold"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Splits the DataFrame based on the frequency of events observed in a given column.\n\nThe method segments the DataFrame into multiple groups based on whether the event frequency \nexceeds or falls below a given threshold. This is useful for categorizing events based on how \noften they occur.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing event frequency information.\n    frequency_column (str): The column name that contains the event frequency data.\n    frequency_threshold (float): The threshold frequency to determine how the dataset is split.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary mapping segment names to their corresponding DataFrame splits."
                                },
                                {
                                    "name": "split_by_event_types",
                                    "args": [
                                        "self",
                                        "df",
                                        "event_type_column"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Splits the DataFrame based on distinct event types as specified in a given column.\n\nThis method categorizes the input data into multiple segments, each corresponding to a unique\nevent type found in the specified column. It facilitates analysis per event category.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing event data.\n    event_type_column (str): The column name that classifies the types of events.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary where each key is an event type and the value is the \n    subset of the DataFrame corresponding to that event type."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/event_based.py"
                    }
                ]
            },
            {
                "node": "split by event types",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Event-based Split/split by event types",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "EventBasedSplitter",
                        "extra": {
                            "docstring": "A class that provides various methods for splitting a DataFrame based on event-related criteria.\n\nThis class encapsulates three event-based splitting strategies:\n1. Splitting the dataset based on event count.\n2. Splitting the dataset based on event frequency.\n3. Splitting the dataset based on event types.\n\nEach method takes a DataFrame and relevant parameters to determine how the data is segmented.\n\nMethods:\n    split_by_event_count: Splits the DataFrame based on the count of events.\n    split_by_event_frequency: Splits the DataFrame based on the frequency of events.\n    split_by_event_types: Splits the DataFrame based on event type categorization.",
                            "methods": [
                                {
                                    "name": "split_by_event_count",
                                    "args": [
                                        "self",
                                        "df",
                                        "event_count_column",
                                        "count_threshold"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Splits the DataFrame based on the count of events recorded in a specific column.\n\nThis method divides the input DataFrame into multiple segments based on a threshold value applied \nto the event count column. Different segments may reflect varying event participation quantities.\n\nArgs:\n    df (pd.DataFrame): The input data containing events.\n    event_count_column (str): The column name that holds the event count.\n    count_threshold (int): The threshold value used to determine splitting logic.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary where keys represent split segment identifiers and values \n    are the corresponding DataFrame segments."
                                },
                                {
                                    "name": "split_by_event_frequency",
                                    "args": [
                                        "self",
                                        "df",
                                        "frequency_column",
                                        "frequency_threshold"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Splits the DataFrame based on the frequency of events observed in a given column.\n\nThe method segments the DataFrame into multiple groups based on whether the event frequency \nexceeds or falls below a given threshold. This is useful for categorizing events based on how \noften they occur.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing event frequency information.\n    frequency_column (str): The column name that contains the event frequency data.\n    frequency_threshold (float): The threshold frequency to determine how the dataset is split.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary mapping segment names to their corresponding DataFrame splits."
                                },
                                {
                                    "name": "split_by_event_types",
                                    "args": [
                                        "self",
                                        "df",
                                        "event_type_column"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Splits the DataFrame based on distinct event types as specified in a given column.\n\nThis method categorizes the input data into multiple segments, each corresponding to a unique\nevent type found in the specified column. It facilitates analysis per event category.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing event data.\n    event_type_column (str): The column name that classifies the types of events.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary where each key is an event type and the value is the \n    subset of the DataFrame corresponding to that event type."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/event_based.py"
                    }
                ]
            },
            {
                "node": "Time-based Split",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Time-based Split",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_splitting/time_based.py"
                    }
                ]
            },
            {
                "node": "split by day",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Time-based Split/split by day",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "TimeBasedSplitter",
                        "extra": {
                            "docstring": "A splitter for performing time-based data splits according to different temporal granularities.\n\nThis class encapsulates multiple strategies to split a pandas DataFrame based on time attributes.\nIt provides methods to split the data by time zones, month, hour, day, and year.\n\nEach method expects a time-indexed pandas DataFrame and a specific column name holding the relevant time information.\nThe returned structure is a dictionary where the keys represent the extracted time units and the values are the corresponding\nsegments of the DataFrame.\n\nMethods:\n    split_by_timezones(df: pd.DataFrame, timezone_column: str) -> dict:\n        Splits data based on time zone values.\n    split_by_month(df: pd.DataFrame, month_column: str) -> dict:\n        Splits data based on the month extracted from a datetime column.\n    split_by_hour(df: pd.DataFrame, hour_column: str) -> dict:\n        Splits data based on the hour extracted from a datetime column.\n    split_by_day(df: pd.DataFrame, day_column: str) -> dict:\n        Splits data based on the day extracted from a datetime column.\n    split_by_year(df: pd.DataFrame, year_column: str) -> dict:\n        Splits data based on the year extracted from a datetime column.\n\nNote:\n    - No splitting logic is implemented in this interface definition.\n    - The client is responsible for verifying that the DataFrame contains the required columns with appropriate data formats.",
                            "methods": [
                                {
                                    "name": "split_by_timezones",
                                    "args": [
                                        "self",
                                        "df",
                                        "timezone_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame based on time zone information provided in the specified column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    timezone_column (str): The name of the column with time zone data.\n\nReturns:\n    dict: A dictionary where keys are time zone identifiers and values are the corresponding DataFrame segments."
                                },
                                {
                                    "name": "split_by_month",
                                    "args": [
                                        "self",
                                        "df",
                                        "month_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the month extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    month_column (str): The name of the column from which the month information will be derived.\n\nReturns:\n    dict: A dictionary with months as keys and DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_hour",
                                    "args": [
                                        "self",
                                        "df",
                                        "hour_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the hour extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    hour_column (str): The name of the column from which the hour information will be derived.\n\nReturns:\n    dict: A dictionary with hours as keys and corresponding DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_day",
                                    "args": [
                                        "self",
                                        "df",
                                        "day_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the day extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    day_column (str): The name of the column from which day information will be derived.\n\nReturns:\n    dict: A dictionary with days as keys and corresponding DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_year",
                                    "args": [
                                        "self",
                                        "df",
                                        "year_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the year extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    year_column (str): The name of the column from which year information will be derived.\n\nReturns:\n    dict: A dictionary with years as keys and corresponding DataFrame segments as values."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/time_based.py"
                    }
                ]
            },
            {
                "node": "split by hour",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Time-based Split/split by hour",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "TimeBasedSplitter",
                        "extra": {
                            "docstring": "A splitter for performing time-based data splits according to different temporal granularities.\n\nThis class encapsulates multiple strategies to split a pandas DataFrame based on time attributes.\nIt provides methods to split the data by time zones, month, hour, day, and year.\n\nEach method expects a time-indexed pandas DataFrame and a specific column name holding the relevant time information.\nThe returned structure is a dictionary where the keys represent the extracted time units and the values are the corresponding\nsegments of the DataFrame.\n\nMethods:\n    split_by_timezones(df: pd.DataFrame, timezone_column: str) -> dict:\n        Splits data based on time zone values.\n    split_by_month(df: pd.DataFrame, month_column: str) -> dict:\n        Splits data based on the month extracted from a datetime column.\n    split_by_hour(df: pd.DataFrame, hour_column: str) -> dict:\n        Splits data based on the hour extracted from a datetime column.\n    split_by_day(df: pd.DataFrame, day_column: str) -> dict:\n        Splits data based on the day extracted from a datetime column.\n    split_by_year(df: pd.DataFrame, year_column: str) -> dict:\n        Splits data based on the year extracted from a datetime column.\n\nNote:\n    - No splitting logic is implemented in this interface definition.\n    - The client is responsible for verifying that the DataFrame contains the required columns with appropriate data formats.",
                            "methods": [
                                {
                                    "name": "split_by_timezones",
                                    "args": [
                                        "self",
                                        "df",
                                        "timezone_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame based on time zone information provided in the specified column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    timezone_column (str): The name of the column with time zone data.\n\nReturns:\n    dict: A dictionary where keys are time zone identifiers and values are the corresponding DataFrame segments."
                                },
                                {
                                    "name": "split_by_month",
                                    "args": [
                                        "self",
                                        "df",
                                        "month_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the month extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    month_column (str): The name of the column from which the month information will be derived.\n\nReturns:\n    dict: A dictionary with months as keys and DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_hour",
                                    "args": [
                                        "self",
                                        "df",
                                        "hour_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the hour extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    hour_column (str): The name of the column from which the hour information will be derived.\n\nReturns:\n    dict: A dictionary with hours as keys and corresponding DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_day",
                                    "args": [
                                        "self",
                                        "df",
                                        "day_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the day extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    day_column (str): The name of the column from which day information will be derived.\n\nReturns:\n    dict: A dictionary with days as keys and corresponding DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_year",
                                    "args": [
                                        "self",
                                        "df",
                                        "year_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the year extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    year_column (str): The name of the column from which year information will be derived.\n\nReturns:\n    dict: A dictionary with years as keys and corresponding DataFrame segments as values."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/time_based.py"
                    }
                ]
            },
            {
                "node": "split by month",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Time-based Split/split by month",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "TimeBasedSplitter",
                        "extra": {
                            "docstring": "A splitter for performing time-based data splits according to different temporal granularities.\n\nThis class encapsulates multiple strategies to split a pandas DataFrame based on time attributes.\nIt provides methods to split the data by time zones, month, hour, day, and year.\n\nEach method expects a time-indexed pandas DataFrame and a specific column name holding the relevant time information.\nThe returned structure is a dictionary where the keys represent the extracted time units and the values are the corresponding\nsegments of the DataFrame.\n\nMethods:\n    split_by_timezones(df: pd.DataFrame, timezone_column: str) -> dict:\n        Splits data based on time zone values.\n    split_by_month(df: pd.DataFrame, month_column: str) -> dict:\n        Splits data based on the month extracted from a datetime column.\n    split_by_hour(df: pd.DataFrame, hour_column: str) -> dict:\n        Splits data based on the hour extracted from a datetime column.\n    split_by_day(df: pd.DataFrame, day_column: str) -> dict:\n        Splits data based on the day extracted from a datetime column.\n    split_by_year(df: pd.DataFrame, year_column: str) -> dict:\n        Splits data based on the year extracted from a datetime column.\n\nNote:\n    - No splitting logic is implemented in this interface definition.\n    - The client is responsible for verifying that the DataFrame contains the required columns with appropriate data formats.",
                            "methods": [
                                {
                                    "name": "split_by_timezones",
                                    "args": [
                                        "self",
                                        "df",
                                        "timezone_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame based on time zone information provided in the specified column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    timezone_column (str): The name of the column with time zone data.\n\nReturns:\n    dict: A dictionary where keys are time zone identifiers and values are the corresponding DataFrame segments."
                                },
                                {
                                    "name": "split_by_month",
                                    "args": [
                                        "self",
                                        "df",
                                        "month_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the month extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    month_column (str): The name of the column from which the month information will be derived.\n\nReturns:\n    dict: A dictionary with months as keys and DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_hour",
                                    "args": [
                                        "self",
                                        "df",
                                        "hour_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the hour extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    hour_column (str): The name of the column from which the hour information will be derived.\n\nReturns:\n    dict: A dictionary with hours as keys and corresponding DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_day",
                                    "args": [
                                        "self",
                                        "df",
                                        "day_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the day extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    day_column (str): The name of the column from which day information will be derived.\n\nReturns:\n    dict: A dictionary with days as keys and corresponding DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_year",
                                    "args": [
                                        "self",
                                        "df",
                                        "year_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the year extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    year_column (str): The name of the column from which year information will be derived.\n\nReturns:\n    dict: A dictionary with years as keys and corresponding DataFrame segments as values."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/time_based.py"
                    }
                ]
            },
            {
                "node": "split by time zones",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Time-based Split/split by time zones",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "TimeBasedSplitter",
                        "extra": {
                            "docstring": "A splitter for performing time-based data splits according to different temporal granularities.\n\nThis class encapsulates multiple strategies to split a pandas DataFrame based on time attributes.\nIt provides methods to split the data by time zones, month, hour, day, and year.\n\nEach method expects a time-indexed pandas DataFrame and a specific column name holding the relevant time information.\nThe returned structure is a dictionary where the keys represent the extracted time units and the values are the corresponding\nsegments of the DataFrame.\n\nMethods:\n    split_by_timezones(df: pd.DataFrame, timezone_column: str) -> dict:\n        Splits data based on time zone values.\n    split_by_month(df: pd.DataFrame, month_column: str) -> dict:\n        Splits data based on the month extracted from a datetime column.\n    split_by_hour(df: pd.DataFrame, hour_column: str) -> dict:\n        Splits data based on the hour extracted from a datetime column.\n    split_by_day(df: pd.DataFrame, day_column: str) -> dict:\n        Splits data based on the day extracted from a datetime column.\n    split_by_year(df: pd.DataFrame, year_column: str) -> dict:\n        Splits data based on the year extracted from a datetime column.\n\nNote:\n    - No splitting logic is implemented in this interface definition.\n    - The client is responsible for verifying that the DataFrame contains the required columns with appropriate data formats.",
                            "methods": [
                                {
                                    "name": "split_by_timezones",
                                    "args": [
                                        "self",
                                        "df",
                                        "timezone_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame based on time zone information provided in the specified column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    timezone_column (str): The name of the column with time zone data.\n\nReturns:\n    dict: A dictionary where keys are time zone identifiers and values are the corresponding DataFrame segments."
                                },
                                {
                                    "name": "split_by_month",
                                    "args": [
                                        "self",
                                        "df",
                                        "month_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the month extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    month_column (str): The name of the column from which the month information will be derived.\n\nReturns:\n    dict: A dictionary with months as keys and DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_hour",
                                    "args": [
                                        "self",
                                        "df",
                                        "hour_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the hour extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    hour_column (str): The name of the column from which the hour information will be derived.\n\nReturns:\n    dict: A dictionary with hours as keys and corresponding DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_day",
                                    "args": [
                                        "self",
                                        "df",
                                        "day_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the day extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    day_column (str): The name of the column from which day information will be derived.\n\nReturns:\n    dict: A dictionary with days as keys and corresponding DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_year",
                                    "args": [
                                        "self",
                                        "df",
                                        "year_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the year extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    year_column (str): The name of the column from which year information will be derived.\n\nReturns:\n    dict: A dictionary with years as keys and corresponding DataFrame segments as values."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/time_based.py"
                    }
                ]
            },
            {
                "node": "split by year",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Time-based Split/split by year",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "TimeBasedSplitter",
                        "extra": {
                            "docstring": "A splitter for performing time-based data splits according to different temporal granularities.\n\nThis class encapsulates multiple strategies to split a pandas DataFrame based on time attributes.\nIt provides methods to split the data by time zones, month, hour, day, and year.\n\nEach method expects a time-indexed pandas DataFrame and a specific column name holding the relevant time information.\nThe returned structure is a dictionary where the keys represent the extracted time units and the values are the corresponding\nsegments of the DataFrame.\n\nMethods:\n    split_by_timezones(df: pd.DataFrame, timezone_column: str) -> dict:\n        Splits data based on time zone values.\n    split_by_month(df: pd.DataFrame, month_column: str) -> dict:\n        Splits data based on the month extracted from a datetime column.\n    split_by_hour(df: pd.DataFrame, hour_column: str) -> dict:\n        Splits data based on the hour extracted from a datetime column.\n    split_by_day(df: pd.DataFrame, day_column: str) -> dict:\n        Splits data based on the day extracted from a datetime column.\n    split_by_year(df: pd.DataFrame, year_column: str) -> dict:\n        Splits data based on the year extracted from a datetime column.\n\nNote:\n    - No splitting logic is implemented in this interface definition.\n    - The client is responsible for verifying that the DataFrame contains the required columns with appropriate data formats.",
                            "methods": [
                                {
                                    "name": "split_by_timezones",
                                    "args": [
                                        "self",
                                        "df",
                                        "timezone_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame based on time zone information provided in the specified column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    timezone_column (str): The name of the column with time zone data.\n\nReturns:\n    dict: A dictionary where keys are time zone identifiers and values are the corresponding DataFrame segments."
                                },
                                {
                                    "name": "split_by_month",
                                    "args": [
                                        "self",
                                        "df",
                                        "month_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the month extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    month_column (str): The name of the column from which the month information will be derived.\n\nReturns:\n    dict: A dictionary with months as keys and DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_hour",
                                    "args": [
                                        "self",
                                        "df",
                                        "hour_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the hour extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    hour_column (str): The name of the column from which the hour information will be derived.\n\nReturns:\n    dict: A dictionary with hours as keys and corresponding DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_day",
                                    "args": [
                                        "self",
                                        "df",
                                        "day_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the day extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    day_column (str): The name of the column from which day information will be derived.\n\nReturns:\n    dict: A dictionary with days as keys and corresponding DataFrame segments as values."
                                },
                                {
                                    "name": "split_by_year",
                                    "args": [
                                        "self",
                                        "df",
                                        "year_column"
                                    ],
                                    "return_type": "dict",
                                    "docstring": "Split the DataFrame into groups based on the year extracted from a datetime column.\n\nArgs:\n    df (pd.DataFrame): The pandas DataFrame containing time-based data.\n    year_column (str): The name of the column from which year information will be derived.\n\nReturns:\n    dict: A dictionary with years as keys and corresponding DataFrame segments as values."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/time_based.py"
                    }
                ]
            },
            {
                "node": "Train-Test Temporal Split",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Train-Test Temporal Split",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_splitting/train_test_temporal.py"
                    }
                ]
            },
            {
                "node": "split by event intervals",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Train-Test Temporal Split/split by event intervals",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "TrainTestTemporalSplitter",
                        "extra": {
                            "docstring": "Provides train-test splitting strategies based on temporal criteria.\n\nThis interface allows splitting a dataset into training and testing portions based on three distinct\ntemporal features:\n  1. split by test period: Splitting based on a designated test period (e.g., a specific date or time span).\n  2. split by time intervals: Splitting by uniform or specified time intervals.\n  3. split by event intervals: Splitting based on event-triggered intervals in the data.\n\nMethods in this class should accept the input data (typically in a pandas DataFrame) and splitting parameters,\nand return a dictionary that contains at least two keys (commonly 'train' and 'test') pointing to the respective\nDataFrame splits.\n\nAssumptions:\n  - The input DataFrame contains a temporal index or column that can be leveraged for splitting.\n  - The split parameters are pre-validated and provided in a format understandable by each method.\n  - This interface is stateless; any stateful configuration should be managed externally.\n\nUsage:\n  splitter = TrainTestTemporalSplitter()\n  splits = splitter.split_by_test_period(df, test_period=\"2021-01-01\")",
                            "methods": [
                                {
                                    "name": "split_by_test_period",
                                    "args": [
                                        "self",
                                        "df",
                                        "test_period"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Split the input DataFrame into training and testing sets based on a specified test period.\n\nThis method uses a user-defined test period (which could be a specific date or a time range) to\ndetermine the boundary between training and testing data.\n\nArgs:\n    df (pd.DataFrame): The input dataset containing temporal data.\n    test_period (Any): The criterion defining the test period. This might be a date string, a tuple of dates,\n                       or any object representing the test period boundary.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n    the split DataFrames.\n\nEdge Cases:\n    - If the test period does not match any records, the function should return an empty test set.\n    - It is assumed that the DataFrame's temporal column is pre-processed and valid."
                                },
                                {
                                    "name": "split_by_time_intervals",
                                    "args": [
                                        "self",
                                        "df",
                                        "interval_params"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Split the input DataFrame into training and testing sets based on specified time intervals.\n\nThis method creates splits by dividing the dataset into segments defined by uniform or custom time intervals.\n\nArgs:\n    df (pd.DataFrame): The input dataset containing time-related data.\n    interval_params (Dict[str, Any]): Parameters defining the time interval splits. For example, it may include\n                                      the interval duration, the start time, and the end time.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n    the resulting DataFrame splits based on time intervals.\n\nConstraints:\n    - It is assumed that the interval parameters are provided correctly.\n    - The DataFrame must contain a time-index or a dedicated time column."
                                },
                                {
                                    "name": "split_by_event_intervals",
                                    "args": [
                                        "self",
                                        "df",
                                        "event_intervals"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Split the input DataFrame into training and testing sets based on event-driven intervals.\n\nThis method leverages events annotated in the DataFrame (or computed externally) to decide the split points,\nwhere the events signify transitions between training and testing periods.\n\nArgs:\n    df (pd.DataFrame): The input dataset that includes event-related data markers.\n    event_intervals (Dict[str, Any]): A dictionary containing parameters that define the event intervals. These\n                                      parameters might dictate how many events mark the split or conditions based\n                                      on event types.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' containing the respective splits determined \n    by event intervals.\n\nEdge Cases:\n    - The method should correctly handle cases where events are sparse or overly frequent.\n    - It assumes that the DataFrame includes valid and pre-processed event markers."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/train_test_temporal.py"
                    }
                ]
            },
            {
                "node": "split by test period",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Train-Test Temporal Split/split by test period",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "TrainTestTemporalSplitter",
                        "extra": {
                            "docstring": "Provides train-test splitting strategies based on temporal criteria.\n\nThis interface allows splitting a dataset into training and testing portions based on three distinct\ntemporal features:\n  1. split by test period: Splitting based on a designated test period (e.g., a specific date or time span).\n  2. split by time intervals: Splitting by uniform or specified time intervals.\n  3. split by event intervals: Splitting based on event-triggered intervals in the data.\n\nMethods in this class should accept the input data (typically in a pandas DataFrame) and splitting parameters,\nand return a dictionary that contains at least two keys (commonly 'train' and 'test') pointing to the respective\nDataFrame splits.\n\nAssumptions:\n  - The input DataFrame contains a temporal index or column that can be leveraged for splitting.\n  - The split parameters are pre-validated and provided in a format understandable by each method.\n  - This interface is stateless; any stateful configuration should be managed externally.\n\nUsage:\n  splitter = TrainTestTemporalSplitter()\n  splits = splitter.split_by_test_period(df, test_period=\"2021-01-01\")",
                            "methods": [
                                {
                                    "name": "split_by_test_period",
                                    "args": [
                                        "self",
                                        "df",
                                        "test_period"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Split the input DataFrame into training and testing sets based on a specified test period.\n\nThis method uses a user-defined test period (which could be a specific date or a time range) to\ndetermine the boundary between training and testing data.\n\nArgs:\n    df (pd.DataFrame): The input dataset containing temporal data.\n    test_period (Any): The criterion defining the test period. This might be a date string, a tuple of dates,\n                       or any object representing the test period boundary.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n    the split DataFrames.\n\nEdge Cases:\n    - If the test period does not match any records, the function should return an empty test set.\n    - It is assumed that the DataFrame's temporal column is pre-processed and valid."
                                },
                                {
                                    "name": "split_by_time_intervals",
                                    "args": [
                                        "self",
                                        "df",
                                        "interval_params"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Split the input DataFrame into training and testing sets based on specified time intervals.\n\nThis method creates splits by dividing the dataset into segments defined by uniform or custom time intervals.\n\nArgs:\n    df (pd.DataFrame): The input dataset containing time-related data.\n    interval_params (Dict[str, Any]): Parameters defining the time interval splits. For example, it may include\n                                      the interval duration, the start time, and the end time.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n    the resulting DataFrame splits based on time intervals.\n\nConstraints:\n    - It is assumed that the interval parameters are provided correctly.\n    - The DataFrame must contain a time-index or a dedicated time column."
                                },
                                {
                                    "name": "split_by_event_intervals",
                                    "args": [
                                        "self",
                                        "df",
                                        "event_intervals"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Split the input DataFrame into training and testing sets based on event-driven intervals.\n\nThis method leverages events annotated in the DataFrame (or computed externally) to decide the split points,\nwhere the events signify transitions between training and testing periods.\n\nArgs:\n    df (pd.DataFrame): The input dataset that includes event-related data markers.\n    event_intervals (Dict[str, Any]): A dictionary containing parameters that define the event intervals. These\n                                      parameters might dictate how many events mark the split or conditions based\n                                      on event types.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' containing the respective splits determined \n    by event intervals.\n\nEdge Cases:\n    - The method should correctly handle cases where events are sparse or overly frequent.\n    - It assumes that the DataFrame includes valid and pre-processed event markers."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/train_test_temporal.py"
                    }
                ]
            },
            {
                "node": "split by time intervals",
                "feature_path": "Data Engineering/Data Splitting/Temporal Split/Train-Test Temporal Split/split by time intervals",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "TrainTestTemporalSplitter",
                        "extra": {
                            "docstring": "Provides train-test splitting strategies based on temporal criteria.\n\nThis interface allows splitting a dataset into training and testing portions based on three distinct\ntemporal features:\n  1. split by test period: Splitting based on a designated test period (e.g., a specific date or time span).\n  2. split by time intervals: Splitting by uniform or specified time intervals.\n  3. split by event intervals: Splitting based on event-triggered intervals in the data.\n\nMethods in this class should accept the input data (typically in a pandas DataFrame) and splitting parameters,\nand return a dictionary that contains at least two keys (commonly 'train' and 'test') pointing to the respective\nDataFrame splits.\n\nAssumptions:\n  - The input DataFrame contains a temporal index or column that can be leveraged for splitting.\n  - The split parameters are pre-validated and provided in a format understandable by each method.\n  - This interface is stateless; any stateful configuration should be managed externally.\n\nUsage:\n  splitter = TrainTestTemporalSplitter()\n  splits = splitter.split_by_test_period(df, test_period=\"2021-01-01\")",
                            "methods": [
                                {
                                    "name": "split_by_test_period",
                                    "args": [
                                        "self",
                                        "df",
                                        "test_period"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Split the input DataFrame into training and testing sets based on a specified test period.\n\nThis method uses a user-defined test period (which could be a specific date or a time range) to\ndetermine the boundary between training and testing data.\n\nArgs:\n    df (pd.DataFrame): The input dataset containing temporal data.\n    test_period (Any): The criterion defining the test period. This might be a date string, a tuple of dates,\n                       or any object representing the test period boundary.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n    the split DataFrames.\n\nEdge Cases:\n    - If the test period does not match any records, the function should return an empty test set.\n    - It is assumed that the DataFrame's temporal column is pre-processed and valid."
                                },
                                {
                                    "name": "split_by_time_intervals",
                                    "args": [
                                        "self",
                                        "df",
                                        "interval_params"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Split the input DataFrame into training and testing sets based on specified time intervals.\n\nThis method creates splits by dividing the dataset into segments defined by uniform or custom time intervals.\n\nArgs:\n    df (pd.DataFrame): The input dataset containing time-related data.\n    interval_params (Dict[str, Any]): Parameters defining the time interval splits. For example, it may include\n                                      the interval duration, the start time, and the end time.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' where the corresponding values are \n    the resulting DataFrame splits based on time intervals.\n\nConstraints:\n    - It is assumed that the interval parameters are provided correctly.\n    - The DataFrame must contain a time-index or a dedicated time column."
                                },
                                {
                                    "name": "split_by_event_intervals",
                                    "args": [
                                        "self",
                                        "df",
                                        "event_intervals"
                                    ],
                                    "return_type": "Dict[str, pd.DataFrame]",
                                    "docstring": "Split the input DataFrame into training and testing sets based on event-driven intervals.\n\nThis method leverages events annotated in the DataFrame (or computed externally) to decide the split points,\nwhere the events signify transitions between training and testing periods.\n\nArgs:\n    df (pd.DataFrame): The input dataset that includes event-related data markers.\n    event_intervals (Dict[str, Any]): A dictionary containing parameters that define the event intervals. These\n                                      parameters might dictate how many events mark the split or conditions based\n                                      on event types.\n\nReturns:\n    Dict[str, pd.DataFrame]: A dictionary with keys 'train' and 'test' containing the respective splits determined \n    by event intervals.\n\nEdge Cases:\n    - The method should correctly handle cases where events are sparse or overly frequent.\n    - It assumes that the DataFrame includes valid and pre-processed event markers."
                                }
                            ]
                        },
                        "path": "src/data_engineering/data_splitting/train_test_temporal.py"
                    }
                ]
            },
            {
                "node": "Data Validation",
                "feature_path": "Data Engineering/Data Validation",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_validation"
                    }
                ]
            },
            {
                "node": "Cross-validation",
                "feature_path": "Data Engineering/Data Validation/Cross-validation",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/data_engineering/data_validation"
                    }
                ]
            },
            {
                "node": "Details",
                "feature_path": "Data Engineering/Data Validation/Cross-validation/Details",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/data_engineering/data_validation/cross_validation.py"
                    }
                ]
            },
            {
                "node": "stratified cross-validation",
                "feature_path": "Data Engineering/Data Validation/Cross-validation/Details/stratified cross-validation",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "stratified_cross_validation",
                        "extra": {
                            "args": [
                                "X",
                                "y",
                                "n_splits",
                                "random_state"
                            ],
                            "return_type": "List[Tuple[np.ndarray, np.ndarray]]",
                            "docstring": "Perform stratified cross-validation splitting on the given dataset.\n\nThis function splits the dataset into training and testing folds ensuring that each fold maintains approximately \nthe same percentage of samples from each target class. It is particularly useful in scenarios involving imbalanced\nclassification where preserving class distribution during evaluation is crucial.\n\nArgs:\n    X (pd.DataFrame): The input features dataset.\n    y (pd.Series): The target labels corresponding to the data in X.\n    n_splits (int): The number of folds to split the data into. Must be at least 2. Default is 5.\n    random_state (Optional[int]): A seed for the random number generator to ensure reproducibility. Default is None.\n\nReturns:\n    List[Tuple[np.ndarray, np.ndarray]]: A list where each element is a tuple containing two numpy arrays:\n        - The first array represents the indices for the training set.\n        - The second array represents the indices for the testing set.\n\nEdge Cases and Constraints:\n    - n_splits must be at least 2, otherwise valid splits cannot be produced.\n    - If the distribution of classes in y is such that any class does not have enough samples to support the specified n_splits, \n      the function's behavior is undefined and should either raise an error or be handled by pre-validation.\n    - It is assumed that the order of samples in X and y is aligned.\n\nAssumptions:\n    - Input X and y are non-empty and correctly aligned.\n    - The function does not perform the actual splitting logic; it defines the interface to be implemented."
                        },
                        "path": "src/data_engineering/data_validation/cross_validation.py"
                    }
                ]
            }
        ],
        "Advanced Modeling Techniques": [
            {
                "node": "Model Evaluation",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/advanced_modeling/model_evaluation"
                    }
                ]
            },
            {
                "node": "Comparative Evaluation",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/advanced_modeling/model_evaluation"
                    }
                ]
            },
            {
                "node": "Precision-Recall Analysis",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/Precision-Recall Analysis",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/advanced_modeling/model_evaluation/precision_recall.py"
                    }
                ]
            },
            {
                "node": "plot precision-recall curves",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/Precision-Recall Analysis/plot precision-recall curves",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "plot_precision_recall_curve",
                        "extra": {
                            "args": [
                                "precisions",
                                "recalls",
                                "average_precision",
                                "title"
                            ],
                            "return_type": "None",
                            "docstring": "Plot a precision-recall curve given the precision and recall values, and optionally display the average precision score.\n\nThis function takes as input the computed precision and recall values, and generates a plot of the\nprecision-recall curve. The plot can include an annotation for average precision if provided. It is used\nas part of the model evaluation process to visually assess the trade-off between precision and recall\nacross different threshold levels.\n\nArgs:\n    precisions (np.ndarray): An array of precision values calculated for different threshold values.\n    recalls (np.ndarray): An array of recall values corresponding to the precision values.\n    average_precision (Optional[float]): An optional scalar representing the average precision score.\n    title (str): Title for the plot. Defaults to \"Precision-Recall Curve\".\n\nReturns:\n    None: This function does not return any value; it simply generates a plot.\n\nEdge Cases:\n    - The lengths of 'precisions' and 'recalls' must be equal; otherwise, the plot cannot be generated.\n    - If the arrays are empty, the function will not produce a meaningful plot.\n    - It is assumed that 'precisions' and 'recalls' are properly precomputed and correspond to sorted threshold values."
                        },
                        "path": "src/advanced_modeling/model_evaluation/precision_recall.py"
                    }
                ]
            },
            {
                "node": "ROC Analysis",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/ROC Analysis",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/advanced_modeling/model_evaluation/roc_analysis.py"
                    }
                ]
            },
            {
                "node": "micro-average roc",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/ROC Analysis/micro-average roc",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "compute_micro_average_roc",
                        "extra": {
                            "args": [
                                "y_true",
                                "y_score"
                            ],
                            "return_type": "dict",
                            "docstring": "Compute the micro-average ROC curve for multi-label or multi-class classification.\n\nThis function aggregates the performance across all classes to generate a single ROC curve,\nwhich is particularly useful when dealing with imbalanced or multi-label datasets.\n\nArgs:\n    y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix.\n    y_score (np.ndarray): Array of probability estimates or decision scores for each class.\n\nReturns:\n    dict: A dictionary containing 'fpr' (np.ndarray), 'tpr' (np.ndarray), and 'thresholds' (np.ndarray)\n          representing the micro-average ROC curve metrics.\n\nEdge Cases:\n    - If the arrays are mismatched in dimensions, it is assumed that validation occurs externally."
                        },
                        "path": "src/advanced_modeling/model_evaluation/roc_analysis.py"
                    }
                ]
            },
            {
                "node": "multi-class roc",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/ROC Analysis/multi-class roc",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "compute_multiclass_roc",
                        "extra": {
                            "args": [
                                "y_true",
                                "y_score",
                                "classes"
                            ],
                            "return_type": "dict",
                            "docstring": "Compute ROC metrics for multi-class classification.\n\nThis function calculates the ROC curve for each class in a multi-class classification setting.\nIt returns false positive rates, true positive rates, and corresponding thresholds for each class.\n\nArgs:\n    y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix indicating true classes.\n    y_score (np.ndarray): Array of probability estimates or decision scores from the classifier.\n    classes (list): List of class identifiers corresponding to columns in y_score.\n\nReturns:\n    dict: A dictionary where each key is a class identifier and each value is another dictionary\n          containing 'fpr' (np.ndarray), 'tpr' (np.ndarray), and 'thresholds' (np.ndarray).\n\nNotes:\n    - It is assumed that y_true and y_score have matching dimensions and ordering.\n    - The function does not handle any plotting or visualization."
                        },
                        "path": "src/advanced_modeling/model_evaluation/roc_analysis.py"
                    }
                ]
            },
            {
                "node": "precision-recall curve",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/ROC Analysis/precision-recall curve",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "compute_precision_recall_curve",
                        "extra": {
                            "args": [
                                "y_true",
                                "y_score",
                                "average"
                            ],
                            "return_type": "dict",
                            "docstring": "Compute the precision-recall curve for model evaluation.\n\nThis function calculates precision and recall at various threshold settings, aiding in the analysis\nof the trade-off between precision and recall in classification tasks.\n\nArgs:\n    y_true (np.ndarray): Array of true binary labels or a one-hot encoded matrix for multi-class problems.\n    y_score (np.ndarray): Array of predicted probabilities or decision scores.\n    average (str, optional): Specifies the averaging method ('binary', 'macro', 'micro'). Defaults to 'binary'.\n\nReturns:\n    dict: A dictionary containing arrays for 'precision', 'recall', and 'thresholds' (each as np.ndarray).\n\nEdge Cases:\n    - The function assumes that y_true and y_score are properly formatted and that the 'average'\n      parameter is appropriately set for the task."
                        },
                        "path": "src/advanced_modeling/model_evaluation/roc_analysis.py"
                    }
                ]
            },
            {
                "node": "roc curve plotting",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/ROC Analysis/roc curve plotting",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ROCCurveVisualizer",
                        "extra": {
                            "docstring": "Provides functionalities for visualizing ROC curves along with threshold selection and AUC computation.\n\nThis class encapsulates methods to plot ROC curves, select optimal thresholds based on metrics,\nand compute the Area Under the Curve (AUC) from the ROC data. This combined interface supports\ninteractive and analytical evaluation of classifier performance.\n\nMethods:\n    plot_roc_curve:\n        Generate a plot of the ROC curve given arrays of false positive and true positive rates.\n    select_threshold:\n        Determine the optimal threshold value using a metric (e.g., Youden's index).\n    compute_auc:\n        Calculate the Area Under the ROC Curve (AUC) from input ROC data.\n\nUsage:\n    Instantiate the class and invoke its methods with appropriate numpy arrays to perform visualization\n    and analysis of ROC performance.",
                            "methods": [
                                {
                                    "name": "plot_roc_curve",
                                    "args": [
                                        "self",
                                        "fpr",
                                        "tpr"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Plot the ROC curve using false positive rates and true positive rates.\n\nArgs:\n    fpr (np.ndarray): Array of false positive rates.\n    tpr (np.ndarray): Array of true positive rates.\n\nReturns:\n    None\n\nEdge Cases:\n    - It is assumed that fpr and tpr are of equal length. Inconsistent array lengths must be handled externally."
                                },
                                {
                                    "name": "select_threshold",
                                    "args": [
                                        "self",
                                        "thresholds",
                                        "metric_values"
                                    ],
                                    "return_type": "float",
                                    "docstring": "Select the optimal threshold based on a given metric (e.g., maximizing Youden's index).\n\nArgs:\n    thresholds (np.ndarray): Array of threshold values corresponding to the ROC points.\n    metric_values (np.ndarray): Array of computed metric values at each threshold.\n\nReturns:\n    float: The selected optimal threshold value.\n\nNotes:\n    - In cases where multiple thresholds achieve the same optimal metric value, the first occurrence is returned."
                                },
                                {
                                    "name": "compute_auc",
                                    "args": [
                                        "self",
                                        "fpr",
                                        "tpr"
                                    ],
                                    "return_type": "float",
                                    "docstring": "Compute the Area Under the Curve (AUC) for the provided ROC data.\n\nArgs:\n    fpr (np.ndarray): Array of false positive rates.\n    tpr (np.ndarray): Array of true positive rates.\n\nReturns:\n    float: The computed AUC value.\n\nEdge Cases:\n    - The method assumes that the input arrays are non-empty and of equal length."
                                }
                            ]
                        },
                        "path": "src/advanced_modeling/model_evaluation/roc_analysis.py"
                    }
                ]
            },
            {
                "node": "roc curve with auc",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/ROC Analysis/roc curve with auc",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ROCCurveVisualizer",
                        "extra": {
                            "docstring": "Provides functionalities for visualizing ROC curves along with threshold selection and AUC computation.\n\nThis class encapsulates methods to plot ROC curves, select optimal thresholds based on metrics,\nand compute the Area Under the Curve (AUC) from the ROC data. This combined interface supports\ninteractive and analytical evaluation of classifier performance.\n\nMethods:\n    plot_roc_curve:\n        Generate a plot of the ROC curve given arrays of false positive and true positive rates.\n    select_threshold:\n        Determine the optimal threshold value using a metric (e.g., Youden's index).\n    compute_auc:\n        Calculate the Area Under the ROC Curve (AUC) from input ROC data.\n\nUsage:\n    Instantiate the class and invoke its methods with appropriate numpy arrays to perform visualization\n    and analysis of ROC performance.",
                            "methods": [
                                {
                                    "name": "plot_roc_curve",
                                    "args": [
                                        "self",
                                        "fpr",
                                        "tpr"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Plot the ROC curve using false positive rates and true positive rates.\n\nArgs:\n    fpr (np.ndarray): Array of false positive rates.\n    tpr (np.ndarray): Array of true positive rates.\n\nReturns:\n    None\n\nEdge Cases:\n    - It is assumed that fpr and tpr are of equal length. Inconsistent array lengths must be handled externally."
                                },
                                {
                                    "name": "select_threshold",
                                    "args": [
                                        "self",
                                        "thresholds",
                                        "metric_values"
                                    ],
                                    "return_type": "float",
                                    "docstring": "Select the optimal threshold based on a given metric (e.g., maximizing Youden's index).\n\nArgs:\n    thresholds (np.ndarray): Array of threshold values corresponding to the ROC points.\n    metric_values (np.ndarray): Array of computed metric values at each threshold.\n\nReturns:\n    float: The selected optimal threshold value.\n\nNotes:\n    - In cases where multiple thresholds achieve the same optimal metric value, the first occurrence is returned."
                                },
                                {
                                    "name": "compute_auc",
                                    "args": [
                                        "self",
                                        "fpr",
                                        "tpr"
                                    ],
                                    "return_type": "float",
                                    "docstring": "Compute the Area Under the Curve (AUC) for the provided ROC data.\n\nArgs:\n    fpr (np.ndarray): Array of false positive rates.\n    tpr (np.ndarray): Array of true positive rates.\n\nReturns:\n    float: The computed AUC value.\n\nEdge Cases:\n    - The method assumes that the input arrays are non-empty and of equal length."
                                }
                            ]
                        },
                        "path": "src/advanced_modeling/model_evaluation/roc_analysis.py"
                    }
                ]
            },
            {
                "node": "roc curve with threshold selection",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/ROC Analysis/roc curve with threshold selection",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "ROCCurveVisualizer",
                        "extra": {
                            "docstring": "Provides functionalities for visualizing ROC curves along with threshold selection and AUC computation.\n\nThis class encapsulates methods to plot ROC curves, select optimal thresholds based on metrics,\nand compute the Area Under the Curve (AUC) from the ROC data. This combined interface supports\ninteractive and analytical evaluation of classifier performance.\n\nMethods:\n    plot_roc_curve:\n        Generate a plot of the ROC curve given arrays of false positive and true positive rates.\n    select_threshold:\n        Determine the optimal threshold value using a metric (e.g., Youden's index).\n    compute_auc:\n        Calculate the Area Under the ROC Curve (AUC) from input ROC data.\n\nUsage:\n    Instantiate the class and invoke its methods with appropriate numpy arrays to perform visualization\n    and analysis of ROC performance.",
                            "methods": [
                                {
                                    "name": "plot_roc_curve",
                                    "args": [
                                        "self",
                                        "fpr",
                                        "tpr"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Plot the ROC curve using false positive rates and true positive rates.\n\nArgs:\n    fpr (np.ndarray): Array of false positive rates.\n    tpr (np.ndarray): Array of true positive rates.\n\nReturns:\n    None\n\nEdge Cases:\n    - It is assumed that fpr and tpr are of equal length. Inconsistent array lengths must be handled externally."
                                },
                                {
                                    "name": "select_threshold",
                                    "args": [
                                        "self",
                                        "thresholds",
                                        "metric_values"
                                    ],
                                    "return_type": "float",
                                    "docstring": "Select the optimal threshold based on a given metric (e.g., maximizing Youden's index).\n\nArgs:\n    thresholds (np.ndarray): Array of threshold values corresponding to the ROC points.\n    metric_values (np.ndarray): Array of computed metric values at each threshold.\n\nReturns:\n    float: The selected optimal threshold value.\n\nNotes:\n    - In cases where multiple thresholds achieve the same optimal metric value, the first occurrence is returned."
                                },
                                {
                                    "name": "compute_auc",
                                    "args": [
                                        "self",
                                        "fpr",
                                        "tpr"
                                    ],
                                    "return_type": "float",
                                    "docstring": "Compute the Area Under the Curve (AUC) for the provided ROC data.\n\nArgs:\n    fpr (np.ndarray): Array of false positive rates.\n    tpr (np.ndarray): Array of true positive rates.\n\nReturns:\n    float: The computed AUC value.\n\nEdge Cases:\n    - The method assumes that the input arrays are non-empty and of equal length."
                                }
                            ]
                        },
                        "path": "src/advanced_modeling/model_evaluation/roc_analysis.py"
                    }
                ]
            },
            {
                "node": "Statistical Testing",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/Statistical Testing",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/advanced_modeling/model_evaluation/statistical_testing.py"
                    }
                ]
            },
            {
                "node": "interpret results",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/Statistical Testing/interpret results",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "interpret_test_results",
                        "extra": {
                            "args": [
                                "test_statistic",
                                "p_value",
                                "alpha"
                            ],
                            "return_type": "str",
                            "docstring": "Interpret the results of a statistical test by providing a textual explanation based on the test statistic and p-value.\n\nThis function helps in understanding the outcome of statistical tests by comparing the p-value to the significance level\nand summarizing the result in plain language.\n\nArgs:\n    test_statistic (float): The computed test statistic.\n    p_value (float): The p-value obtained from the statistical test.\n    alpha (float, optional): The significance level used in the test; default is 0.05.\n\nReturns:\n    str: A textual interpretation of the statistical test result detailing whether the null hypothesis is rejected.\n\nEdge Cases:\n    - If the p-value is exactly equal to alpha, the interpretation will note the borderline significance."
                        },
                        "path": "src/advanced_modeling/model_evaluation/statistical_testing.py"
                    }
                ]
            },
            {
                "node": "one-tailed test",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/Statistical Testing/one-tailed test",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "one_tailed_t_test",
                        "extra": {
                            "args": [
                                "sample1",
                                "sample2",
                                "tail",
                                "alpha"
                            ],
                            "return_type": "float",
                            "docstring": "Perform a one-tailed t-test to compare the means of two independent samples.\n\nThis function calculates the t-test statistic and p-value for a one-tailed hypothesis, where the alternative hypothesis \nspecifies a direction (either 'right' indicating sample1 > sample2 or 'left' indicating sample1 < sample2).\n\nArgs:\n    sample1 (np.ndarray or list): The first set of observations.\n    sample2 (np.ndarray or list): The second set of observations.\n    tail (str, optional): Indicates the test direction; valid options are \"right\" or \"left\". Default is \"right\".\n    alpha (float, optional): The significance level; default is 0.05.\n\nReturns:\n    float: The p-value for the one-tailed test.\n\nRaises:\n    ValueError: If the 'tail' argument is not \"right\" or \"left\".\n\nEdge Cases:\n    - Small sample sizes or non-normal data can impact the test's reliability.\n    - The function does not handle equal variances issues explicitly."
                        },
                        "path": "src/advanced_modeling/model_evaluation/statistical_testing.py"
                    }
                ]
            },
            {
                "node": "paired t-test with confidence interval",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/Statistical Testing/paired t-test with confidence interval",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "paired_t_test_with_confidence_interval",
                        "extra": {
                            "args": [
                                "before",
                                "after",
                                "confidence"
                            ],
                            "return_type": "Tuple[float, Tuple[float, float]]",
                            "docstring": "Perform a paired t-test and calculate the confidence interval for the mean difference between paired observations.\n\nThis function is used when comparing paired samples (e.g., pre-test and post-test measurements) and needs to\nreport both the p-value and the confidence interval of the difference.\n\nArgs:\n    before (np.ndarray or list): Measurements from the first condition (e.g., before treatment).\n    after (np.ndarray or list): Measurements from the second condition (e.g., after treatment).\n    confidence (float, optional): The confidence level for the confidence interval; default is 0.95.\n\nReturns:\n    Tuple[float, Tuple[float, float]]:\n        - float: The p-value from the paired t-test.\n        - Tuple[float, float]: The lower and upper bounds of the confidence interval for the mean difference.\n\nEdge Cases:\n    - Requires that both inputs have the same length.\n    - Results may be influenced by outliers."
                        },
                        "path": "src/advanced_modeling/model_evaluation/statistical_testing.py"
                    }
                ]
            },
            {
                "node": "paired t-test with multiple metrics",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/Statistical Testing/paired t-test with multiple metrics",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "paired_t_test_multiple_metrics",
                        "extra": {
                            "args": [
                                "metrics_data",
                                "alpha"
                            ],
                            "return_type": "Dict[str, float]",
                            "docstring": "Perform paired t-tests on multiple metrics simultaneously, returning p-values for each metric.\n\nThis function processes a dictionary of paired data for various metrics and computes the p-value \nfor the paired t-test for each metric. It is useful in multi-metric evaluation scenarios.\n\nArgs:\n    metrics_data (Dict[str, Tuple[np.ndarray or list, np.ndarray or list]]):\n        A dictionary where each key is a metric name and its value is a tuple containing two lists or arrays,\n        representing paired observations.\n    alpha (float, optional): The significance level for the tests; default is 0.05.\n\nReturns:\n    Dict[str, float]: A mapping from each metric name to its corresponding p-value from the paired t-test.\n\nEdge Cases:\n    - Each metric's paired data must have matching lengths.\n    - The function does not correct for multiple comparisons by default."
                        },
                        "path": "src/advanced_modeling/model_evaluation/statistical_testing.py"
                    }
                ]
            },
            {
                "node": "test assumptions",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/Statistical Testing/test assumptions",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "test_statistical_assumptions",
                        "extra": {
                            "args": [
                                "data",
                                "assumptions"
                            ],
                            "return_type": "Dict[str, Any]",
                            "docstring": "Test the statistical assumptions underlying the use of t-tests (e.g., normality, variance homogeneity).\n\nThis function evaluates whether the input data meets key assumptions required for valid t-testing.\nIt returns a dictionary with the results of these assumption tests.\n\nArgs:\n    data (np.ndarray): The dataset to be tested for normality, homogeneity of variances, and other relevant assumptions.\n    assumptions (Dict[str, Any], optional): A configuration dictionary specifying which assumption tests to perform \n                                              and their parameters; if None, default tests are applied.\n\nReturns:\n    Dict[str, Any]: A mapping from assumption names to their test outcomes (e.g., p-values or pass/fail indicators).\n\nEdge Cases:\n    - The function assumes that the data is one-dimensional or structured in a way suitable for assumption testing.\n    - When assumptions dictionary is provided, keys must match recognized assumption tests."
                        },
                        "path": "src/advanced_modeling/model_evaluation/statistical_testing.py"
                    }
                ]
            },
            {
                "node": "two-tailed t-test",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/Statistical Testing/two-tailed t-test",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "two_tailed_t_test",
                        "extra": {
                            "args": [
                                "sample1",
                                "sample2",
                                "alpha"
                            ],
                            "return_type": "float",
                            "docstring": "Perform a two-tailed t-test to compare the means of two independent samples.\n\nThis function calculates the t-test statistic and corresponding p-value for two samples under a two-tailed hypothesis.\nIt is used to assess whether the means of two independent groups are statistically different.\n\nArgs:\n    sample1 (np.ndarray or list): The first set of observations.\n    sample2 (np.ndarray or list): The second set of observations.\n    alpha (float, optional): The significance level to determine statistical threshold; default is 0.05.\n\nReturns:\n    float: The p-value indicating the probability of observing the data under the null hypothesis.\n\nEdge Cases:\n    - Samples with very small sizes may result in low statistical power.\n    - Non-normal data distributions may affect test validity."
                        },
                        "path": "src/advanced_modeling/model_evaluation/statistical_testing.py"
                    }
                ]
            },
            {
                "node": "visualize paired t-test results",
                "feature_path": "Advanced Modeling Techniques/Model Evaluation/Comparative Evaluation/Statistical Testing/visualize paired t-test results",
                "file_paths": [
                    {
                        "type": "function",
                        "name": "visualize_paired_t_test_results",
                        "extra": {
                            "args": [
                                "test_results",
                                "title"
                            ],
                            "return_type": "None",
                            "docstring": "Visualize the results of a paired t-test, optionally including the confidence interval and significant differences.\n\nThis function generates a plot to help illustrate the outcome of a paired t-test including elements such as \nmean differences, confidence intervals, and individual sample points if applicable.\n\nArgs:\n    test_results (Any): A data structure containing the paired t-test results. This can include p-values, \n                        confidence intervals, and mean differences.\n    title (str, optional): The title for the plot; default is \"Paired T-Test Results\".\n\nReturns:\n    None: The function creates a plot but does not return any data.\n\nEdge Cases:\n    - Expects the input test_results to be in a format compatible with the plotting logic."
                        },
                        "path": "src/advanced_modeling/model_evaluation/statistical_testing.py"
                    }
                ]
            },
            {
                "node": "Optimization Methods",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/advanced_modeling/optimization"
                    }
                ]
            },
            {
                "node": "Ensemble Strategies",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods/Ensemble Strategies",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/advanced_modeling/optimization"
                    }
                ]
            },
            {
                "node": "Meta-Learning",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods/Ensemble Strategies/Meta-Learning",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/advanced_modeling/optimization/meta_learning.py"
                    }
                ]
            },
            {
                "node": "meta-learning",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods/Ensemble Strategies/Meta-Learning/meta-learning",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "MetaLearningModel",
                        "extra": {
                            "docstring": "A meta-learning model that integrates multiple base learners using a meta-learner.\n\nThis class implements a meta-learning approach, in which predictions from multiple base \nmodels are combined by a meta-learner to improve overall performance. It encapsulates the \nfunctionality for training and prediction in ensemble strategies, specifically within the \nmeta-learning paradigm.\n\nAttributes:\n    base_learners (List[Any]): A list of trained base learning models.\n    meta_learner (Any): A model that aggregates the predictions from the base learners.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "base_learners",
                                        "meta_learner"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the meta-learning model with the given base learners and meta-learner.\n\nArgs:\n    base_learners (List[Any]): A list of pre-configured base model instances.\n    meta_learner (Any): A meta-model responsible for combining base learners' outputs.\n\nReturns:\n    None"
                                },
                                {
                                    "name": "fit",
                                    "args": [
                                        "self",
                                        "X",
                                        "y"
                                    ],
                                    "return_type": "'MetaLearningModel'",
                                    "docstring": "Train the meta-learning model on the provided dataset.\n\nThis method fits both the base learners and the meta-learner, based on the input \ndata and targets, so that the meta-learner can learn to integrate predictions effectively.\n\nArgs:\n    X (Any): The feature set used for training (e.g., pd.DataFrame or np.ndarray).\n    y (Any): The target variable used for training (e.g., pd.Series or np.ndarray).\n    **kwargs: Additional keyword arguments for configuring the training process.\n\nReturns:\n    MetaLearningModel: The fitted meta-learning model instance."
                                },
                                {
                                    "name": "predict",
                                    "args": [
                                        "self",
                                        "X"
                                    ],
                                    "return_type": "Any",
                                    "docstring": "Generate predictions using the trained meta-learning model.\n\nThis method aggregates predictions from base learners and applies the meta-learner to produce\na final output.\n\nArgs:\n    X (Any): Input data for which predictions are to be generated (e.g., pd.DataFrame or np.ndarray).\n\nReturns:\n    Any: The aggregated predictions from the meta-learning model."
                                }
                            ]
                        },
                        "path": "src/advanced_modeling/optimization/meta_learning.py"
                    }
                ]
            },
            {
                "node": "Evolutionary Optimization",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods/Evolutionary Optimization",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/advanced_modeling/optimization"
                    }
                ]
            },
            {
                "node": "Genetic Algorithms",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods/Evolutionary Optimization/Genetic Algorithms",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/advanced_modeling/optimization/genetic_algorithms.py"
                    }
                ]
            },
            {
                "node": "crossover operation",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods/Evolutionary Optimization/Genetic Algorithms/crossover operation",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "GeneticAlgorithm",
                        "extra": {
                            "docstring": "Implements a genetic algorithm incorporating elitism strategy,\ncustomizable fitness function, and a crossover operation for generating new individuals.\n\nThis class encapsulates the core operations of a genetic algorithm including:\n  - Preserving a fraction of the best individuals (elitism).\n  - Computing fitness using a custom fitness function.\n  - Generating offspring via a crossover operation.\n\nAttributes:\n    population (List[Any]): The current population of individuals.\n    fitness_function (Callable[[Any], float]): A callable that computes the fitness score of an individual.\n    elitism_rate (float): The fraction of top individuals to retain for the next generation.\n    mutation_rate (float): The probability of mutation for offspring.\n\nMethods:\n    run(generations: int) -> List[Any]:\n        Executes the genetic algorithm for a specified number of generations.\n        \n    crossover(parent1: Any, parent2: Any) -> Any:\n        Combines two parent individuals to create a new offspring.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "population",
                                        "fitness_function",
                                        "elitism_rate",
                                        "mutation_rate"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initializes the genetic algorithm with the given population, fitness function, and parameters for elitism and mutation.\n\nArgs:\n    population (List[Any]): Initial list of individuals representing the starting population.\n    fitness_function (Callable[[Any], float]): A custom function to evaluate the fitness of an individual.\n    elitism_rate (float, optional): Proportion of top individuals to be carried over unaltered to the next generation. Defaults to 0.1.\n    mutation_rate (float, optional): Probability of mutation during offspring generation. Defaults to 0.01.\n\nAssumptions:\n    - The population is non-empty and diverse enough for evolutionary processes.\n    - The fitness_function properly returns a numeric score indicating individual performance."
                                },
                                {
                                    "name": "run",
                                    "args": [
                                        "self",
                                        "generations"
                                    ],
                                    "return_type": "List[Any]",
                                    "docstring": "Executes the genetic algorithm over a specified number of generations.\n\nIncorporates elitism by retaining the top-performing individuals,\napplies the custom fitness function to evaluate each generation,\nand uses the crossover operation to create new offspring.\n\nArgs:\n    generations (int): The number of generations (iterations) for which the algorithm should run.\n\nReturns:\n    List[Any]: The final population after completion of all generations.\n\nEdge Cases:\n    - If generations is less than or equal to zero, the function should either return the initial population or handle it gracefully."
                                },
                                {
                                    "name": "crossover",
                                    "args": [
                                        "self",
                                        "parent1",
                                        "parent2"
                                    ],
                                    "return_type": "Any",
                                    "docstring": "Performs a crossover operation between two parent individuals to produce a new offspring.\n\nThis function is responsible for combining genetic information from both parents,\npotentially incorporating techniques such as one-point or two-point crossover.\n\nArgs:\n    parent1 (Any): The first parent individual.\n    parent2 (Any): The second parent individual.\n\nReturns:\n    Any: A new offspring individual created by merging the attributes of the two parents.\n\nAssumptions:\n    - Both parents are compatible in structure such that their genetic material can be sensibly combined.\n    - The method does not enforce mutation; mutation is handled separately in the algorithm."
                                }
                            ]
                        },
                        "path": "src/advanced_modeling/optimization/genetic_algorithms.py"
                    }
                ]
            },
            {
                "node": "genetic algorithm with custom fitness function",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods/Evolutionary Optimization/Genetic Algorithms/genetic algorithm with custom fitness function",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "GeneticAlgorithm",
                        "extra": {
                            "docstring": "Implements a genetic algorithm incorporating elitism strategy,\ncustomizable fitness function, and a crossover operation for generating new individuals.\n\nThis class encapsulates the core operations of a genetic algorithm including:\n  - Preserving a fraction of the best individuals (elitism).\n  - Computing fitness using a custom fitness function.\n  - Generating offspring via a crossover operation.\n\nAttributes:\n    population (List[Any]): The current population of individuals.\n    fitness_function (Callable[[Any], float]): A callable that computes the fitness score of an individual.\n    elitism_rate (float): The fraction of top individuals to retain for the next generation.\n    mutation_rate (float): The probability of mutation for offspring.\n\nMethods:\n    run(generations: int) -> List[Any]:\n        Executes the genetic algorithm for a specified number of generations.\n        \n    crossover(parent1: Any, parent2: Any) -> Any:\n        Combines two parent individuals to create a new offspring.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "population",
                                        "fitness_function",
                                        "elitism_rate",
                                        "mutation_rate"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initializes the genetic algorithm with the given population, fitness function, and parameters for elitism and mutation.\n\nArgs:\n    population (List[Any]): Initial list of individuals representing the starting population.\n    fitness_function (Callable[[Any], float]): A custom function to evaluate the fitness of an individual.\n    elitism_rate (float, optional): Proportion of top individuals to be carried over unaltered to the next generation. Defaults to 0.1.\n    mutation_rate (float, optional): Probability of mutation during offspring generation. Defaults to 0.01.\n\nAssumptions:\n    - The population is non-empty and diverse enough for evolutionary processes.\n    - The fitness_function properly returns a numeric score indicating individual performance."
                                },
                                {
                                    "name": "run",
                                    "args": [
                                        "self",
                                        "generations"
                                    ],
                                    "return_type": "List[Any]",
                                    "docstring": "Executes the genetic algorithm over a specified number of generations.\n\nIncorporates elitism by retaining the top-performing individuals,\napplies the custom fitness function to evaluate each generation,\nand uses the crossover operation to create new offspring.\n\nArgs:\n    generations (int): The number of generations (iterations) for which the algorithm should run.\n\nReturns:\n    List[Any]: The final population after completion of all generations.\n\nEdge Cases:\n    - If generations is less than or equal to zero, the function should either return the initial population or handle it gracefully."
                                },
                                {
                                    "name": "crossover",
                                    "args": [
                                        "self",
                                        "parent1",
                                        "parent2"
                                    ],
                                    "return_type": "Any",
                                    "docstring": "Performs a crossover operation between two parent individuals to produce a new offspring.\n\nThis function is responsible for combining genetic information from both parents,\npotentially incorporating techniques such as one-point or two-point crossover.\n\nArgs:\n    parent1 (Any): The first parent individual.\n    parent2 (Any): The second parent individual.\n\nReturns:\n    Any: A new offspring individual created by merging the attributes of the two parents.\n\nAssumptions:\n    - Both parents are compatible in structure such that their genetic material can be sensibly combined.\n    - The method does not enforce mutation; mutation is handled separately in the algorithm."
                                }
                            ]
                        },
                        "path": "src/advanced_modeling/optimization/genetic_algorithms.py"
                    }
                ]
            },
            {
                "node": "genetic algorithm with elitism",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods/Evolutionary Optimization/Genetic Algorithms/genetic algorithm with elitism",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "GeneticAlgorithm",
                        "extra": {
                            "docstring": "Implements a genetic algorithm incorporating elitism strategy,\ncustomizable fitness function, and a crossover operation for generating new individuals.\n\nThis class encapsulates the core operations of a genetic algorithm including:\n  - Preserving a fraction of the best individuals (elitism).\n  - Computing fitness using a custom fitness function.\n  - Generating offspring via a crossover operation.\n\nAttributes:\n    population (List[Any]): The current population of individuals.\n    fitness_function (Callable[[Any], float]): A callable that computes the fitness score of an individual.\n    elitism_rate (float): The fraction of top individuals to retain for the next generation.\n    mutation_rate (float): The probability of mutation for offspring.\n\nMethods:\n    run(generations: int) -> List[Any]:\n        Executes the genetic algorithm for a specified number of generations.\n        \n    crossover(parent1: Any, parent2: Any) -> Any:\n        Combines two parent individuals to create a new offspring.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "population",
                                        "fitness_function",
                                        "elitism_rate",
                                        "mutation_rate"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initializes the genetic algorithm with the given population, fitness function, and parameters for elitism and mutation.\n\nArgs:\n    population (List[Any]): Initial list of individuals representing the starting population.\n    fitness_function (Callable[[Any], float]): A custom function to evaluate the fitness of an individual.\n    elitism_rate (float, optional): Proportion of top individuals to be carried over unaltered to the next generation. Defaults to 0.1.\n    mutation_rate (float, optional): Probability of mutation during offspring generation. Defaults to 0.01.\n\nAssumptions:\n    - The population is non-empty and diverse enough for evolutionary processes.\n    - The fitness_function properly returns a numeric score indicating individual performance."
                                },
                                {
                                    "name": "run",
                                    "args": [
                                        "self",
                                        "generations"
                                    ],
                                    "return_type": "List[Any]",
                                    "docstring": "Executes the genetic algorithm over a specified number of generations.\n\nIncorporates elitism by retaining the top-performing individuals,\napplies the custom fitness function to evaluate each generation,\nand uses the crossover operation to create new offspring.\n\nArgs:\n    generations (int): The number of generations (iterations) for which the algorithm should run.\n\nReturns:\n    List[Any]: The final population after completion of all generations.\n\nEdge Cases:\n    - If generations is less than or equal to zero, the function should either return the initial population or handle it gracefully."
                                },
                                {
                                    "name": "crossover",
                                    "args": [
                                        "self",
                                        "parent1",
                                        "parent2"
                                    ],
                                    "return_type": "Any",
                                    "docstring": "Performs a crossover operation between two parent individuals to produce a new offspring.\n\nThis function is responsible for combining genetic information from both parents,\npotentially incorporating techniques such as one-point or two-point crossover.\n\nArgs:\n    parent1 (Any): The first parent individual.\n    parent2 (Any): The second parent individual.\n\nReturns:\n    Any: A new offspring individual created by merging the attributes of the two parents.\n\nAssumptions:\n    - Both parents are compatible in structure such that their genetic material can be sensibly combined.\n    - The method does not enforce mutation; mutation is handled separately in the algorithm."
                                }
                            ]
                        },
                        "path": "src/advanced_modeling/optimization/genetic_algorithms.py"
                    }
                ]
            },
            {
                "node": "Hyperparameter Optimization",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods/Hyperparameter Optimization",
                "file_paths": [
                    {
                        "type": "dir",
                        "path": "src/advanced_modeling/optimization"
                    }
                ]
            },
            {
                "node": "Tuning Strategies",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods/Hyperparameter Optimization/Tuning Strategies",
                "file_paths": [
                    {
                        "type": "file",
                        "path": "src/advanced_modeling/optimization/hyperparameter.py"
                    }
                ]
            },
            {
                "node": "hyperband",
                "feature_path": "Advanced Modeling Techniques/Optimization Methods/Hyperparameter Optimization/Tuning Strategies/hyperband",
                "file_paths": [
                    {
                        "type": "class",
                        "name": "HyperbandOptimizer",
                        "extra": {
                            "docstring": "Interface for hyperparameter optimization using the Hyperband tuning strategy.\n\nHyperbandOptimizer implements the Hyperband algorithm to efficiently search a\nlarge hyperparameter space. It dynamically allocates resources to promising\nconfigurations while discarding poor performers early using successive halving.\n\nUsage:\n    1. Instantiate the optimizer with the maximum iteration count, a downsampling factor,\n       and a user-defined evaluation function.\n    2. Call the 'tune' method with a list of hyperparameter configurations.\n    3. Retrieve the best found configuration according to the given evaluation function.\n\nArgs:\n    max_iter (int): The maximum resource allocation (e.g., maximum iterations or epochs)\n        for the tuning process.\n    eta (int): The reduction factor; determines the proportion of configurations discarded\n        in each round (typically, eta > 1).\n    evaluation_fn (Callable[[Dict[str, Any], int], float]): A callable function that evaluates\n        each hyperparameter configuration with a given allocated resource. It accepts a configuration\n        dictionary and an integer resource allocation, returning a performance metric (e.g., loss).\n\nMethods:\n    tune(configurations: List[Dict[str, Any]]) -> Dict[str, Any]:\n        Executes the Hyperband algorithm on a list of hyperparameter configurations and returns\n        the configuration that achieved the best performance metric.",
                            "methods": [
                                {
                                    "name": "__init__",
                                    "args": [
                                        "self",
                                        "max_iter",
                                        "eta",
                                        "evaluation_fn"
                                    ],
                                    "return_type": "None",
                                    "docstring": "Initialize the HyperbandOptimizer with tuning parameters.\n\nArgs:\n    max_iter (int): Maximum resource (iterations/epochs) available for hyperparameter tuning.\n    eta (int): Downsampling factor for successive halving; number of configurations to retain in each round.\n    evaluation_fn (Callable[[Dict[str, Any], int], float]): Function to evaluate a hyperparameter\n        configuration with a given resource allocation."
                                },
                                {
                                    "name": "tune",
                                    "args": [
                                        "self",
                                        "configurations"
                                    ],
                                    "return_type": "Dict[str, Any]",
                                    "docstring": "Run the Hyperband algorithm to select the best hyperparameter configuration.\n\nThis method evaluates the provided hyperparameter configurations using the Hyperband\nstrategy. It allocates resources dynamically and prunes configurations based on their\nperformance, ultimately returning the best configuration.\n\nArgs:\n    configurations (List[Dict[str, Any]]): A list of hyperparameter configurations where\n        each configuration is represented as a dictionary mapping parameter names to values.\n\nReturns:\n    Dict[str, Any]: The best hyperparameter configuration determined by the evaluation function.\n\nRaises:\n    ValueError: If the provided configurations list is empty."
                                }
                            ]
                        },
                        "path": "src/advanced_modeling/optimization/hyperparameter.py"
                    }
                ]
            }
        ]
    }
}